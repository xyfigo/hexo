{"pages":[{"title":"关于我","text":"我叫肖飞，毕业于中南大学信息与计算科学专业和清华大学计算机技术专业，取得系统分析师资格，现就职于自然资源部信息中心，现任高级工程师。自2009年起，一直在生产技术一线从事电子政务、地理信息系统和信息化专业技术和管理工作，历任助理工程师、工程师，在信息化规划设计、项目实施、运行维护和网络安全保密管理领域拥有多年经验，具备较高的政治素质和专业技术能力。 近年来，共参与完成近20项原国家测绘地理信息局电子政务和信息安全建设项目，运行维护30余个部级各类管理信息系统，支撑保障门户网站、内网办公、行政审批、综合监管等行政管理事项日常运转，服务于1.8万家测绘资质单位、3万多家产业单位和3700余个各级管理部门通过网络办理政务服务事项，形成了行政审批快速开发平台、数据集成标准规范、综合监管平台、测绘行业信息安全等级保护技术体系等各类成果，并在电子政务和信息安全工作中得到实际应用和推广。其中1个项目获得测绘科技进步二等奖（个人排名第3）和地理信息产业协会优秀工程奖，3个项目获得电子政务理事会部委优秀案例奖，个人5次获评中心评优表彰。 在新技术研究和应用方面，热爱技术，乐于技术探索，持续致力于计算机技术和地理信息技术的交叉应用，坚持将地理信息系统与计算机技术最新发展相结合，探索基于云计算的地理空间信息海量数据管理、分布式资源共享等方面研究，特别是运用互联网技术解决传统地理信息和电子政务发展遇到的瓶颈问题，推动政务信息化与业务信息化相互促进、共同发展。目前，正在积极研究探索地理空间大数据、人工智能、Cloud Native、DevOps等新一代信息技术在国家级政府部门的落地应用，做互联网+政府的探索者和实践者。 在学术交流方面，目前担任SCI期刊Computers and Geosciences特邀审稿人，在国内外刊物公开发表论文5篇，其中1篇被国际摄影测量和遥感学会ISPRS Annals收录，1篇被ISPRS Archives收录，并被EI索引；解决生产技术难题5项，在研和初步完成4项新技术开发应用转化，在测绘地理信息电子政务和业务应用中取得良好效果。 积极参与国际开源社区技术交流，为开源云计算项目OpenStack4J和开源地理信息大数据项目SpatialHadoop贡献源代码，参与项目维护和技术支持。 我的教育背景 2005至2009 中南大学 信息与计算科学 本科 2011至2015 清华大学 计算机技术 硕士 2015至2016 美国乔治梅森大学 GIS 访问学者 我的论文 《学习索引——一种基于机器学习方法的索引模型》，2018年9月，中国测绘地理信息学会学术年会论文集 《A Big Spatial Data Processing Framework Applied to National Geographic Conditions Monitoring（空间大数据处理框架在地理国情监测中的应用）》，Fei Xiao，The International Archives of the Photogrammetry，Remote Sensing and Spatial Information Sciences（摄影测量、遥感和空间信息科学国际档案），2018.5，EI索引 《A Spark Based Computing Framework for Spatial Data（基于Spark的空间数据计算框架）》，Fei Xiao，ISPRS Annals（国际摄影测量与遥感学会年报），2017.10，CPCI(ISTP)索引 《基于OpenStack的应用自动部署管理系统研究》，肖飞，清华大学工程硕士毕业论文，2015.5 《Research on the Automatic Deployment and Management System of Applications Based on OpenStack》，Fei Xiao， Tsinghua University 参与项目 序号 项目名称 起始时间 任务来源 角色 1 国家测绘地理信息局行业综合监管平台建设项目 2017年5月 国家测绘地理信息局 技术负责人 2 国家测绘地理信息局双随机抽查管理系统 2016年10月 国家测绘地理信息局 技术负责人 3 管理信息中心内网办公系统开发项目 2016年6月 国家测绘地理信息局管理信息中心 技术负责人 4 国家测绘地理信息局统计网络直报系统建设项目 2015年6月 国家测绘地理信息局 技术负责人 5 国家测绘地理信息局行政许可网上审批系统开发项目 2015年6月 国家测绘地理信息局 技术负责人 6 测绘地理信息行政执法管理信息系统（一期）建设项目 2014年6月 国家测绘地理信息局 技术负责人 7 国家测绘地理信息局测绘资质管理信息系统升级改造项目 2014年5月 国家测绘地理信息局 技术负责人 8 国家测绘地理信息局机关内网办公系统升级改造项目 2013年12月 国家测绘地理信息局 主要参加人 9 测绘业务管理信息化体系研究与建设示范 2015年1月 国家测绘地理信息局 主要参加人 10 国家测绘地理信息局政务信息资源中心建设项目 2016年12月 国家测绘地理信息局 主要参加人 11 国家测绘地理信息局外网网上办事大厅建设项目 2016年11月 国家测绘地理信息局 主要参加人 12 国家测绘地理信息局网上办事大厅项目（二期）与政府信息公开目录系统项目 2016年5月 国家测绘地理信息局 主要参加人 13 国家测绘地理信息局政务应用支撑平台集成实施项目 2014年9月 国家测绘地理信息局 主要参加人 14 国家测绘地理信息局地图审批子系统开发 2014年12月 国家测绘地理信息局 主要参加人 获奖情况 序号 获奖项目 奖项 排名 获奖时间 授予机构 1 国家测绘地理信息局综合监管平台（一期） 测绘科技进步二等奖 3 2018年9月 中国测绘学会 2 国家测绘地理信息局综合监管平台（一期） 地理信息产业优秀工程奖金奖 2 2018年7月 中国地理信息产业协会 3 国家测绘地理信息局综合监管平台 2018年电子政务优秀案例 1 2019年8月 电子政务理事会 4 国家测绘地理信息局测绘资质管理信息系统 2014年电子政务优秀案例 1 2015年9月 电子政务理事会 5 国家测绘地理信息局政务应用支撑平台 2016年电子政务优秀案例 1 2017年7月 电子政务理事会 6 国家测绘地理信息局“互联网+政务服务”专项成果 2016年电子政务优秀案例 2 2017年7月 电子政务理事会 7 个人 2011年度优秀职员 2011年12月 国家测绘局管理信息中心 8 个人 2013年度优秀职员 2013年12月 国家测绘地理信息局管理信息中心 9 个人 2016年度优秀职员 2017年1月 国家测绘地理信息局管理信息中心 10 个人 2017年度民主评议优秀党员 2018年2月 国家测绘地理信息局管理信息中心 11 个人 2018年度优秀职员 2019年2月 自然资源部信息中心","link":"/about/index.html"}],"posts":[{"title":"CentOS 7 部署Ceph集群","text":"#三、部署集群 1. 安装准备，创建文件夹在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。 123$ cd ~$ mkdir my-cluster$ cd my-cluster 注：若安装ceph后遇到麻烦可以使用以下命令进行清除包和配置： 123456// 删除安装包$ ceph-deploy purge admin-node node1 node2 node3// 清除配置$ ceph-deploy purgedata admin-node node1 node2 node3$ ceph-deploy forgetkeys 2. 创建集群和监控节点创建集群并初始化监控节点： 1$ ceph-deploy new &#123;initial-monitor-node(s)&#125; 这里node1是monitor节点，所以执行： 1$ ceph-deploy new ceph_node1 完成后，my-clster 下多了3个文件：ceph.conf、ceph-deploy-ceph.log 和 ceph.mon.keyring。 问题：如果出现 “[ceph_deploy][ERROR ] RuntimeError: remote connection got closed, ensure requiretty is disabled for node1”，执行 sudo visudo 将 Defaults requiretty 注释掉。 3. 修改配置文件1$ cat ceph.conf 内容如下： 123456789[global]fsid = 2539e16a-2b19-476d-8005-3d749e288583mon_initial_members = ceph_node1mon_host = 10.10.20.61auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd pool default size = 2rbd_default_features = 1 把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把 osd pool default size = 2 加入 [global] 段： 1$ sed -i &apos;$a\\osd pool default size = 2&apos; ceph.conf 如果有多个网卡，可以把 public network 写入 Ceph 配置文件的 [global] 段： 1public network = &#123;ip-address&#125;/&#123;netmask&#125; 如果Ceph存储需要提供kubernetes使用的OSD，注意要设置 1rbd_default_features = 1 4. 安装Ceph在所有节点上安装ceph： 1$ ceph-deploy install ceph_admin_node ceph_node1 ceph_node2 ceph_node3 ceph_node4 问题：[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: yum -y install epel-release 解决方法： 12&gt; sudo yum -y remove epel-release&gt; 5. 配置初始 monitor(s)、并收集所有密钥1$ ceph-deploy mon create-initial 完成上述操作后，当前目录里应该会出现这些密钥环： 1234&#123;cluster-name&#125;.client.admin.keyring&#123;cluster-name&#125;.bootstrap-osd.keyring&#123;cluster-name&#125;.bootstrap-mds.keyring&#123;cluster-name&#125;.bootstrap-rgw.keyring 6. 添加OSD1) 登录到 Ceph 节点、并给 OSD 守护进程创建一个目录，并添加权限。 123456789$ ssh node1$ sudo mkdir /var/local/osd1$ sudo chmod 777 /var/local/osd1/$ exit$ ssh node2$ sudo mkdir /var/local/osd2$ sudo chmod 777 /var/local/osd2/$ exit 注意，从最好将osd单独做一个分区，log单独做一个分区。其中osd分区使用xfs格式，这是ceph最佳推荐设置。 我是用了linux lvm，LV名称为/dev/data/osd 12345678910111213141516171819202122[root@localhost ceph]# mkfs.xfs -f /dev/data/osdmeta-data=/dev/data/osd isize=256 agcount=4, agsize=32761600 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0data = bsize=4096 blocks=131046400, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=0log =internal log bsize=4096 blocks=63987, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0[root@localhost ceph]# mount -t xfs /dev/data/osd /var/local/osd4[root@localhost ceph]# df -Th文件系统 类型 容量 已用 可用 已用% 挂载点/dev/mapper/centos-root xfs 50G 1.7G 49G 4% /devtmpfs devtmpfs 7.8G 0 7.8G 0% /devtmpfs tmpfs 7.8G 0 7.8G 0% /dev/shmtmpfs tmpfs 7.8G 8.6M 7.8G 1% /runtmpfs tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup/dev/mapper/centos-home xfs 42G 33M 42G 1% /home/dev/sda1 xfs 497M 124M 373M 25% /boottmpfs tmpfs 1.6G 0 1.6G 0% /run/user/0/dev/mapper/data-osd xfs 500G 33M 500G 1% /var/local/osd4 2) 然后，从管理节点执行 ceph-deploy 来准备 OSD 。 1$ ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1 3) 最后，激活 OSD 。 1$ ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1","link":"/2019/04/29/CentOS-7-部署Ceph集群/"},{"title":"Cloud Native Geospatial之一: 基本假设和工作过程","text":"实现假设所有数据和计算都在云中，从开发人员的角度探讨其对工作流的影响是有意义的。结论意义重大，并且引发了一系列创新，例如容器编配和微服务架构不仅是纯技术性的工作，更是过程和方法论。 以上4原则的含义外延十分广阔，引领我们创新出难以想象的工作流程。但是在Cloud Native Geospatial不长的历史过程中，已经开始出现了进展： 不需要复制数据当数据所处的云环境中，各种各样的软件能够直接工作，所以不需要维护任何数据的拷贝。数据提供商为数据存储付费，并且按数据访问进行收费。数据用户根据需要访问数据，进行可视化或生产数据产品等处理。 发送算法到数据数据存储在云中的某个位置，导致了一种发送算法到数据的范式，而不是其他的方式。如果所有的数据都在云中的某处，并且数据太大不能够在本地下载和处理，所以用户必须打包他们的算法，在云中运行。这会形成一个容器，像完整的Docker处理实例一样。但是更高级的云GIS系统，例如RasterFoundry 和Google Earth Engine，用户只是发送一个脚本和操作描述来运行。 算法缓冲随着数据在一处访问和云提供一系列可用的算法，与桌面范式相比，处理工具变得的越来越可共享。事实上提高协作性可能是Web和云带来的最重要的意义。未来大多数用户只需要从最流行的算法中进行挑选，而不需要自己做大气辐射转换、去云、大气校正、表面反射率计算等专业性的算法区实现一个常态化差值植生指标(Normalized Difference Vegetation Index)的计算。只需要进行预处理，并选择一个最流行的NDVI算法。Web分层映射将帮助这种分析协作变为可能，因为处理的每个步骤都可以随时进行可视化和验证。 实时GIS新IoT传感器、手机和卫星影像实时产生更多的数据流，并持续更新数据库。计算机视觉和深度学习的结合的最新算法（目标识别、变化检测等）会被添加到地理信息工具箱，用户必须访问持续更新的一系列地图。局限在于数据收集频率。 随着在云中处理这些更新数据，这开启了一个新的过程，产生更高级的分析产品，例如在机场中发现新的飞机、发现农田健康的显著变化，以及发现新的油井。 地图核实当前GIS和遥感工作的成果一般以地图的形式输出，并经常被嵌入在PPT中。但是地图中的点会带来信息和知识，例如在这个机场上出现10架新飞机。关心这类信息的用户希望直接订阅这个信息，他们希望在所关心的信息到达阀值的时候得到提醒，而不需要查看检查每个图像。这类地理信息智能处理过程中的任何步骤都应该进行可视化，但是大多数用户应该在得到信息结果后使用地图进行核实，而不是以地图作为主要的信息界面。 未来展望本文介绍了可访问性、集中数据和工作流程的显著不同，Cloud Native Geospatial能够为新的非专业用户提供地理信息的强大能力，这种能力被GIS从业者使用了几十年。当前空间智能的受益人是那些构建GIS生态系统的参与者，他们依靠数据进行工作，手工生成信息，借助Web地图印制地图、报告。当这些生态系统本身真正实现云原生，开发信息价值的工作不再只由从业者承担。地理信息专家能够协作创造分析和成果，但是任何非专业的用户也能够选择和应用这些技术在他们关心的地理领域。 有了正确的云架构，随着越来越多的新的、有创造性的用户从地理空间数据中创造价值，我们有潜力发现GIS技术的新应用。 目前实现Cloud Native Geospatial仍有很多工作，但是各类组织和个人正在一起进行着努力。请继续关注本系列文章，我们将深入研究Cloud Native Geospatial如何工作，并探索当前运行的一些先进的Cloud Native Geospatial架构。 下面，我们将研究Cloud Optimized GeoTIFFs——一个基于Cloud Native Geospatial之上构建的地理数据格式。","link":"/2019/03/08/Cloud-Native-Geospatial之一-基本假设和工作过程/"},{"title":"IDEA 快捷键","text":"Navigation 搜索类：Ctrl+N 搜索文件：Ctrl+Shift+N 搜索符号：Crtl+Shift+Alt+N 全部搜索：Shift+Shift 回退/前进到上/下一个窗口：Ctrl+Alt+左/右 视图 全屏：Shift+Ctrl+F12","link":"/2019/04/04/IDEA-快捷键/"},{"title":"Kubectl 常用命令","text":"1.进入某个pod如何进入kubernetes的一个pod呢，其实和进入docker的一个容器相似： 进入docker容器 ： 1docker exec -ti &lt;your-container-name&gt; /bin/sh 进入pod： 1kubectl exec -ti &lt;your-pod-name&gt; -n &lt;your-namespace&gt; -- /bin/sh","link":"/2018/12/17/Kubectl-常用命令/"},{"title":"Cloud Native Geospatial之二云优化的GeoTIFF","text":"这篇文章是Cloud Native GeoSpatial系列文章的第二篇。本系列文章主要探寻如何使用原生云环境构建地理空间系统和工作流程。本文将深入研究地理空间云原生领域最重要的技术：云优化的GeoTIFF。 云优化的GeoTIFF（Cloud Optimized GeoTIFF，简称COG）是一种特殊格式化的GeoTiFF文件，它借助HTTP的新特性Byte Serving实现。cogeo.org提供了更多详细信息。Byte Serving是一种提供在线视频或音频流文件的技术，能够跳过文件前面或者后面的某些内容。你可以告知服务器从某个特定的位置开始获取文件，而并不需要下载全部的多媒体文件。COG格式以同样的方式工作，允许用户访问一个栅格文件中他们所需要的那一部分。 任意跳转到一个栅格文件中任何所需的部分，这能够创造很多新的工作流程，因为数据能够像视频一样成为数据流，而不需要全部通过网络传输。在地理空间领域，用户可以在线访问Web瓦片，但是做实际的分析仍然需要原始删个文件。这意味着要获取一个几百兆或者更大的文件需要较长的下载时间。这因为原始的删个文件在云中分布式部署，但是他们不能够表现为流格式。所以用户在处理和可视化云中文件的时候，必须全部下载文件。 COG格式始于Amazon、Planet Labs、MapBox、ESRI和USGS之间的合作，为了以可访问的形式把Landsat档案放到AWS中。GeoTIFF是一种广泛应用的影像文件格式，但同时在Landsat-pds的邮件列表中对如何以最佳格式使用流式格式传输数据有着广泛的讨论。好的存储格式能使合作伙伴在他们现有的工作流程中利用Amazon Web Services中的归档数据，而不需要进行复制和重复处理这些数据。一旦这种云原生的模式建立，采用这种架构的软件将会用同样的方式使用数据，这将显著的提高数据利用。 有文献给出了针对云工作流程优化GeoTIFF的最佳实践，GDAL的实现包括了明确的文档和发布在GDALWiKi的性能测试结果。Planet Labs通过处理管道将所有数据的生产逐渐转向了云优化的GeoTIFF，并借此和它的合作伙伴FarmShots和Santiago&amp;Cintra一起重新构建起特定领域的应用。 在产业界方面，领先的开源项目如DGAL、QGIS和GeoServer已经可以读取这种格式（通过QGIS和GeoServer的高级配置）。DigitalGlobe目前正在将他们的IDAHO系统迁移到使用COG格式，同时利用它处理大量的数据。OpenAerialMap的架构就是将用户上传的数据转换为Web可读的GeoTIFF格式并且直接从这些数据中流式传输他们的图层。GeoTrellis项目在短期开发计划中将对COG进行支持，另外一些项目包括相似的空间集群计算处理系统也有意支持它。 这些新的线上处理系统强调利用云原生地理空间架构。这种系统能同时地在上百个甚至几千个计算机中处理影像，在几秒内就能返回原来需要几天甚至数周的分析结果。尽管有这些现代化的有点，因为核心的标准是GeoTIFF，任何软件都可以读取数据，甚至是旧的桌面应用程序。 尽管核心概念十分简单——将图像移至线上并通过流式传输，这却是构建云原生空间系统的基础。数据在云中，许多软件系统更接近数据端运行，已达到获取数据价值的目的，不需要额外的下载和存储成本。对此生态系统更全面的研究将在后续中介绍，但是核心的COG格式和原理是根本，用户可以将他们的精力花在接近实时的工作来探寻数据价值，而不用依靠少数具备空间分析专业能力的专家。 尽管云优化的GeoTIFF是一种相对新的格式，但是向后兼容性和相对容易的实现使得这种格式引人注意，这些组织的目的是鼓励更多的软件和数据提供商接收它。如果你感兴趣想了解更多的东西，请访问cogeo.org。","link":"/2019/03/22/Cloud-Native-Geospatial之二云优化的GeoTIFF/"},{"title":"Flink适用场景","text":"本文翻译自Flink Documents：https://flink.apache.org/usecases.html 事件驱动型应用定义事件驱动型应用是一个有状态的应用，应用从一个或多个事件流中获取事件，并且通过触发计算、更新状态或其他外部行为的方式对获取的事件做出响应。 事件驱动型应用是采用分离计算和数据存储层的设计方式对传统应用的一种改进。在传统应用架构中，应用从远程事务性数据库中读取和持久化数据。 与此相反，事件驱动型应用基于状态流处理应用。在它的设计中，数据和计算位于一处（co-located），以实现本地化数据访问（内存或硬盘）。通过在远程持久化存储中周期性地写入检查点来实现容错性。下图描述了传统应用和事件驱动型应用的不同。 事件驱动型应用的优势相比于从远程数据库中查询数据，事件驱动型应用从本地访问数据，得以在吞吐量和延迟时间上取得更好地性能。检查点是以异步和增量的方式在远程持久化存储中周期性的写入。通常在分层架构中，应用的多个层次都共享同一个数据库。因此，数据库的任何改动，例如因为应用更新或服务扩展需要改变数据展现，都需要协调应用的方方面面。因为时间驱动型应用只对自己的数据负责，所以改变数据展现或者扩展应用需要更少的协调工作。 Flink是如何支持事件驱动型应用的Flink的许多优秀的特性都围绕这些概念。Flink提供丰富的状态单元集合，它能够管理大数据容量（上至太字节）的一致性保证。此外，Flink提供ProcessFunction以支持事件时间、可定制的窗口逻辑和细粒度的时间控制，以此实现高级的业务逻辑。而且Flink提供了复杂事件处理（CEP）程序库来发现数据流中的模式。 另外，Flink还为事件驱动型应用提供了另一个优秀的特性：保存点（savepoint）。保存点是一个持久化的状态，可以被用来作为兼容应用的一个始点。给应用一个保存点，应用就此开始更新和调整，或者使用多个应用版本开始进行A/B测试。 典型的事件驱动型应用 欺诈监测 异常检测 基于规则的预警 业务流程监控 Web应用程序（社交网络） 数据分析应用分析型任务从原始数据中提取信息并获得知识。典型的分析一般表现为批处理查询或者有限的事件记录数据集。为了在分析结果中合并最新的数据，必须在分析中增加被分析的数据集，并且重新运行查询或分析应用，将结果写入存储系统或者给出分析报告。 如果拥有一个精良的流处理引擎，分析可以实时的进行。流查询应用不再是读取有限的数据集合，而是获取实时事件流并持续地生产和更新分析结果，与此同时事件被消费。结果写入外部数据库或者维护一个内部的状态。数据可视化应用从外部数据库读取最新的结果或者直接查询应用内部的状态。 如下图所示，Apache Flink同时支持流处理和批处理型分析应用。 流分析应用的优势与批处理应用相比，持续流分析应用由于消除周期性的数据导入和执行查询，不仅仅可以更快的从事件中获得知识。与批处理查询相比，流查询不需要处理由于周期性导入和输入数据等导致的人为造成的数据边界。 另一个方面是更简单的应用架构。批处理分析管道包括了几个独立组件周期性调度数据输入和执行查询。管道是一种可靠性的操作，这非常重要，因为一个组件的错误会影响管道后续的过程。与此相反，运行在Flink精密的流处理器之上的流处理分析应用包括了从数据输入到持续结果计算所有的过程。因此，它能够依靠引擎的错误恢复机制。 Flink如何支持数据分析应用Flink对流分析和批处理分析都有很好的支持。具体的说，它提供了一个对批处理和流处理标准一致的，并且与ANSI兼容的SQL接口。SQL查询计算对记录的静态数据集或实时事件流都可以得到相同的计算结果。对用户定义函数（UDF）丰富的支持，确保用户代码能够在SQL查询中执行。如果需要更多的客户逻辑，Flink的DataStream API或DataSetAPI提供更多的底层控制。此外，Flink的Gelly库面向批处理数据集提供大规模和高性能图分析的算法和基础功能。 典型的数据分析应用 电信网络的质量监控 在移动应用中分析产品更新和实验评估 消费分析中的专用实时数据分析 大规模图分析 数据管道应用ETL是一种在不同存储系统之间转换和移动数据的普通方式。需要经常周期性的触发ETL任务，从事务数据库系统到分析数据库或数据仓库拷贝数据。 数据管道应用于ETL任务的目的相似。它能够在一个存储系统到另一个存储系统之间转换和扩充数据。但是，它以连续的流模式运行而不是周期性的调用。因此，它能够从持续产生数据的数据源读取记录，并以较低的延迟将之移动到目的地。例如，数据管道可以监控文件系统目录的新文件，并将新文件数据记录到时间日志中。另一个应用能够将事件记录到数据库中，或者递增地构建和优化查询索引。 下图描述了周期性ETL任务和持续数据管道之间的区别。 数据管道的优势连续数据管道比周期性ETL任务的明显优势在于移动数据的低延迟。而且，因为数据管道可以连续地消费和发出数据，它具有更多使用场景下的通用性。 Flink对数据管道的支持Flink的SQL接口（或表API）和对UDF的支持可以解决大量的普通数据转换和扩充任务。更通用的DataStream API可以实现数据管道更高级的需求。Flink提供丰富的与其他存储系统的连接器，例如Kafka、Kinesis、Elasticsearch、JDBC数据库。它也为文件系统提供连续数据源，用来以时间段的形式监控数据目录。 典型的数据管道应用 电子商务中实时查询索引的构建 电子商务中连续的ETL","link":"/2019/01/17/Flink适用场景/"},{"title":"Jeffrey Dean论文：基于学习的索引结构","text":"摘要索引即是模型：B树索引是一个在排序数组中用键映射记录位置的模型，哈希索引是一个在非排序数组中用键映射记录位置的模型，位图索引是一个指示记录是否存在的模型。本文假设现有的索引结构都可以被替换为其他的模型，包括深度学习模型，我们称此学习索引。其核心思想是，模型可以学习查找键的顺序或结构，并使用这个信息有效地预测记录的位置或是否存在。我们从理论上分析了在何种情况下学习索引优于传统索引，并且概况了设计学习索引的诀窍和调整。初步结果表明，通过神经网络，学习索引比高速缓存优化的B树速度快70%，并且在多个真实的结果集上节省一个数量级的内存。更重要的是，我们相信通过深度学习模型取代数据管理系统的核心组件对未来系统设计有着深远的影响，并且这只是未来应用的冰山一角。","link":"/2018/06/20/Jeffrey-Dean论文：基于学习的索引结构/"},{"title":"LVM简单介绍","text":"原文见：http://www.debian-administration.org/articles/410 LVM允许你用一种有效地方法创建和管理服务器上的存储，按照意愿增加、移除、改变分区大小。刚开始接触LVM会使初学者感到疑惑，而这篇文字就用简单的方式讲述LVM的基本知识。 以下术语对你学习使用LVM很重要，是你必须知道的： 物理卷（physical volumes） 这是你的物理硬盘或磁盘分区，例如/dev/hda或/dev/hdb1.这是你在mounting/unmounting时经常使用的。利用LVM能够合并多个物理卷成为卷群（volume groups） 卷群（volume groups） 卷群由真实的物理卷组成，它能够用来创建逻辑卷（logical volumes）。你能够创建、改变大小、移除、使用逻辑卷。可以认为卷群是一个由任意数量的物理卷组成的虚拟分区。 逻辑卷（logical volumes） 逻辑卷是你最终在系统上挂载的，它们能够快速的添加、移除、改变大小。因为逻辑卷由卷群组成，所有它的空间能大于组成它的单一物理卷的空间。例如由4个5G大小的硬盘（逻辑卷）能够组合成20G的卷群，所有你能在此基础上创建两个10G的逻辑卷。 在逻辑层面上，他们的概念层次自上而下如图所示。 创建卷群（Volume Group）使用LVM你至少需要一个分区，并使用LVM进行初始化。然后把它加入一个卷群。怎样操作呢？ 在我的例子中，我的笔记本有如下设置： Name Flags Part Type FS Type [Label] Size (MB)123hda1 Boot Primary Linux ext3 [/] 8000.01 hda2 Primary Linux swap / Solaris 1000.20hda3 Primary Linux 31007.57 这里我有一个7GB的根分区，用来存储我的Debian GNU/Linux安装。我有一个28GB的分区用来给LVM使用。按照我的设置，我能用LVM创建一个专用的/home分区，而且我能够在空间不足时扩展/home分区增大空间。 在这个例子中hda1、hda2、hda3是物理卷。我对hda3进行物理卷初始化： 1root@lappy:~# pvcreate /dev/hda3 如果你想合并硬盘或者分区，你可以像这样操作： 12root@lappy:~# pvcreate /dev/hdbroot@lappy:~# pvcreate /dev/hdc 一旦你初始化好了分区或磁盘驱动器，我们用这些物理卷来创建卷群：1root@lappy:~# vgcreate skx-vol /dev/hda3 这里skx-vol是卷群的名称。如果你想跨两个磁盘创建卷群，你可以使用“vgcreate skx-vol /dev/hdb /dev/hdc”的方式。 如果你操作正确，你可以通过vgscan命令查看： 123root@lappy:~# vgscan Reading all physical volumes. This may take a while... Found volume group \"skx-vol\" using metadata type lvm2 这样我们就创建好了一个名为skx-vol的卷群，我们下面开始使用他。 创建和使用逻辑卷（Working with logical volumes）我们真正想要的是创建一个能够挂载和使用的逻辑卷。如果以后我们的逻辑卷空间不足，可以对其空间进行重新分配，增加新的存储空间。当然这取决于你算则的文件系统类型。 我们将创建一个名为test的卷： 12root@lappy:~# lvcreate -n test --size 1g skx-volLogical volume \"test\" created 这个命令创建了一个大小为1G名为test的逻辑卷，它在LVM卷群skx-vol之上。 现在这个逻辑卷可以通过/dev/skx-vol/test进行操作，它能像其他分区一样进行格式化和挂载： 123root@lappy:~# mkfs.ext3 /dev/skx-vol/testroot@lappy:~# mkdir /home/testroot@lappy:~# mount /dev/skx-vol/test /home/test 很酷是吧？ 下面的操作将更有意思！我们假设这个test分区空间已满，我们要对这个分区进行扩大。首先，我们可以通过lvdisplay命令查看当前硬盘的状态： 1234567891011121314root@lappy:~# lvdisplay --- Logical volume --- LV Name /dev/skx-vol/test VG Name skx-vol LV UUID J5XlaT-e0Zj-4mHz-wtET-P6MQ-wsDV-Lk2o5A LV Write Access read/write LV Status available # open 0 LV Size 1.00 GB Current LE 256 Segments 1 Allocation inherit Read ahead sectors 0 Block device 254:0 我们能看到test分区大小为1G。在我们对它重新分配空间时记得要先卸载这个卷（unmount）1234root@lappy:~# umount /home/test/root@lappy:~# lvextend -L+1g /dev/skx-vol/test Extending logical volume test to 2.00 GBLogical volume test successfully resized (注意：ext3文件系统可以在挂载状态重划大小，但是我建议还是先把它卸载掉，这样可以免除不必要的担心) 在此通过lvdisplay命令查看，test卷已经重新分配了大小：1234567891011121314root@lappy:~# lvdisplay --- Logical volume --- LV Name /dev/skx-vol/test VG Name skx-vg LV UUID uh7umg-7DqT-G2Ve-nNSX-03rs-KzFA-4fEwPX LV Write Access read/write LV Status available # open 0 LV Size 2.00 GB Current LE 512 Segments 1 Allocation inherit Read ahead sectors 0 Block device 254:0 需要着重提醒的是，即使这个卷的空间重新分配，但是它上面的ext3文件系统却没有改变，我们需要对文件系统重划大小满足扩大了的卷：12root@lappy:~# e2fsck -f /dev/skx-vol/test root@lappy:~# resize2fs /dev/skx-vol/test 然后对test卷重新挂载后你会发现它的空间变成了2G。 当然你也可以通过lvremove操纵移除这个卷：123root@lappy:~# lvremove /dev/skx-vol/testDo you really want to remove active logical volume \"test\"? [y/n]: yLogical volume \"test\" successfully removed 挂载逻辑卷（Mounting Logical Volumes）在之前的章节，我们使用如下的命令来挂载一个逻辑卷：1mount /dev/skx-vol/test /home/test 如果你想在开机引导的时候就挂载你的分区，你可以编辑/etc/fstab文件，增加如下内容：12/dev/skx-vol/home /home ext3 noatime 0 2/dev/skx-vol/backups /backups ext3 noatime 0 2","link":"/2018/10/22/LVM简单介绍/"},{"title":"Minio Java SDK for Amazon S3 Compatible Cloud Storage","text":"Minio Java客户端SDK提供了访问任何Amazon S3兼容对象存储服务的简单API接口。 环境要求Java1.8以上版本 Maven配置12345&lt;dependency&gt; &lt;groupId&gt;io.minio&lt;/groupId&gt; &lt;artifactId&gt;minio&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt;&lt;/dependency&gt; gradle配置123dependencies &#123; compile 'io.minio:minio:6.0.0'&#125; API接口使用例子文件上传这个例子连接到对象存储服务，在服务器上创建一个bucket，并且上传一个文件到bucket。 连接服务器需要3个参数： Params Description Endpoint URL to object storage service. Access Key Access key is like user ID that uniquely identifies your account. Secret Key Secret key is the password to your account. FileUploader.java1234567891011121314151617181920212223242526272829303132import java.io.IOException;import java.security.NoSuchAlgorithmException;import java.security.InvalidKeyException;import org.xmlpull.v1.XmlPullParserException;import io.minio.MinioClient;import io.minio.errors.MinioException;public class FileUploader &#123; public static void main(String[] args) throws NoSuchAlgorithmException, IOException, InvalidKeyException, XmlPullParserException &#123; try &#123; // Create a minioClient with the Minio Server name, Port, Access key and Secret key. MinioClient minioClient = new MinioClient(\"https://play.minio.io:9000\", \"Q3AM3UQ867SPQQA43P2F\", \"zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG\"); // Check if the bucket already exists. boolean isExist = minioClient.bucketExists(\"asiatrip\"); if(isExist) &#123; System.out.println(\"Bucket already exists.\"); &#125; else &#123; // Make a new bucket called asiatrip to hold a zip file of photos. minioClient.makeBucket(\"asiatrip\"); &#125; // Upload the zip file to the bucket with putObject minioClient.putObject(\"asiatrip\",\"asiaphotos.zip\", \"/home/user/Photos/asiaphotos.zip\"); System.out.println(\"/home/user/Photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.\"); &#125; catch(MinioException e) &#123; System.out.println(\"Error occurred: \" + e); &#125; &#125;&#125; 编译FileUploader1javac -cp \"minio-6.0.0-all.jar\" FileUploader.java 运行FileUploader12345java -cp \"minio-6.0.0-all.jar:.\" FileUploader/home/user/Photos/asiaphotos.zip is successfully uploaded as asiaphotos.zip to `asiatrip` bucket.mc ls play/asiatrip/[2016-06-02 18:10:29 PDT] 82KiB asiaphotos.zip API参考The full API Reference is available here. Complete API Reference API Reference: Bucket Operations makeBucket listBuckets bucketExists removeBucket listObjects listIncompleteUploads API Reference: Object Operations getObject putObject copyObject statObject removeObject removeIncompleteUpload API Reference: Presigned Operations presignedGetObject presignedPutObject presignedPostPolicy API Reference: Bucket Policy Operations getBucketPolicy setBucketPolicy Full ExamplesFull Examples: Bucket Operations ListBuckets.java ListObjects.java BucketExists.java MakeBucket.java RemoveBucket.java ListIncompleteUploads.java Full Examples: Object Operations PutObject.java PutObjectEncrypted.java GetObject.Java GetObjectEncrypted.Java GetPartialObject.java RemoveObject.java RemoveObjects.java StatObject.java Full Examples: Presigned Operations PresignedGetObject.java PresignedPutObject.java PresignedPostPolicy.java Full Examples: Bucket Policy Operations SetBucketPolicy.java GetBucketPolicy.Java Full Examples: Server Side Encryption CopyObjectEncrypted.java CopyObjectEncryptedKms.java CopyObjectEncryptedS3.java PutGetObjectEncrypted.java PutObjectEncryptedKms.java PutObjectEncryptedS3.java Explore Further Complete Documentation Minio Java Client SDK API Reference Build your own Photo API Service - Full Application Example","link":"/2019/01/28/Minio-Java-SDK-for-Amazon-S3-Compatible-Cloud-Storage/"},{"title":"Minio现代数据湖：第一部分","text":"现代数据湖构筑于云存储之上，帮助用户取得扩展性和对象存储经济性的平衡，并简化全部的数据存储和分析流程。 在此系列的第一部分，我们将探讨对象存储与其他存储方式的不同，以及为何对象存储对于数据湖的构建意义重大。 什么是对象存储对象存储泛指一种平台组织存储单元的方式，这种组织方式成为对象。每个对象通常包括以下三部分： 数据本身。数据可以是任何想要存储的东西，从一张家庭照片到有40万页的火箭建造指南。 数量可扩展的元数据。元数据被创建对象存储的人所定义，它包括描述数据内容的上下文信息、数据的用途、机密性、以及与数据使用方式相关的任何其他内容。 全局唯一的标识符。标识符是一个赋予对象的地址，可以通过标识符在分布式系统中找到该对象。用这种方式，能够在不知道其物理地址的情况下找到数据。它可能位于一个数据中心的不同主机上甚至是不同的数据中心中。 块存储和对象存储 对象存储不像块存储，它不将文件切分为原始数据块。相反，数据被作为一个包括真实文件数据、元数据和唯一标识符的对象进行存储。注意元数据可以包括与对象有关的任何文本。 现代企业数据中心越来越像私有云，使用对象存储作为事实上的存储标准。对象存储可以以高可用、高容错的方式提供更好的扩展经济性，确保数据可以全球可用。 以下是块存储和对象存储的详细对比： 为何对象存储如此重要Hadoop一度成为数据湖选型的主宰。但是现在技术发展迅速，已经出现了更加先进的解决方案。现代数据湖基于对象存储，使用包括Apache Spark、Presto、TensorFlow等工具进行高级分析和机器学习。 让我们回顾过去，看看事情是如何改变的。Hadoop出现在21世纪前十年，并且在过去5年内非常流行。事实上，因为许多公司都得益于开源，5、6年前的许多大数据项目都基于Hadoop。 简单的说，Hadoop有两个主要的功能： 分布式文件系统（HDFS）用来存储数据 处理框架（MapReduce）可以并行地处理这些数据 越来越多的组织开始希望处理和分析他们拥有的全部数据，而不只是一部分。所以，Hadoop逐渐流行因为它可以存储和处理新的数据源，包括系统日志、点击流、传感器和机器生成的数据。 在2008-2009年左右，游戏改变了。当时，Hadoop的主要设计目标是利用通用商业硬件构建一个本地集群，从而以较低的成本存储和处理这些新数据。Hadoop非常适合这个目标。这在当时是正确的，但在今天却不是。 Spark的出现开源世界最美妙的是新事物不断涌现，最坏的事情也是如此。 我的意思是，随着最新的、最大的、最好的新项目陆续推出，你不得不追随潮流。让我们看看现在发生了什么。 近年来，比MapReduce更新的扩建不断涌现：Apache Spark。从概念是看，它与MapReduce相似。但是一个重要的不同是，它优化了Hadoop原本基于硬盘的计算方式，改为基于内存的数据计算。这导致了运行在Spark上的算法运行速度显著加快。 事实上，今天要开始建设一个大数据项目，如果没有强制要求与历史遗留的Hadoop或MapReduce应用协同的话，Spark是最佳的方案。你仍然需要持久化数据，因为Spark已经捆绑了许多Hadoop发行版，大多数本地集群使用HDFS。但是，随着云计算的兴起，出现了持久化数据更好的方式：对象存储。 对象存储不同于文件存储和块存储。因为它将数据存储为一个对象，而不是一个构成文件的数据块。元数据与该文件绑定，就不需要在文件存储中的使用的层级结构，并且没有限制元数据的大小。任何内容都可以存储在易于扩展的扁平的地址空间。 对象存储的诸多优势根本上来讲，对象存储在大文件和高流量吞吐方面性能优越。它可以运行数据跨区域存储，容量可以无限扩展至P字节甚至更大，它提供可以帮助取回文件的自定义的元数据。 许多公司，特别是运营私有云环境的公司，将对象存储视作达到合规要求的一种面向非结构化大数据的长期存储库。但是合规性并不是唯一的原因。公司可以使用对象存储在Facebook上保存照片，在Spotify上保存歌曲，在Dropbox上保存文件。 事实上对象存储让人们更为青睐的原因是成本优势。使用对象存储的成本较使用HDFS的块存储成本低许多。你可以看看大部分的云计算提供商，对象存储的成本只是块存储的1/3到1/5。这意味着在HDFS存储相同容量的数据要比使用对象存储贵3-5倍。 综上所述，Spark是一个比MapReduce更快的框架，对象存储是比使用块存储的HDFS更便宜。下面我们把这两部分放在一起，形成一个新的架构。 对象存储+Spark的优势我们特别推荐的是在云环境中基于对象存储和Spark构建数据湖。对象存储和Spark的解决方案比基于Hadoop的数据湖解决方案更具扩展性、成本更低。 在云环境中将对象存储与Spark结合，比经典的Hadoop/MapReduce配置更有弹性。如果你曾经在Hadoop集群中增加或移除节点，你肯定理解我的意思。即使配置也并不容易，但是在云环境下这个工作并不重要。 灵活性的另一个方面。如果要使用Hadoop增加更多的存储容量，你需要增加更多的（计算）节点。不论你是否需要计算能力，你都在需要存储能力的时候，无形中增加了计算容量。 对象存储架构则完全不同。如果需要更多的计算能力，只需要扩充Spark集群，而使存储集群保持不变。如果需要存储新的数据，只需要扩充对象存储即可。在云环境下，计算和存储不只是弹性的，并且是独立弹性的。这样非常不错，因为我们需要计算和存储具备独立的弹性。 对象存储+Spark业务敏捷要获得性能提升有许多方式。 稳定性、可靠性更低的TCO","link":"/2018/12/26/Minio现代数据湖：第一部分/"},{"title":"Mvnw 简介","text":"背景maven是一款非常流行的java项目构建软件，它集项目的依赖管理、测试用例运行、打包、构件管理于一身，是我们工作的好帮手，maven飞速发展，它的发行版本也越来越多，如果我们的项目是基于maven构件的，那么如何保证拿到我们项目源码的同事的maven版本和我们开发时的版本一致呢，可能你认为很简单，一个公司嘛，规定所有的同事都用一maven版本不就万事大吉了吗？一个组织内部这是可行的，要是你开源了一个项目呢？如何保证你使用的maven的版本和下载你源码的人的maven的版本一致呢，这时候mvnw就大显身手了。mvnw 全名是maven wrapper,它的原理是在maven-wrapper.properties文件中记录你要使用的maven版本，当用户执行mvnw clean 命令时，发现当前用户的maven版本和期望的版本不一致，那么就下载期望的版本，然后用期望的版本来执行mvn命令，比如刚才的mvn clean。为项目添加mvnw支持很简单，有两种方式： 方法一，在Pom.Xml中添加Plugin声明：12345&lt;plugin&gt; &lt;groupId&gt;com.rimerosolutions.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;wrapper-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.0.4&lt;/version&gt;&lt;/plugin&gt; 这样当我们执行mvn wrapper:wrapper 时，会帮我们生成mvnw.bat, mvnw, maven/maven-wrapper.jar, maven/maven-wrapper.properties这些文件。然后我们就可以使用mvnw代替mvn命令执行所有的maven命令，比如mvnw clean package 方法二，直接执行Goal12#表示我们期望使用的maven的版本为3.3.3mvn -N io.takari:maven:wrapper -Dmaven=3.3.3 产生的内容和第一种方式是一样的，只是目录结构不一样，maven-wrapper.jar和maven-wrapper.properties在”.mvn/wrapper”目录下 使用的注意事项1、由于我们使用了新的maven ,如果你的settings.xml没有放在当前用户下的.m2目录下，那么执行mvnw时不会去读取你原来的settings.xml文件2、在mvnw.bat中有如下的一段脚本if exist “%M2_HOME%\\bin\\mvn.cmd” goto init意思是如果找到mvn.cmd就执行初始化操作，但是maven早期版本不叫mvn.cmd,而是叫mvn.bat,所以会报”Error: M2_HOME is set to an invalid directory”错误，改成你本地的maven的匹配后缀就好了。","link":"/2018/11/07/Mvnw-简介/"},{"title":"ORA-00257: 归档程序错误。在释放之前仅限于内部连接","text":"由于近期全国测绘统计年报进入集中上报阶段，造成数据库压力过大，且频繁的数据读写造成数据库归档日志增长非常快，月初才清理的归档日志空间已满，造成数据库报错。 错误提示：ORA-00257: 归档程序错误。在释放之前仅限于内部连接 Oracle数据库版本：Oracle 11g R2 这是Oracle数据库较为常见的一类错误，错误原因是数据库归档日志已满，造成数据库无法提供服务。处理方案就是扩大归档日志空间并且使用RMAN清理已有的归档日志。 操作步骤： 从服务器本地登录数据库： 123456789101112Microsoft Windows [版本 6.1.7601]版权所有 (c) 2009 Microsoft Corporation。保留所有权利。C:\\Users\\Administrator&gt;sqlplus / as sysdbaSQL*Plus: Release 11.2.0.1.0 Production on 星期三 4月 24 20:10:13 2019Copyright (c) 1982, 2010, Oracle. All rights reserved.连接到:Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing options 查询归档日志使用情况： 123456789SQL&gt; select * from v$recovery_file_dest;NAME--------------------------------------------------------------------------------SPACE_LIMIT SPACE_USED SPACE_RECLAIMABLE NUMBER_OF_FILES----------- ---------- ----------------- ---------------D:\\app\\Administrator\\flash_recovery_area 1.0737E+10 1.0729E+10 0 281 或 1select * from v$flash_recovery_area_usage; 查询归档日志设置大小 12345SQL&gt; show parameter db_recovery_file_dest_sizeNAME TYPE VALUE------------------------------------ ----------- ------------------------------db_recovery_file_dest_size big integer 10G 扩大归档日志空间设置 123SQL&gt; alter system set db_recovery_file_dest_size=20G scope=both;系统已更改。 确认设置情况 12345SQL&gt; show parameter db_recovery_file_dest_sizeNAME TYPE VALUE------------------------------------ ----------- ------------------------------db_recovery_file_dest_size big integer 20G 123456789SQL&gt; select * from v$recovery_file_dest;NAME--------------------------------------------------------------------------------SPACE_LIMIT SPACE_USED SPACE_RECLAIMABLE NUMBER_OF_FILES----------- ---------- ----------------- ---------------D:\\app\\Administrator\\flash_recovery_area 2.1475E+10 6496842240 0 167 进入RMAN 123456789101112Microsoft Windows [版本 6.1.7601]版权所有 (c) 2009 Microsoft Corporation。保留所有权利。C:\\Users\\Administrator&gt;rman target /恢复管理器: Release 11.2.0.1.0 - Production on 星期三 4月 24 20:49:42 2019Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved.连接到目标数据库: ORL (DBID=1362993468)RMAN&gt; 找出状态为expired的归档日志 1RMAN&gt;crosscheck archivelog all; 删除3天前的归档日志，腾出有效空间 1RMAN&gt; DELETE ARCHIVELOG ALL COMPLETED BEFORE &apos;SYSDATE-3&apos;; 在plsql界面关闭数据库 12SQL&gt; shutdown abort;ORACLE 例程已经关闭。 启动并装载数据库 123456789SQL&gt; startup mount;ORACLE 例程已经启动。Total System Global Area 1732554752 bytesFixed Size 1378008 bytesVariable Size 1065355560 bytesDatabase Buffers 654311424 bytesRedo Buffers 11509760 bytes数据库装载完毕。 打开数据库服务 123SQL&gt; alter database open;数据库已更改。","link":"/2019/04/24/ORA-00257-归档程序错误。在释放之前仅限于内部连接/"},{"title":" ORA-00604: 递归 SQL 级别 1 出现错误 ORA-01653: 表 SYS.AUD$ 无法通过 8192 (在表空间 SYSTEM 中) 扩展","text":"今日遇到OA数据库报错，报错内容如下： 1234ORA-00604: 递归 SQL 级别 1 出现错误ORA-01653: 表 SYS.AUD$ 无法通过 8192 (在表空间 SYSTEM 中) 扩展ORA-02002: 写入审计线索时出错ORA-00604: 递归 SQL 级别 1 出现错误 这种情况是遇到了表空间已满的情况，由于Windows版32位的Oracle限制每个表空间存储文件最大32G，所以必须手动增加表空间存储文件进行扩容。步骤如下： 本地模式登录数据库 12345678910C:\\Users\\Administrator&gt;sqlplus / as sysdbaSQL*Plus: Release 11.2.0.1.0 Production on Thu Oct 25 16:16:49 2018Copyright (c) 1982, 2010, Oracle. All rights reserved.Connected to:Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - ProductionWith the Partitioning, OLAP, Data Mining and Real Application Testing options 查看表空间情况 123456789101112131415161718192021222324252627282930313233SQL&gt; SELECT UPPER(F.TABLESPACE_NAME) &quot;表空间名&quot;, D.TOT_GROOTTE_MB &quot;表空间大小(M)&quot;, D.TOT_GROOTTE_MB - F.TOTAL_BYTES &quot;已使用空间(M)&quot;, TO_CHAR(ROUND((D.TOT_GROOTTE_MB - F.TOTAL_BYTES) / D.TOT_GROOTTE_MB * 100,2),&apos;990.99&apos;) &quot;使用比&quot;, F.TOTAL_BYTES &quot;空闲空间(M)&quot;, F.MAX_BYTES &quot;最大块(M)&quot; FROM (SELECT TABLESPACE_NAME, ROUND(SUM(BYTES) / (1024 * 1024), 2) TOTAL_BYTES, ROUND(MAX(BYTES) / (1024 * 1024), 2)MAX_BYTES FROM SYS.DBA_FREE_SPACE GROUP BY TABLESPACE_NAME) F, (SELECT DD.TABLESPACE_NAME, ROUND(SUM(DD.BYTES) / (1024 * 1024), 2) TOT_GROOTTE_MB FROM SYS.DBA_DATA_FILES DD GROUP BY DD.TABLESPACE_NAME) D WHERE D.TABLESPACE_NAME = F.TABLESPACE_NAME ORDER BY 4 DESC;表空间名 表空间大小(M) 已使用空间(M) 使用比 空闲空间(M)------------------------------ ------------- ------------- ------- ----------- 最大块(M)----------SYSTEM 65482 65409.19 99.89 72.81 47OA 1248 1184.94 94.95 63.06 63SYSAUX 920 852.06 92.62 67.94 59表空间名 表空间大小(M) 已使用空间(M) 使用比 空闲空间(M)------------------------------ ------------- ------------- ------- ----------- 最大块(M)----------UNDOTBS1 1000 420.5 42.05 579.5 353USERS 5 1.31 26.20 3.69 3.69 查看表空间文件名和是否自动增长 123456789101112131415161718192021222324252627282930313233SQL&gt; SELECT FILE_NAME,TABLESPACE_NAME,AUTOEXTENSIBLE FROM dba_data_files;FILE_NAME--------------------------------------------------------------------------------TABLESPACE_NAME AUT------------------------------ ---D:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\USERS01.DBFUSERS YESD:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\UNDOTBS01.DBFUNDOTBS1 YESD:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\SYSAUX01.DBFSYSAUX YESFILE_NAME--------------------------------------------------------------------------------TABLESPACE_NAME AUT------------------------------ ---D:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\SYSTEM01.DBFSYSTEM YESD:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\OA.DBFOA YESD:\\APP\\ADMINISTRATOR\\ORADATA\\ORCL\\SYSTEM02.DBFSYSTEM NO6 rows selected. 增加表空间的存储文件，system03.dbf 1234SQL&gt; ALTER TABLESPACE SYSTEM ADD DATAFILE &apos;D:\\app\\Administrator\\oradata\\orcl\\SYSTEM03.DBF&apos; SIZE 32752M;Tablespace altered. 执行完成后查看表空间的使用情况 123456789101112131415161718192021222324252627282930313233SQL&gt; SELECT UPPER(F.TABLESPACE_NAME) &quot;表空间名&quot;, D.TOT_GROOTTE_MB &quot;表空间大小(M)&quot;, D.TOT_GROOTTE_MB - F.TOTAL_BYTES &quot;已使用空间(M)&quot;, TO_CHAR(ROUND((D.TOT_GROOTTE_MB - F.TOTAL_BYTES) / D.TOT_GROOTTE_MB * 100,2),&apos;990.99&apos;) &quot;使用比&quot;, F.TOTAL_BYTES &quot;空闲空间(M)&quot;, F.MAX_BYTES &quot;最大块(M)&quot; FROM (SELECT TABLESPACE_NAME, ROUND(SUM(BYTES) / (1024 * 1024), 2) TOTAL_BYTES, ROUND(MAX(BYTES) / (1024 * 1024), 2)MAX_BYTES FROM SYS.DBA_FREE_SPACE GROUP BY TABLESPACE_NAME) F, (SELECT DD.TABLESPACE_NAME, ROUND(SUM(DD.BYTES) / (1024 * 1024), 2) TOT_GROOTTE_MB FROM SYS.DBA_DATA_FILES DD GROUP BY DD.TABLESPACE_NAME) D WHERE D.TABLESPACE_NAME = F.TABLESPACE_NAME ORDER BY 4 DESC;表空间名 表空间大小(M) 已使用空间(M) 使用比 空闲空间(M)------------------------------ ------------- ------------- ------- ----------- 最大块(M)----------OA 1248 1184.94 94.95 63.06 63SYSAUX 920 852.06 92.62 67.94 59SYSTEM 98234 65474.19 66.65 32759.81 3968表空间名 表空间大小(M) 已使用空间(M) 使用比 空闲空间(M)------------------------------ ------------- ------------- ------- ----------- 最大块(M)----------UNDOTBS1 1000 460.5 46.05 539.5 353USERS 5 1.31 26.20 3.69 3.69 可见SYSTEM表空间已经扩大了，任务完成。","link":"/2018/10/25/ORA-00604-递归-SQL-级别-1-出现错误-ORA-01653-表-SYS-AUD-无法通过-8192-在表空间-SYSTEM-中-扩展/"},{"title":"Presto Thrift Connector","text":"Thrift 连接器使得无需自定义Presto连接器实现即可与外部存储系统相连。 为了使用Thrift连接器与外部系统连接，需要实现PrestoThriftService接口。接下来需要配置Thrift连接器指向一组实现了该接口的服务器——被称为Thrift servers。作为接口实现的一部分，Thrift servers提供元数据，分片方法和数据本身。连接器会随机选择一个可用的服务器用于调取元数据，或者调取分片和数据。假设所有请求都是幂等的，并且可在任何服务器之中随意重试。 Configuration配置Thrift 连接器需要创建一个catalog配置文件etc/catalog/thrift.properties，内容如下： 12connector.name=presto-thriftpresto.thrift.client.addresses=host:port,host:port Multiple Thrift Systems可以定义多个与外部Thrift系统连接的catalog，只需要定义多个不重名的properties文件即可。 配置说明Configuration Properties Property Name Description presto.thrift.client.addresses 服务地址Location of Thrift servers presto-thrift.max-response-size 最大返回数量Maximum size of data returned from Thrift server presto-thrift.metadata-refresh-threads Number of refresh threads for metadata cache presto.thrift.client.max-retries Maximum number of retries for failed Thrift requests presto.thrift.client.max-backoff-delay Maximum interval between retry attempts presto.thrift.client.min-backoff-delay Minimum interval between retry attempts presto.thrift.client.max-retry-time Maximum duration across all attempts of a Thrift request presto.thrift.client.backoff-scale-factor Scale factor for exponential back off presto.thrift.client.connect-timeout Connect timeout presto.thrift.client.request-timeout Request timeout presto.thrift.client.socks-proxy SOCKS proxy address presto.thrift.client.max-frame-size Maximum size of a raw Thrift response presto.thrift.client.transport Thrift transport type (UNFRAMED, FRAMED, HEADER) presto.thrift.client.protocol Thrift protocol type (BINARY, COMPACT, FB_COMPACT) presto.thrift.client.addresses","link":"/2019/10/21/Presto-Thrift-Connector/"},{"title":"Oracle备份的概念和术语","text":"Oracle备份是Oracle运维工作中非常重要的一个环节。Oracle备份的概念比较多，容易引起混淆。下面就具体概念和术语进行一个简单的辨析。 概念辨析 用操作系统命令执行的备份被称为用户管理备份。使用RMAN执行的备份被称为服务器管理的备份。 关闭状态的备份在数据库关闭期间执行，也称为冷备份、一致备份或脱机备份。打开状态的备份在数据库使用期间执行，也称为热备份、非一致备份或联机备份。 打开状态的备份只能在数据库处于归档日志模式下才能进行。 全部备份指的是备份所有数据文件和控制文件。局部备份只备份其子集。在大部分情况下，局部备份只能在数据库处于归档日志模式下才能进行。 完整备份备份所有使用的文件块。增量备份只包括自上次备份以来更改的块。 增量备份可以是累积增量备份或差异增量备份。累积增量备份指自上一次完整备份以来更改的所有块。差异增量备份指自上一次增量备份以来更改的所有块。 使用RMAN Backup命令创建备份1.服务器管理的一致备份RMAN备份脚本： 1234567run &#123; shutdown immediate; startup mount; allocate channel d1 type disk; backup as backupset database format &apos;/home/oracle/Documents/offline_full_whole%U.bus&apos;;alter database open; &#125; 使用RMAN运行一致性备份： 1rman target sys/密码@orcl @/home/oracle/Documents/offline_all.rman 如果遇到报错，可以按照如下方式进行处理： [linuxidc@rhel55 ~]$rman target sys/Oracle@orcl ORA-12514: TNS:listener does not currently know of service requested in connect descriptor[linuxidc@rhel55 ~]$lsnrctl stop [linuxidc@rhel55 ~]$cd $ORACLE_HOME [linuxidc@rhel55 db_1]$vim listener.ora 修改/u01/app/oracle/product/11.2.0/db_1/network/admin/listener.ora文件加上SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = orcl) (ORACLE_HOME = /u01/app/oracle/product/11.2.0/db_1) ) )然后重启lisener服务，就ok了 [linuxidc@rhel55 ~]$lsnrctl start 2. 服务器管理的打开状态的备份使用BACKUP命令执行绝对可靠的打开备份。","link":"/2019/07/23/Oracle备份的概念和术语/"},{"title":"Presto（一）用例指南","text":"Presto不是什么因为网上很多人错误地认为Presto是一个数据库，所以首先从反面定义Presto不是什么更有意义。 不要以为Presto能够解析并执行SQL就将其与标准的数据库混为一谈。Presto并不是一个普遍意义上的关系数据库。它无法替代诸如Mysql、PostgreSQL和Oracle这样的关系型数据库。Presto的设计目标不是为了处理OLTP工作，这与很多其他为优化数据仓库和分析型数据库而设计的数据库一样。 Presto是什么Presto是一个面向大规模数据集通过分布式查询任务进行高效查询的工具。需要处理TB或PB级数据时，会经常使用与Hadoop和HDFS交互的工具。Presto的设计目标是为替换诸如Hive、Pig等使用MapReduce管道查询HDFS的工具提供另一种选择。Presto通过扩展还能够处理其他类型的数据源，包括传统关系型数据看看和其他No-SQL数据库例如Cassandra。 Presto设计用于处理数据仓库和数据分析业务：例如数据分析、聚合大规模数据和制作报表。这类应用场景经常被划分为OLAP范围。 Presto核心概念连接器Connector连接器将Presto适配到一个数据源，例如Hive或关系型数据库。可以将连接器视为一个连接数据库的驱动。它实现了Presto的SPI，Presto依靠SPI标准接口与数据源进行互动。 Presto包括几个内置的连接器：JMX连接器，作为一个系统连接器提供对内置系统表的访问。Hive连接器。以及一个TPCH连接器用于服务TPC-H基准测试数据。第三方开发人员贡献了很多其他连接器，所以Presto能够适配许多不同类型的数据源。 每一个catalog都对应一个连接器。如果你查看catalog配置文件，会看到每个catalog都会有一个必需的配置项connector.name，它被catalog manager用于创建与catalog想匹配的连接器。可以为多个catalog指定相同的连接器，用于访问同类数据库的不同实例。例如你有两个Hive集群，你可以在一个Presto集群中配置两个catalog都使用Hive连接器，是你能够从两个Hive集群中查询数据。 CatalogPresto的catalog包含schema模式和指向数据源的连接器的引用。例如，你可以配置一个JMX catalog，通过JMX连接器提供对JMX信息的访问。Presto可以在一个或多个catalog上运行SQL。其他catalog类型包括连接到Hive数据源的Hive catalog。 当使用Presto在解析表时，使用表的完全限定（fully-qualified）名，完全限定名以catalog为根。例如hive.test_data.test的完全限定名，表示hive库中的test_data模式中的test表。 catalog通过Presto配置文件目录下的属性文件进行定义。 Schemaschema是一种组织表的方式。一个catalog和模式定义了一组可以被查询的表。当使用Presto访问Hive或关系型数据库例如MySQL时，schema跟目标数据库中的具有相同的概念。其他类型的连接器可能会选择以一种对底层数据源有意义的方式将表组织为模式。 Table一个表是一组无序的行，这些行由不同类型的列组成。这与关系型数据库概念相同。源数据到表的映射关系由连接器进行定义。","link":"/2019/08/22/Presto（一）用例指南/"},{"title":"Service mesh概述","text":"Linkerd处理请求的流程： Linkerd应用动态路由规则决定哪一个服务是请求者将要请求的服务。请求被路由到一个生产服务还是一个staging服务？在本数据中心的服务还是云上的服务？正在测试的最新版本服务还是审查过的生产服务？所有这些路由规则是动态配置的，并且能够应用于全部或者部分分片的流量。 找到正确的目标后，Linkerd从相关服务发现端点中获取相应的实例池，实例池可能存在多个。如果此信息与Linkerd实际发现的冲突，则Linkerd决定相信哪一个信息源。 Linkerd选择最有可能快速响应的实例，这取决于多个因素，包括最近请求的延迟时间等。 Linkerd尝试发送请求到这个实例，并记录结果的响应类型和延迟时间。 如果实例宕机、无响应、或者发生处理错误，Linkerd在另一个实例上重试请求（只有它知道请求是幂等的情况下）。 如果实例始终返回错误，Linkerd将其剔除出负载均衡池，并周期性地进行重试（实例可能会顺势失效）。 如果请求超时过后，Linkerd主动地返回失败相应，而不是增加重试的流量。 Linkerd通过测量和分布式跟踪捕获上述行为全部方面，并记录到集中式的度量系统。 以上只是最简单的版本。Linkerd也能够使用TLS通信，执行协议升级、动态转移流量、数据中心间失效切换。","link":"/2019/02/20/Service-mesh概述/"},{"title":"Sharding-Sphere开源分布式数据库中间件","text":"Sharding-Sphere是一套开源的分布式数据库中间件解决方案组成的生态圈，它由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（计划中）这3款相互独立的产品组成。他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。 Sharding-Sphere定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 它与NoSQL和NewSQL是并存而非互斥的关系。NoSQL和NewSQL作为新技术探索的前沿，放眼未来，拥抱变化，是非常值得推荐的。反之，也可以用另一种思路看待问题，放眼未来，关注不变的东西，进而抓住事物本质。关系型数据库当今依然占有巨大市场，是各个公司核心业务的基石，未来也难于撼动，我们目前阶段更加关注在原有基础上的增量，而非颠覆。 Sharding-JDBC定位为轻量级Java框架，在Java的JDBC层提供的额外服务。 它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。 适用于任何基于Java的ORM框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template或直接使用JDBC。 基于任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP等。 支持任意实现JDBC规范的数据库。目前支持MySQL，Oracle，SQLServer和PostgreSQL。 Sharding-Proxy定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前先提供MySQL版本，它可以使用任何兼容MySQL协议的访问客户端(如：MySQL Command Client, MySQL Workbench等)操作数据，对DBA更加友好。 向应用程序完全透明，可直接当做MySQL使用。 适用于任何兼容MySQL协议的客户端。","link":"/2018/10/29/Sharding-Sphere开源分布式数据库中间件/"},{"title":"Services","text":"Kubenetes Pods 不是永生的。当他们生命结束后无法复活。实际上ReplicationController动态的创建和销毁Pods（例如，应用扩展、停用或者回滚更新时）。由于每个Pod拥有各自的Ip地址，所以这些IP地址无法被固定下来。这导致一个问题：如果某些Pods（所谓后端）在kubenetes集群中向其他Pods提供了某些功能，这些前端是怎么样找到并记录与哪一个后端关联的呢？通过Services Kubenetes Service是一组有内在逻辑Pods的抽象，并且定义了如何访问他们的策略，有时称为微服务。一组成为Service的Pods通常使用Label Selector进行定义。 举个例子，假如一个图像处理后端程序运行3个副本。这些副本是可替代的，前端不关心使用哪一个副本。而实际上组成后端的Pods会改变，前端业务无法知道和跟踪后端的列表。Services抽象使这种解耦变得可能。 对于Kubenetes原生的应用，kubenetes提供简单的Endpoints API，每当服务中的Pods改变的时候都会进行更新。对于非原生的应用，kubenetes提供虚拟Ip的网桥，转至后端的Pods。 定义服务Kubenetes服务是一个REST对象，与Pod类似。像所有的REST对象一样，Service 定义能够被提交至apiserver用来创建新的实例。例如，有一组具有“app=MyApp”标签的Pods，对外暴露9376端口。1234567891011kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector:​ app: MyApp ports: - protocol:TCP port: 80 targetPort:9376 这个配置将创建一个新的服务对象，命名为“my-service”，在任何标签是“app=MyApp”的Pod上映射目标端口是9376。这个服务会被分配一个IP地址（有时被称为“cluster IP”），改地址被服务代理使用。这个服务的选择器会持续不断地进行检索，结果会被提交到一样被命名为“my-service”的Endpoints对象。 注意，一个服务能够映射一个入栈端口到一个出栈端口。默认地，目标端口会被设置与上面“port”字段一样的值。更有意思的是，目标端口能够被设置成为一个字符串，引用在后端Pods中一样的端口名。在每个后端pod中，被分配给那个名称的实际端口可以是不同的。这给部署和演化服务提供了很大的灵活性。例如，你可以在下一个后端软件版本中改变pods暴露的端口，而对前端没有影响。 kubenetes服务支持TCP和UDP协议。默认是TCP。 没有选择器的服务服务通常是对kubenetes Pods的访问抽象，但是他们也可以抽象其他类型的后端。例如： 在生产环境中使用外部数据库，而在测试环境中使用自己的数据库。 在另一个命名空间或者另一个集群中将一个服务咨询另一个服务。 将一部分负载迁移到kubenetes中，而其中一些负载在kubenetes外的后端。 在上面的情景下，你可以定义一个没有选择器的服务： 123456789kind: ServiceapiVersion: v1metadata: name: my-servicespec: ports: - protocol: TCP port: 80 targetPort: 9376 因为这个服务没有选择器，相关联的Endpoints对象是不会被创建。你可以手动的映射服务到自己的特定endpoints： 123456789kind: EndpointsapiVersion: v1metadata: name: my-servicesubsets: - addresses: - ip: 1.2.3.4 ports: - port: 9376 注意：Endpoint IP不能设置为回环地址 (127.0.0.0/8) ,本地连接地址(169.254.0.0/16) ，或者本地广播地址(224.0.0.0/24)。 访问一个没有选择器的服务同有选择器的服务一样。流量会被路由至用户定义的endpoints（在上面的例子中是1.2.3.4:9376） 一个ExternalName服务是另一类没有选择器的特殊服务。它不定义任何端口和endpoints。它提供了一种向集群外的外部服务返回别名的方式。 12345678kind: ServiceapiVersion: v1metadata: name: my-service namespace: prodspec: type: ExternalName externalName: my.database.example.com 当查找my-service.prod.svc.CLUSTER主机时，集群DNS服务会返回一个值为my.database.example.com的CNAME记录。访问这个服务的方式跟访问其他服务一样，仅有的不同只是在DNS级别进行重定向，没有进行代理和转发。如果以后决定将数据库迁移到集群中，可以启动pods，添加适当的选择器和端点并改变服务的类型。 虚拟IP和服务代理kubenetes中的每一个节点都会运行kube-proxy。kube-proxy负责实现一种ExternalName以外的虚拟IP形式。在kubenetes1.0版本中，服务是4层的（IP上的TCP/UDP）结构，代理纯粹在用户空间。在kubenetes1.1中，添加了Ingress API表示7层（HTTP）服务，也添加了iptables 代理，并从1.2版本后成为默认的操作模式。在kubenetes1.8-beta.0中，增加了ipvs proxy。 Proxy-mode：userspace在这个模式下，kube-proxy观察kubenetes master添加和删除Service和endpoint对象。对于每一个服务它都会在本地节点打开一个端口（任意选择）。任意与代理端口的连接都会被代理到其中一个服务后端Pods（在endpoints中报告的）。使用哪一个后端pod是基于服务的SessionAffinity决定。最终它会安装iptables规则，这个规则捕获到服务cluster IP（虚拟IP）和端口的流量，并转发流量到代理后端pod的代理端口。默认地，后端选择使用轮询方式。 注意，在上图中clusterIP被写成ServiceIP","link":"/2018/06/12/Services/"},{"title":"TensorFlow实战（一）：简单示例","text":"本文是TensorFlow实战的开篇，请确认好您已经安装好Python3开发环境，并安装设置好NumPy、matplotlib、TensorFlow和Jupyter Notebook。 软件介绍Jupyter Notebookmatplotlibmatplotlib是一个绘图库，它允许用户使用Python创绩动态的、自定义的可视化结果。它与Numpy无缝集成，其绘图结果可直接显示在Jupyter Notebook中。matplotlib也可将数值以图像的形式可视化，这个功能可用于验证图像识别任务的输出，并将神经网络的内部单元可视化。构建在matplotlib之上的其它层，如Seaborn，可用于增强其功能。 Numpy代码示例以下代码可以运行在Jupyter Notebook中： 12345678910import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinea= tf.random_normal([2,20])sess=tf.Session()out=sess.run(a)x,y=outplt.scatter(x,y)plt.show() 成功运行如下图： 我们逐条讲解代码： 用TensorFlow定义一个由随机数组成的2*20矩阵，比将其赋给变量a。 启动tensorFlow Session，并将其赋给一个sess对象。 用sess.run()方法执行对象a，并将输出（NumPy数组）赋给out。 将这个220的矩阵划分成为两个1 10的向量x和y。 利用pyplot模块绘制散点图，x对应横轴，y对应纵轴。","link":"/2019/06/11/TensorFlow实战（一）：简单示例/"},{"title":"spark读取和处理Excel数据","text":"spark-excel是一个使用spark读取Excel2007格式的插件，注意只支持.xlsx格式（.xls不行）。 下面使用pyspark在命令行窗口中进行使用： This package can be added to Spark using the --packages command line option. For example, to include it when starting the spark shell: Spark compiled with Scala 2.121$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.12:0.12.5 Spark compiled with Scala 2.111$SPARK_HOME/bin/spark-shell --packages com.crealytics:spark-excel_2.11:0.12.5 ###读取Excel文件创建一个DataFrame 12345678910111213df = spark.read .format(\"com.crealytics.spark.excel\") .option(\"dataAddress\", \"'My Sheet'!B3:C35\") // Optional, default: \"A1\" .option(\"useHeader\", \"true\") // Required .option(\"treatEmptyValuesAsNulls\", \"false\") // Optional, default: true .option(\"inferSchema\", \"false\") // Optional, default: false .option(\"addColorColumns\", \"true\") // Optional, default: false .option(\"timestampFormat\", \"MM-dd-yyyy HH:mm:ss\") // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff] .option(\"maxRowsInMemory\", 20) // Optional, default None. If set, uses a streaming reader which can help with big files .option(\"excerptSize\", 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from .option(\"workbookPassword\", \"pass\") // Optional, default None. Requires unlimited strength JCE for older JVMs .schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings .load(\"Worktime.xlsx\") 为了简化操作，spark-excel的Scala版本包括一个装饰了spark.read返回的DataFrameReader的implicit，并提供一个.excel方法，接受默认和可选的属性。 12345678910111213141516import org.apache.spark.sql._import com.crealytics.spark.excel._val spark: SparkSession = ???val df = spark.read.excel( useHeader = true, // Required dataAddress = \"'My Sheet'!B3:C35\", // Optional, default: \"A1\" treatEmptyValuesAsNulls = false, // Optional, default: true inferSchema = false, // Optional, default: false addColorColumns = true, // Optional, default: false timestampFormat = \"MM-dd-yyyy HH:mm:ss\", // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff] maxRowsInMemory = 20, // Optional, default None. If set, uses a streaming reader which can help with big files excerptSize = 10, // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from workbookPassword = \"pass\" // Optional, default None. Requires unlimited strength JCE for older JVMs).schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings .load(\"Worktime.xlsx\") ###DataFrame保存为Excel 12345678df.write .format(\"com.crealytics.spark.excel\") .option(\"dataAddress\", \"'My Sheet'!B3:C35\") .option(\"useHeader\", \"true\") .option(\"dateFormat\", \"yy-mmm-d\") // Optional, default: yy-m-d h:mm .option(\"timestampFormat\", \"mm-dd-yyyy hh:mm:ss\") // Optional, default: yyyy-mm-dd hh:mm:ss.000 .mode(\"append\") // Optional, default: overwrite. .save(\"Worktime2.xlsx\") DataFrame Join123joined=excel1.join(excel2, excel1[\"id\"]==excel2[\"id\"]) #或者以下格式joined=excel1.join(excel2, excel1.id==excel2.id)","link":"/2020/02/23/spark读取和处理Excel数据/"},{"title":"为Linux逻辑卷增加物理磁盘空间","text":"测绘资质管理系统的文件附件系统后端使用的是基于Linux逻辑卷群作为文件系统的SMB服务，由于附件容量非常庞大，在虚拟化平台上每个虚拟磁盘最大只能分派2T的容量，所以必须由多个虚拟磁盘共同组成一个大的逻辑卷空间进行使用，目前已分派的8T空间已满，需要继续增加空间以维持文件附件数据的存储。 未来希望将此套NAS服务替换为Minio NAS Gateway，后端使用华为5500系列存储直接提供的NAS服务。 查看逻辑卷组成 pvdisplay 停止SMB服务： service smb stop 扩展卷空间 （假如新增加的物理磁盘为/dev/sdd，空间为2T） vgextend data /dev/sdd lvextend -L +2T /dev/data/file 卸载逻辑卷，检查空间并重新扫描空间 umount /data/file/ e2fsck -f /dev/data/file 非常慢 resize2fs /dev/data/file 查看完成情况 df -h lvdisplay 重新挂载逻辑卷，并重启SMB服务 mount /dev/data/file /data/file df -h service smb start","link":"/2018/12/28/为Linux逻辑卷增加物理磁盘空间/"},{"title":"vim输入补全","text":"当关键字自动补全功能被激活后，VIM会试图猜测我们正在输入的单词，从而省去了手动输入完整单词的麻烦。在插入模式下的ctrl-p和ctrl-n组合键，不仅可以在插入模式下触发VIM的自动补全，而且还可以用它们在补全列表中反向或者正向选择。这两个组合键均会调用关键字自动补全，还有其他几种自动补全功能。 CTRL+n 普通关键字CTRL+n CTRL+x 当前缓冲区关键字CTRL+x CTRL+i 文件关键字CTRL+x CTRL+] 标签文件关键字CTRL+x CTRL+k字典查找CTRL+x CTRL+f 文件名补全CTRL+x CTRL+o全能（Omni）补全 上面两个在一行的都是表示一条命令。当挑出自动补全的菜单之后，如果想关掉，如果发现手动输入会更快，CTRL+e可以终止本次操作。 英文单词补全有时候可能想通过自动补全功能输入某个单词，但是它没有在任何打开的缓冲区，包含文件或者标签文件出现过，。在这种情况下输入上面提到的CTRL+x CTRL+k可用于触发这功能。为了激活这个功能需要为VIM提供一份合适的单词列表，最简单的办法就是运行set spell来激活这个功能，这个功能对于自己要输入一个非常长的单词，很有帮助，只要先输入前几个字母，再利用自动补全功能就OK！ 代码的关键字补全程序代码里面的关键字因为VIM本身就已经支持多种编程语言，所以会自己识别的。 自动补全整行文本VIM可以为整行的文本实现自动补全，ctrl+x ctrl+l,这个作用最妙的就是，我们不用知道要复制的行的具体位置，只需要知道有这么一行文本即可，输入几个字符后，输入上面的命令，一行文字出来了。 自动补全文件名字Vim总是维护着一个当前的工作目录。Vim的文件名自动补全功能，只相对于工作目录的路径进行扩展，而不是相对于当前编辑文件的路径。自己感觉用不着很理解，例子关于网络web应用。","link":"/2019/03/07/vim输入补全/"},{"title":"使用Guava处理文件改名并拷贝的例子，超简单！","text":"今天需要对原有档案文件进行一个处理：需要把原文件名中包括“DD”或“KJ”的pdf文档重新改名为“WS”，并复制到另一个文件夹。如果使用Java文件IO去处理比较繁琐，自己还需要处理Steam等很多接口。这时想到了Java库中的神器——谷歌的基础库Guava，能够提供很多便利的API给用户，可以轻而易举地进行文件操作和字符串处理等，这里把代码记录下来，有空闲可以发几篇Guava的使用。 这里有几点需要额外注意： 文件夹路径的字符串表示，Windows中的路径在字符串双引号中要进行转义\\\\ 寻找路径下的文件，可以使用 Files.fileTreeTraverser().breadthFirstTraversal(rootDir)方法。 使用public String replaceAll(String regex, String replacement)进行字符串的匹配和替换。 123456789101112131415161718192021222324252627282930313233343536373839package org.sbsm;import com.google.common.base.CharMatcher;import com.google.common.io.Files;import java.io.File;import java.io.IOException;/** * Hello world! * */public class App&#123; public static void main( String[] args ) &#123; String sourcePath = \"F:\\\\changeFileName\\\\\"; String targetPath = \"F:\\\\new\\\\\"; int count=0; File rootDir = new File(sourcePath); for(File f: Files.fileTreeTraverser().breadthFirstTraversal(rootDir))&#123; if(f.isFile())&#123; String fName=f.getName(); String newName=fName.replaceAll(\"DD\",\"WS\").replaceAll(\"KJ\",\"WS\"); File newFile = new File(targetPath+newName); try &#123; Files.copy(f,newFile); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; count++; System.out.println(f.getName()+\"处理成功\"+\"newFile:\"+newFile.getName()); &#125; &#125; System.out.println(\"复制文件总数：\"+count); &#125;&#125; 使用Maven构建工程，需要在pom文件中引入Guava： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;21.0&lt;/version&gt;&lt;/dependency&gt; 总共执行了800多个文件的拷贝，比较稳定。","link":"/2018/10/26/使用Guava处理文件改名并拷贝的例子，超简单！/"},{"title":"使用TensorFlow进行神经网络计算","text":"神经网络简介学习目标： 使用 TensorFlow DNNRegressor 类定义神经网络 (NN) 及其隐藏层 训练神经网络学习数据集中的非线性规律，并实现比线性回归模型更好的效果 在之前的练习中，我们使用合成特征来帮助模型学习非线性规律。 一组重要的非线性关系是纬度和经度的关系，但也可能存在其他非线性关系。 现在我们从之前练习中的逻辑回归任务回到标准的（线性）回归任务。也就是说，我们将直接预测 median_house_value。 设置首先加载和准备数据。 12345678910111213141516171819202122232425from __future__ import print_functionimport mathfrom IPython import displayfrom matplotlib import cmfrom matplotlib import gridspecfrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdfrom sklearn import metricsimport tensorflow as tffrom tensorflow.python.data import Datasettf.logging.set_verbosity(tf.logging.ERROR)pd.options.display.max_rows = 10pd.options.display.float_format = '&#123;:.1f&#125;'.formatcalifornia_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv\", sep=\",\")\"\"\"为了保证数据训练集和测试集选取的随机性，故对原数据集进行shuffle\"\"\"california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index)) 12345678910111213141516171819202122232425262728293031323334353637383940def preprocess_features(california_housing_dataframe): \"\"\"Prepares input features from California housing data set. Args: california_housing_dataframe: A Pandas DataFrame expected to contain data from the California housing data set. Returns: A DataFrame that contains the features to be used for the model, including synthetic features. \"\"\" selected_features = california_housing_dataframe[ [\"latitude\", \"longitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]] processed_features = selected_features.copy() # Create a synthetic feature. processed_features[\"rooms_per_person\"] = ( california_housing_dataframe[\"total_rooms\"] / california_housing_dataframe[\"population\"]) return processed_featuresdef preprocess_targets(california_housing_dataframe): \"\"\"Prepares target features (i.e., labels) from California housing data set. Args: california_housing_dataframe: A Pandas DataFrame expected to contain data from the California housing data set. Returns: A DataFrame that contains the target feature. \"\"\" output_targets = pd.DataFrame() # Scale the target to be in units of thousands of dollars. output_targets[\"median_house_value\"] = ( california_housing_dataframe[\"median_house_value\"] / 1000.0) return output_targets 123456789101112131415161718# Choose the first 12000 (out of 17000) examples for training.training_examples = preprocess_features(california_housing_dataframe.head(12000))training_targets = preprocess_targets(california_housing_dataframe.head(12000))# Choose the last 5000 (out of 17000) examples for validation.validation_examples = preprocess_features(california_housing_dataframe.tail(5000))validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))# Double-check that we've done the right thing.print(\"Training examples summary:\")display.display(training_examples.describe())print(\"Validation examples summary:\")display.display(validation_examples.describe())print(\"Training targets summary:\")display.display(training_targets.describe())print(\"Validation targets summary:\")display.display(validation_targets.describe()) 构建神经网络神经网络由 DNNRegressor 类定义。 使用 hidden_units 定义神经网络的结构。hidden_units 参数会创建一个整数列表，其中每个整数对应一个隐藏层，表示其中的节点数。以下面的赋值为例： hidden_units=[3,10] 上述赋值为神经网络指定了两个隐藏层： 第一个隐藏层包含 3 个节点。 第二个隐藏层包含 10 个节点。 如果我们想要添加更多层，可以向该列表添加更多整数。例如，hidden_units=[10,20,30,40] 会创建 4 个分别包含 10、20、30 和 40 个单元的隐藏层。 默认情况下，所有隐藏层都会使用 ReLu 激活函数，且是全连接层。 12345678910def construct_feature_columns(input_features): \"\"\"Construct the TensorFlow Feature Columns. Args: input_features: The names of the numerical input features to use. Returns: A set of feature columns \"\"\" return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features])","link":"/2019/02/12/使用TensorFlow进行神经网络计算/"},{"title":"使用Minio为NAS设置S3兼容的对象存储接口","text":"Minio 网关可以为NAS存储增加Amazon S3兼容的接口。可以同时运行多个minio实例指向同一个共享的NAS卷，作为分布式的对象网关。 Run Minio Gateway for NAS Storage使用Docker将/shared/nasvol替换为挂载NAS的真实路径。 12345docker run -p 9000:9000 --name nas-s3 \\ -e \"MINIO_ACCESS_KEY=minio\" \\ -e \"MINIO_SECRET_KEY=minio123\" \\ -v /shared/nasvol:/container/vol \\ minio/minio gateway nas /container/vol 使用二进制安装123export MINIO_ACCESS_KEY=minioexport MINIO_SECRET_KEY=minio123minio gateway nas /shared/nasvol Test using Minio BrowserMinio 网关有一个内嵌的对象浏览Web页面，地址是http://127.0.0.1:9000 ，如果Minio服务正常启动将会看到如下页面： Test using Minio Client mcmc 提供了Unix命令行与Minio Server进行交互，例如：ls, cat, cp, mirror, diff等。它支持文件系统和S3兼容的云存储服务。 配置mc1mc config host add mynas http://gateway-ip:9000 access_key secret_key 列出NAS上的buckets1234mc ls mynas[2017-02-22 01:50:43 PST] 0B ferenginar/[2017-02-26 21:43:51 PST] 0B my-bucket/[2017-02-26 22:10:11 PST] 0B test-bucket1/","link":"/2019/01/28/使用Minio为NAS设置S3兼容的对象存储接口/"},{"title":"关闭mysql的表名库名大小写敏感开关","text":"原来Linux下的MySQL默认是区分表名大小写的，通过如下设置，可以让MySQL不区分表名大小写：1、用root登录，修改 /etc/my.cnf；2、在[mysqld]节点下，加入一行： lower_case_table_names=13、重启MySQL即可；其中 lower_case_table_names=1 参数缺省地在 Windows 中这个选项为 1 ，在 Unix 中为 0，因此在window中不会遇到的问题，一旦一直到linux就会出问题的原因（尤其在mysql对表起名时是无法用大写字母的，而查询用了大写字母却会出查不到的错误，真是弄的莫名其妙）","link":"/2019/04/16/关闭mysql的表名库名大小写敏感开关/"},{"title":"使用sed加速使用Docker镜像构建Maven项目的速度","text":"一般我们使用如下的Dockerfile构建Maven项目： 123456FROM maven:3.6-jdk-8-slim AS builder# Get Click Count job and compile itCOPY ./java/flink-playground-clickcountjob /opt/flink-playground-clickcountjobWORKDIR /opt/flink-playground-clickcountjobRUN mvn clean install 但是，这里默认使用的是国外的Maven中央仓库，下载依赖项非常慢，可以考虑替换为国内阿里镜像。所以需要修改Maven的配置文件，加上国内的Mirror。在Dockerfile里可以使用sed命令向Maven的settings.xml文件写入响应的mirror配置字符串，具体如下： 12345678FROM maven:3.6-jdk-8-slim AS builder# Get Click Count job and compile itCOPY ./java/flink-playground-clickcountjob /opt/flink-playground-clickcountjobWORKDIR /opt/flink-playground-clickcountjob#在maven配置文件中加入镜像mirror配置RUN sed -i -e 's/&lt;mirrors&gt;/&amp; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;\\/id&gt; &lt;url&gt;http:\\/\\/maven.aliyun.com\\/nexus\\/content\\/groups\\/public&lt;\\/url&gt; &lt;mirrorOf&gt;*&lt;\\/mirrorOf&gt; &lt;\\/mirror&gt;/' /usr/share/maven/conf/settings.xml RUN mvn clean install 这里需要注意的是sed替换命令格式一般为 1sed 's/要被取代的字串/新的字串/g'","link":"/2020/02/26/加速使用Docker镜像构建Maven项目的速度/"},{"title":"基于Gitlab和Kubernetes的持续部署（三）——将镜像部署到Kubernetes集群","text":"本文将使用Gitlab CI的Kubenetes集群特性将前面构建的容器镜像部署到Kubernetes。 基本环境 kubernetes集群 Gitlab实例 开启Gitlab Container Registry 配置并启用Gitlab runner：CI runner必须可与Kubernetes APIserver通信。 Kubectl配置Kubernetes集群访问 Kubernetes ServiceAccount 有特殊的权限。 Setp1. 从Kubernetes获取ServiceAccount TokenKubernetes1.6及以上版本增加了基于角色的访问控制（RBAC），ServiceAccount需要正确的权限来部署你想要的名称空间。使用kubeadm初始化的1.6及以上版本的Kubernetes集群，已经默认为API Server开启了RBAC，可以查看Master Node上API Server的静态Pod定义文件： 12[root@Kubernetes ~]# cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep RBAC - --authorization-mode=Node,RBAC Kubernetes1.5及以下版本只需要创建一个ServiceAccount或者使用一个选定名称空间中已存在的ServiceAccount。 首先考虑为部署到kubernetes里的Gitlab项目创建一个专门的名称空间： 12345678910111213[root@Kubernetes ~]# kubectl get namespaceNAME STATUS AGEdefault Active 164dkube-public Active 164dkube-system Active 164d[root@Kubernetes ~]# kubectl create namespace devopsnamespace \"devops\" created[root@Kubernetes pki]# kubectl get namespacesNAME STATUS AGEdefault Active 161ddevops Active 14skube-public Active 161dkube-system Active 161d 获取DevOps名称空间下默认的ServiceAccount Token 1234567891011121314151617[root@Kubernetes pki]# kubectl get -n devops secretNAME TYPE DATA AGEdefault-token-ph7sj kubernetes.io/service-account-token 3 2m[root@Kubernetes pki]# kubectl get -n devops secret default-token-ph7sj -o yamlapiVersion: v1data: ca.crt: [REDACATED] namespace: [REDACATED] token: [THIS IS YOUR TOKEN BASE64 ENCODED]kind: Secretmetadata: annotations: kubernetes.io/service-account.name: default [...] name: default-token-nmx1q namespace: presentation-gitlab-k8s [...] 因为Token是进行了base64编码的，索引需要对Token进行base64解码 12$ echo YOUR_TOKEN_HERE | base64 -dYOUR_DECODED_TOKEN Setp2. 获取Kubernetes的CA授权Kubernetes的授权在文件夹/etc/kubernetes/pki目录（我的是V1.9版本） 1234$ cat /etc/kubernetes/pki/ca.crt-----BEGIN CERTIFICATE-----[REDACATED]-----END CERTIFICATE----- Step3. 在Gitlab CI里创建Kubernetes集群需要ServiceAccount Token（step1.）、CA 授权（step2.）、Kubernetes API地址和需要运行应用的名称空间。在Gitlab中选择CI/CD-&gt;Kubernetes，进入如下页面进行设置： 其中Kubernetes Cluster name可以任意设置，Project namespace是可选项，此处设置为你需要创建应用的Kubernetes名称空间，我使用的名称空间是DevOps。 Step4. 为项目添加.gitlab-ci.yml我的项目是一个简单的Spring-boot Java项目，使用Maven进行构建，比较简单。此处贴出.gitlab-ci.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354image: docker:latestservices: - docker:dindvariables: DOCKER_DRIVER: overlay SPRING_PROFILES_ACTIVE: gitlab-cistages: - build - package - deploymaven-build: image: maven:3-jdk-8 stage: build script: \"mvn clean package -B -Dmaven.test.skip=true\" artifacts: paths: - target/*.jardocker-build: stage: package script: - docker info - docker build -t registry.gitlab.nasg.gov.cn/$&#123;CI_PROJECT_PATH&#125;:latest . - docker tag registry.gitlab.nasg.gov.cn/$&#123;CI_PROJECT_PATH&#125;:latest registry.gitlab.nasg.gov.cn/$&#123;CI_PROJECT_PATH&#125;:$&#123;CI_COMMIT_REF_NAME&#125; - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.gitlab.nasg.gov.cn - test ! -z \"$&#123;CI_COMMIT_TAG&#125;\" &amp;&amp; docker push registry.zerbytes.net/$&#123;CI_PROJECT_PATH&#125;:latest - docker push registry.gitlab.nasg.gov.cn/$&#123;CI_PROJECT_PATH&#125;:$&#123;CI_COMMIT_REF_NAME&#125;k8s-deploy: image: lachlanevenson/k8s-kubectl:latest stage: deploy environment: name: live url: https://live-presentation-gitlab-k8s.edenmal.net script: - kubectl version - echo \"$&#123;KUBE_CA_PEM&#125;\" &gt; kube_ca.pem - kubectl config set-cluster kubernetes --server=$&#123;KUBE_URL&#125; --certificate-authority=kube_ca.pem - kubectl config set-credentials kube-admin --token=$&#123;KUBE_TOKEN&#125; - kubectl config set-context kubernetes-system --cluster=kubernetes --user=kube-admin - kubectl config use-context kubernetes-system - kubectl cluster-info - kubectl create -f deployment.yml -n devops# - gcloud auth activate-service-account --key-file key.json# - gcloud config set compute/zone europe-west1-c# - gcloud config set project actuator-sample# - gcloud config set container/use_client_certificate True# - gcloud container clusters get-credentials actuator-sample# - kubectl delete secret registry.gitlab.com# - kubectl create secret docker-registry registry.gitlab.com --docker-server=https://registry.gitlab.com --docker-username=marcolenzo --docker-password=$REGISTRY_PASSWD --docker-email=lenzo.marco@gmail.com# - kubectl apply -f deployment.yml Step 5. 在Kubernetes中增加Docker login 信息为了能够从GitLab registry（GitLab自带的私有镜像仓库功能）下载部署所需的镜像，必须在Kubernetes中增加一个Secret元素，用它访问GitLab Registry的Docker login信息。下面的命令必须在安装有Kubectl的节点中运行。 12345678# YOUR_SECRET_NAME for example \"registry-example-gitlab-key\"$ kubectl create \\ -n devops \\ secret docker-registry regsecret \\ --docker-server=registry.gitlab.nasg.gov.cn \\ --docker-username=xiaof \\ --docker-password=******* \\ --docker-email=xiaof@sbsm.gov.cn","link":"/2018/11/09/基于Gitlab和Kubernetes的持续部署（三）——将镜像部署到Kubernetes集群/"},{"title":"把数据基础做实做准","text":"为把数据基础做实做准，需统筹考虑决策支持系统中数据从来源、分析处理到产生决策价值并提供决策使用等全生命周期的过程治理。数据基础是整个治理过程中最基本的工作，关系到后续各个过程。打牢数据基础，主要有三个方面的工作。 一是建立清晰的数据目录。对于决策支持系统中的每类数据建立一个数据条目，包括名称、来源、数量、业务类别、更新频率、数据类型、访问权限等信息，从数据的业务、存储、使用和持续运营的多个角度进行梳理。数据目录作为决策支持系统最基础的战略性资产，用于数据登记、管理、更新和提供服务。 二是做好不同类型数据的管理。用于信息研究和决策支持的数据从结构上分为三类，第一类是以舆情、情报、文献等知识数据组成的非结构化数据，第二类是以统计数据组成的非关系模式的半结构化数据，第三类是以业务系统产生的管理数据组成的结构化数据，它通常存储于关系型数据库中。上述分类可见他们在存储结构、更新方式、分析方法和使用方式上有较大的差异，需要因地制宜对每类数据使用与其相适应的管理方法和技术。 三是做好数据的持续运营。打牢数据基础不是一蹴而就、一气呵成的, 需要一个持续的过程。积累的数据越多，周期越长,数据价值才越大，利用数据所做的研究也越多。数据的更新和积累属于决策支持系统运行维护的日常性工作，需要坚持不懈地去做。对于每一项数据资源需要从运营的角度建立常态化更新机制，尽量运用信息化手段实现自动更新并监控更新过程。对于无法自动化处理的更新过程，需要明确更新任务和工作量，作为日常数据处理任务进行统筹安排。","link":"/2019/08/19/把数据基础做实做准/"},{"title":"开发常用中间件、数据库的docker快速启动","text":"[TOC] MySQL1docker run --name=orders-mysqldb -d -p 3306:3306 -e MYSQL_ROOT_HOST=% -e MYSQL_ROOT_PASSWORD=root -e MYSQL_USER=orders -e MYSQL_PASSWORD=orders -e MYSQL_DATABASE=orders mysql/mysql-server:5.7.24 1234567# Redis无密码：```shelldocker run --name some-redis -p 6379:6379 -d redis redis-server --appendonly yes 带密码：1docker run --name some-redis -p 6379:6379 -d redis redis-server --appendonly yes --requirepass \"mypassword\"","link":"/2019/04/10/开发常用中间件、数据库的docker快速启动/"},{"title":"基于Spark的新一代数据湖产品：DELTA LAKE","text":"简介Delta Lake是一个开源存储层，可为数据湖带来可靠性。Delta Lake提供ACID事务，可伸缩的元数据处理，并统一流和批处理数据处理。Delta Lake在您现有的数据湖之上运行，并且与Apache Spark API完全兼容。 具体来说，Delta Lake提供： Spark上的ACID事务：可序列化的隔离级别确保读者永远不会看到不一致的数据。 可扩展的元数据处理：利用Spark的分布式处理能力，可以轻松处理数十亿个文件的PB级表的所有元数据。 流和批处理的统一：Delta Lake中的表既是批处理表，又是流的源和接收器。流数据提取，批处理历史回填，交互式查询都可以直接使用。 模式实施：自动处理模式变化，以防止在摄取过程中插入不良记录。 时间旅行：数据版本控制支持回滚，完整的历史审核跟踪以及可重复的机器学习实验。 Upserts和Deletes：支持合并（Merge），更新和删除操作，以启用复杂的用例，例如更改数据捕获，缓慢维度变化（SCD）操作，流化Upserts等。","link":"/2020/02/16/基于Spark的新一代数据湖产品：DELTA-LAKE/"},{"title":"摄影作品赏析","text":"向日葵与蜜蜂 夏荷 岁月静好 瓜藤 睡莲 玉渊映塔 生态祁连","link":"/2018/12/14/摄影作品赏析/"},{"title":"挂载NFS时提示：mount: 文件系统类型错误、选项错误、10.10.18.20:/data 上有坏超级块","text":"最近购买力华为OceanStor系列存储，获得了存储级NAS能力。可以由存储直接提供NFS服务。 在将NFS挂载到Linux时，提示了如下错误： 12345678# mount -t nfs 10.10.18.20:/data /mnt/nfsmount: 文件系统类型错误、选项错误、10.10.18.20:/data 上有坏超级块、 缺少代码页或助手程序，或其他错误 (对某些文件系统(如 nfs、cifs) 您可能需要 一款 /sbin/mount.&lt;类型&gt; 助手程序) 有些情况下在 syslog 中可以找到一些有用信息- 请尝试 dmesg | tail 这样的命令看看。 主要原因是由于没有安装nfs的客户端，所以需要使用yum进行安装 12345yum -y install nfs-utilssystemctl start nfs-utilssystemctl enable nfs-utilsrpcinfo -pmount -t nfs 10.10.18.20:/data /home/ddsc/data/ 挂载成功。","link":"/2019/01/28/挂载NFS时提示：mount-文件系统类型错误、选项错误、10-10-18-20-data-上有坏超级块/"},{"title":"推荐一个学习音乐的好资源——Nice Chord 好和弦","text":"最近对音乐的东西比较感兴趣，尝试着搜了一下学习资源，发现台湾有个叫Wiwi官大为的音乐人在网络上开放了一个叫“Nice Chord好和弦“的学习视频，片子不长但制作精良，没有废话，都是他对音乐乐理和编曲、伴奏等方面的心得体会，顺着下来看能学到很多之前隐隐约约能感觉到，但都不是特别懂的东西。 这个教学视频系列主要以钢琴作为伴奏，但是其中讲的内容对吉他等其他乐器都很有用，通过他的介绍渐渐可以了解到为什么大部分流行歌曲的伴奏都差不多，常见的和弦以及它们在音乐表达上的风格，如何变化和正确使用和弦……等等一系列很有意思的问题，让人看完之后忍不住想买把吉他弹奏试试。 如果你有小孩在学习音乐，这部视频将是对他很有帮助。 下面给出好和弦的官方地址： http://nicechord.com/ 国内有视频转载但是好像没有YouTube更新快，想看的更多更好的视频资源，就看你翻墙的本事了。","link":"/2018/10/26/推荐一个学习音乐的好资源——Nice-Chord-好和弦/"},{"title":"数据湖架构的核心原则","text":"以下 内容摘自《企业数据湖》第三章 数据以原始格式（不可变数据）进入数据湖。企业中的所有数据都有价值。不要试图在第一时间获得价值，而只是摄取数据，然后再尝试从中挖掘价值。 在数据摄取期间，不用急于挖掘价值。 准备好接受多种类型的数据（结构化和非结构化的数据） 准备好接受各种数量级的数据。 不要限制数据存储，这样用户可以在数据湖中查询数据。根据需求引入各种技术，来进行各种分析。 为企业应用程序访问数据提供简单的方法。最终这些数据没有太大意义，但一段时间后，这些数据便能够与数据湖中的其他数据元素协作，并为企业带来价值。 存储时无须对数据进行标准化处理。 能够快速、简单且低成本（高度可伸缩）地添加数据源。 能够根据消费需求提供各种格式的企业数据给应用程序使用。 能够通过数据聚合和大规模数据处理来支持数据智能需求。 能够在系统运行或空闲时清理数据。 能够支持各种安全机制。 由于需要提供重要的企业数据，因而必须高度可用。 不要强制改变进入数据的格式，而是以传入数据的格式来接收数据。 尽可能减少数据量和网络带宽需求。通过使用多种压缩方法来实现这一点。","link":"/2019/07/15/数据湖架构的核心原则/"},{"title":"漫谈数据库三：索引基本概念","text":"许多查询只涉及文件中少量的记录。理想情况下，系统应该能够直接定位这些记录。为了允许这种访问方式，需要设计与文件相关联的附加的结构——索引。 在大型数据库中，维护排序的索引列表将有几个挑战：一是索引本身会非常大；二是虽然通过排序的索引减少了搜索时间，但是查找仍是非常耗时的。所以需要使用更复杂的索引技术。一般有两种基本索引类型： 顺序索引：基于值的顺序排序。 散列索引：基于将值平均分布到若干散列桶中。一支值所属的散列桶是由一个函数决定的，该函数成为散列函数（hash function） 对于索引的评价因素有： 访问类型（access type）：能有效支持的访问类型。访问类型包括随机访问、范围查询等。 访问时间（access time）：查询中使用该技术找到一个特定数据项或数据项集所需的时间。 插入时间（insertion time）：插入一个新数据项所需的时间。该值包括插入这个新数据项的正确位置所需的实际，以及更新索引结果所需的时间。 删除时间（deletion time）：删除一个数据项所需的时间。该值包括找到待删除项所需的时间，以及更新索引结构所需的时间。 空间开销（space overhead）：索引结果所占用的额外存储空间。倘若存储索引结构的额外空间大小十渡，通常牺牲一定的空间代价来换取性能的提高是值得的。 总结 许多查询只涉及文件中很少的一部分记录。为了减少查找这些记录的开销，我们可以为存储数据库的文件创建索引。 索引顺序文件是数据库系统中最古老的索引模式之一。为了允许按搜索码顺序快速检索记录，记录按顺序存储，而无序记录链接在一起。为了允许快速的随机访问，使用了索引结构。 可以使用的索引类型有两种：稠密索引和稀疏索引。稠密索引对每个搜索码值都有索引项，而稀疏索引只对某些搜索码值包含索引项。 如果搜索码的排序顺序和关系的排序序列相匹配，则该搜索码上的索引称为聚簇索引，其他索引称为非聚簇索引或辅助索引。辅助索引可以提高不以聚簇索引的搜索码的查询的性能。但是辅助索引增加了修改数据库的开销。 如索引顺序文件组织的主要缺陷是随着文件的增大，性能会下降。为了克服这个缺陷，可以使用B+树索引。 B+树索引采用平衡树的形式，即从跟到树叶的所有路径长度相等。B+树的高度与以关系中的记录N为底的对数成正比，其中每个非叶节点存储N个指针，N值通常约为50~100。因此，B+树比其他的平衡二叉树（如AVL树）要矮很多，故定位记录所需的磁盘访问次数也较少。 B+树上的查询是直接而且高效的。然而，插入和删除要更复杂一些，但是仍然很有效。在B+树中，查询、插入、删除所需操作数与以关系中记录数N为底的对数成正比，其中每个非叶节点存储N个指针。 可以用B+树去索引包含记录的文件，也可以用它组织文件中的记录。 B树索引和B+树索引类似。B树索引的优点在于它去除了搜索码值存储中的冗余。主要缺点在于整体的复杂性以及结点大小给定时减小了扇出。在实际应用中，系统设计者几乎无一例外地倾向于使用B+树索引。 顺序文件组织需要一个索引结构来定位数据。相比之下，基于散列文件组织允许我们通过计算所需记录搜素码上的一个函数直接找出一个数据项的地址。由于设计时我们不能精确知道哪些搜索码值将存储在文件中，因此一个好的散列函数应该能均匀且随机地把搜索码值分散到各个桶中。 静态散列所用散列函数的桶地址集合是固定的。这样的散列函数不容易适应数据库随时间的显著增长。有几种允许修改散列函数的动态散列技术。可扩充散列是其中之一，他可以在数据库增长或缩减时通过分裂和合并桶来应付数据库大小的变化。 也可以用散列技术创建辅助索引：这样的索引称为散列索引。为使记法简便，假定散列文件组织中用于散列的搜索码上有一个隐式的散列索引。 像B+树和散列索引这样的有序索引可以用作涉及单个属性且基于相等条件的选择操作。当一个选择条件中涉及多个属性时，可以取多个索引中检索到的记录标识符的交。 对于索引属性只有少数几个不同值的情况，位图索引提供了一种非常紧凑的表达方式。位图索引的交操作相当得快，使得他成为一种支持多属性上的查询的理想方式。","link":"/2019/01/29/漫谈数据库三：索引基本概念/"},{"title":"空间数据库索引综述","text":"空间索引在某些方面是基于某些已有的点索引，例如kd树和B+树。这些通过扩展点索引来应用于空间对象的技术一般可以分为三类：对象映射、对象复制和对象约束。对象映射方法将n个顶点定义的对象从k维空间映射到n*k维空间的点或者映射到原来k维空间的单值对象。对象复制方法将同一个对象存储在与此对象重叠的多个数据空间，而对象约束方法通过层级分组将数据对象划分为不同的组，每个分组代表一个数据空间。每种方法都有其优点和缺点，并直接影响索引性能。","link":"/2018/06/28/空间数据库索引综述/"},{"title":"漫谈数据库二：文件中记录的组织","text":"众所周知关系是记录的集合，本文着重讨论在给定一个记录的集合后，如何在文件系统中组织他们。 主要的方法有以下三种： 堆文件（heap file）：先来看看heap file的定义：A heap file is an unordered set of records. The following operationsare supported: Heap files can be created and destroyed. Existingheapfiles can be opened and closed. Records can be inserted anddeleted. Records are uniquely identified by a record id (rid). 一条记录可以放在文件的任何地方，只要那个地方有空间存放这条记录。记录是没有顺序的。通常每个关系使用一个单独的文件。 顺序文件（sequential file）：记录根据“搜索码”的值顺序存储。 散列文件（hashing file）：每条记录的某些属性上计算一个散列函数。散列函数的结果确定了记录应当放到文件的哪个块中。 通常每个关系的记录用一个单独的文件存储。但是在多表聚簇文件组织中，几个不同关系的记录存储在同一个文件中。而且不同关系的相关记录存储在相同的块中，于是一个IO操作可以从所有关系中取到相关的记录。例如，两个关系做连接运算时相匹配的记录被认为是相关的。","link":"/2019/01/29/漫谈数据库二：文件中记录的组织/"},{"title":"管理员无法本地登录Oracle数据库，报错ORA-12560: TNS: 协议适配器错误的解决方法","text":"今日财务报销平台出现了数据源连接不上，查找问题后发现数据库故障。 当使用管理员本地登录数据库进行管理的时候，出现了报错： 12C:\\Users\\Administrator&gt;sqlplus / as sysdbaORA-12560: TNS: 协议适配器错误的解决方法 貌似是由于监听连接数过高，无法进行连接响应了。故重启了服务器后故障恢复。","link":"/2018/12/13/管理员无法本地登录Oracle数据库，报错ORA-12560-TNS-协议适配器错误的解决方法/"},{"title":"表批量读取和写入","text":"Delta Lake支持Apache Spark DataFrame读取和写入API提供的大多数选项，用于对表执行批量读取和写入。 建立表格使用DataFrameWriter（Scala或Java / Python）将数据作为原子操作写入Delta Lake。至少必须指定格式delta： 1df.write.format(\"delta\").save(\"/delta/events\") ## 分区数据您可以对数据进行分区以加快查询或对分区列执行的带有谓词的DML。要在创建增量表时，指定分区的列对数据进行分区。常见的模式是按日期分区，例如： 1df.write.format(\"delta\").partitionBy(\"date\").save(\"/delta/events\") 读取表您可以通过指定路径将Delta表作为DataFrame加载： 1spark.read.format(\"delta\").load(\"/delta/events\") 查询表的旧快照（时间旅行）Delta Lake时间旅行允许您查询Delta表的旧快照。时间旅行有许多用例，包括： 重新创建分析，报告或输出（例如，机器学习模型的输出）。这对于调试或审核尤其有用，特别是在受管制的行业中。 编写复杂的时间查询。 修正数据中的错误。 为快速更改表的一组查询提供快照隔离。 本节介绍了查询表的较旧版本时所支持的方法，数据保留问题并提供了示例。 句法有几种查询旧版Delta表的方法。 DataFrameReader选项DataFrameReader选项允许您从固定到表的特定版本的Delta表创建DataFrame。 12df1 = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_string).load(\"/delta/events\")df2 = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(\"/delta/events\") 对于timestamp_string，仅接受日期或时间戳记字符串。例如&quot;2019-01-01&quot;和&quot;2019-01-01&#39;T&#39;00:00:00.000Z&quot;。 一种常见的模式是在执行Databricks作业期间使用Delta表的最新状态来更新下游应用程序。 写入表格使用append模式，您可以将新数据原子添加到现有的Delta表中： 1df.write.format(\"delta\").mode(\"append\").save(\"/delta/events\") 使用DataFrames覆盖要自动替换表中的所有数据，可以使用overwrite模式： 1df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/events\") 您可以有选择地仅覆盖分区列上与谓词匹配的数据。以下命令用中的数据原子替换一月df： 12345df.write .format(\"delta\") .mode(\"overwrite\") .option(\"replaceWhere\", \"date &gt;= '2017-01-01' AND date &lt;= '2017-01-31'\") .save(\"/delta/events\") 此示例代码将数据写入df，验证所有数据均位于指定分区内，并执行原子替换。 注意与Apache Spark中的文件API不同，Delta Lake会记住并强制执行表的架构。这意味着默认情况下，覆盖不会替换现有表的架构。 有关Delta Lake支持更新表的信息，请参阅更新表。 模式验证Delta Lake自动验证正在写入的DataFrame的架构与表的架构兼容。Delta Lake使用以下规则来确定从DataFrame到表的写入是否兼容： 所有DataFrame列都必须存在于目标表中。如果表中不存在DataFrame中的列，则会引发异常。表中存在但DataFrame中不存在的列设置为null。 DataFrame列数据类型必须与目标表中的列数据类型匹配。如果它们不匹配，则会引发异常。 DataFrame列名称不能只是大小写不同（而文字相同）。这意味着您不能在同一表中定义诸如“ Foo”和“ foo”之类的列。虽然可以在区分大小写或不区分大小写（默认）模式下使用Spark，但在存储和返回列信息时，Parquet区分大小写。Delta Lake保留大小写，但在存储模式时不敏感，并且具有此限制以避免潜在的错误，数据损坏或丢失问题。 如果您指定其他选项（例如partitionBy与附加模式结合使用），则Delta Lake会验证它们是否匹配，并为任何不匹配项引发错误。如果partitionBy不存在，则append操作将自动继续按原有数据的分区。 自动模式更新Delta Lake可以作为DML事务的一部分（附加或覆盖）自动更新表的架构，并使该架构与正在写入的数据兼容。 添加列在以下情况下，DataFrame中存在但表中缺失的列将作为写事务的一部分自动添加： write或writeStream有.option(&quot;mergeSchema&quot;, &quot;true&quot;) 添加的列将追加到它们所在的结构的末尾。追加新列时将保留大小写。 NullType 列由于Parquet不支持NullType，在写入Delta表时NullType会从DataFrame中被删除，但仍存储在模式中。当为该列接收到其他不同的数据类型时，Delta Lake会将模式合并到新的数据类型。如果Delta Lake收到NullType更换现有列，则在写入过程中将保留原有旧模式，并删除新列。 NullType不支持流式传输。由于必须在使用流式传输时设置模式，因此这种情况很少见。NullType也不适用于诸如ArrayType和MapType等复杂类型。 替换表架构默认情况下，覆盖表中的数据不会覆盖架构。当使用mode(“overwrite”)覆盖表而没有replaceWhere，你可能仍然要覆盖写入的数据的架构。通过将overwriteSchema选项设置为true`，可以替换表的模式和分区： 1df.write.option(\"overwriteSchema\", \"true\") 表的视图Delta Lake支持在Delta表之上创建视图，就像使用数据源表一样。 使用视图进行操作时的核心挑战是解析模式。如果更改Delta表模式，则必须重新创建派生视图以说明对该模式的任何添加。例如，如果将新列添加到Delta表中，则必须确保该列在该基表之上构建的有关视图中可用。","link":"/2020/02/16/表批量读取和写入/"},{"title":"裸机Kubernetes集群上运行Nginx Ingress的若干考虑","text":"在传统的云环境下，网络负载均衡是按需调用的，一个Kubernetes manifest可以提供一个单独的通信端点用于NGINX Ingress控制器与外部客户端通信。这样间接地可以与集群中的任何应用进行通信。裸机Kubernetes集群环境确实这一服务组件，所以需要进行一些不同的设置才能为外部用户提供类似的访问。 本文描述了在裸机Kubernetes环境上部署NGINX Ingress控制器的推荐的配置。 MetalLB：一种纯软件的解决方案NodePort 服务方式由于这种方式简单，所以这也是《安装指南》中用户默认部署的方式。 在这个配置下，NGINX容器保持与主机网络的隔离。因此它可以安全的绑定至任何的端口，包括标准的HTTP80端口和443端口。但是由于容器名称空间的隔离，集群网络外的客户端（例如公网用户）无法直接访问Ingress主机的80和443端口。所以外部客户端必须在HTTP请求后面加上分配给ingre-nginx服务的NodePort端口号。 示例假设NodePort30100分配给ingress-nginx服务。 1234$ kubectl -n ingress-nginx get svcNAME TYPE CLUSTER-IP PORT(S)default-http-backend ClusterIP 10.0.64.249 80/TCPingress-nginx NodePort 10.0.220.217 80:30100/TCP,443:30101/TCP Kubernetes节点的公共IP地址是203.0.113.2（下面的external IP只做个示例，在大多数裸机环境中这个值一般为） 12345$ kubectl describe nodeNAME STATUS ROLES EXTERNAL-IPhost-1 Ready master 203.0.113.1host-2 Ready node 203.0.113.2host-3 Ready node 203.0.113.3 客户端可以通过myapp.example.com主机的30100端口来访问Ingress，在这里myapp.example.com域名会解析为203.0.113.2。 对主机系统的影响虽然使用–service-node-port-range API服务标志重新配置NodePort范围，并且可以包括非特权端口（例如80和443）端口，这样听起来很有吸引力。但是这样做会导致非预料的情况和问题，包括并不限于使用了其他系统守护进程占用的端口，或授予kube-proxy某些它并不必须的特权。 因此并不推荐使用这一功能，请使用本文介绍的其他替代的做法。 NodePort方式有其他几个需要注意的限制： Source IP addressNodePort类型的服务默认执行原地址转换。这意味着HTTP请求的源IP总是接受NGINX请求的kubernetes节点的IP地址。 在NodePort设置中避免源IP的推荐方式是在ingress-nginx服务spec中设置externalTrafficPolicy的属性值为Local Warning这个设置有效地识别和扔掉发送到没有运行NGINX Ingress控制器实例的Kubernetes节点。为了控制NGINX ingress控制器能够在哪些节点上进行调度，应当考虑分配NGINX Pods到特定节点。 示例在Kubernetes集群中包括3个节点（external IP作为示例显示，大部分裸机环境中这个字段指一般为） 12345$ kubectl describe nodeNAME STATUS ROLES EXTERNAL-IPhost-1 Ready master 203.0.113.1host-2 Ready node 203.0.113.2host-3 Ready node 203.0.113.3 ngingx-ingress-controller部署包括2个复制集 12345$ kubectl -n ingress-nginx get pod -o wideNAME READY STATUS IP NODEdefault-http-backend-7c5bc89cc9-p86md 1/1 Running 172.17.1.1 host-2nginx-ingress-controller-cf9ff8c96-8vvf8 1/1 Running 172.17.0.3 host-3nginx-ingress-controller-cf9ff8c96-pxsds 1/1 Running 172.17.1.4 host-2 发送到host-2和host-3的请求能够被转发到NGINX并且源客户端IP能够被保留下来，发送到host-1的请求将被丢弃，因为在这个节点上没有NGINX复制集。 Ingress 状态因为NodePort服务不能获得定义中分配的LoadBalancerIP，NGINX Ingress控制器不能更新它管理的Ingress对象的状态。 123$ kubectl get ingressNAME HOSTS ADDRESS PORTStest-ingress myapp.example.com 80 尽管事实上没有为NGINX ingress提供公共IP地址的负载均衡器，仍然能够强制更新所有被管理的Ingress对象，方法是设置ingress-nginx服务中的externalIPs字段。 Warning设置externalIPs并不仅仅使NGINX ingress控制器更新ingress对象的状态。请在官方Kubernetes文档的Service页面阅读这个选项，以及关于ExternalIPs的章节获取更多信息。 示例在Kubernetes集群中包括3个节点（external IP作为示例显示，大部分裸机环境中这个字段指一般为&lt; Node &gt;） 12345$ kubectl describe nodeNAME STATUS ROLES EXTERNAL-IPhost-1 Ready master 203.0.113.1host-2 Ready node 203.0.113.2host-3 Ready node 203.0.113.3 可以编辑ingress-nginx服务并添加一下的字段至spec对象： 12345spec: externalIPs: - 203.0.113.1 - 203.0.113.2 - 203.0.113.3 设置完毕后Ingress对象可以显示出以下属性： 123$ kubectl get ingress -o wideNAME HOSTS ADDRESS PORTStest-ingress myapp.example.com 203.0.113.1,203.0.113.2,203.0.113.3 80 Redirects因为NGINX不知道NodePort Service执行的端口转发，后端应用程序应当负责生成重定向URLs，并将外部客户端使用的URL和端口考虑进去。 示例在不使用NodePort的情况下，通过NGINX生成重定向，例如HTTP到HTTPS的重定向或者domain到www.domain 。 1234$ curl -D- http://myapp.example.com:30100`HTTP/1.1 308 Permanent RedirectServer: nginx/1.15.2Location: https://myapp.example.com/ #-&gt; missing NodePort in HTTPS redirect","link":"/2018/12/17/裸机Kubernetes集群上运行Nginx-Ingress的若干考虑/"},{"title":"Kubernetes V1.9.6 安装Dashboard","text":"[TOC] 今天想为测试用的Kubernetes V1.9.6版本集群安装Dashboard，以便更直观的了解各种Kubernetes概念。然后参照网上的教程进行了操作，遇到了HTTPS证书的问题，始终打不开网页。经过Google了一番，终于找到了解决方案，在此记录一下。 安装Dashboard1. 下载文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 2. 修改配置文件添加type: NodePort，暴露Dashboard服务。注意这里只添加行type: NodePort和nodePort: 30001即可，其他配置不用改，大概位置在末尾的Dashboard Service的spec中，参考如下。 123456789101112131415kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 3. 下载镜像 (每个节点)由于网络原因，配置文件中的k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3镜像无法下载，所以需要进行以下操作提前下载好。由于我使用了docker 代理，并且在代理机器上拨了翻墙VPN，所以可以访问到谷歌的镜像库下周k8s有关插件的镜像。 4. 安装dashboardkubectl create -f kubernetes-dashboard.yaml 5. 授予Dashboard账户集群管理权限编写配置文件 vim kubernetes-dashboard-admin.rbac.yaml 12345678910111213141516171819202122apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system 创建账户管理 kubectl create -f kubernetes-dashboard-admin.rbac.yaml 6. 查看dashboard运行的node的IP1$ kubectl -n kube-system get pods -o wide|grep dashboard|awk &apos;&#123;print $7&#125;&apos; 排除NET::ERR_CERT_INVALID错误可以通过之前设置的MasterIP：端口访问到dashboard，但是如果没有设置签名证书，而是用了原始的证书，会出现认证问题，即打开浏览器访问，会报告NET::ERR_CERT_INVALID错误，如下图： 1&#123;% asset_img ERR_CERT_INVALID.png NET::ERR_CERT_INVALID%&#125; 如访问提示了证书错误NET::ERR_CERT_INVALID，原因是由于物理机的浏览器证书不可用。我们可以生成一个私有证书或者使用公有证书，下面开始配置证书。 1.查看kubernetes-dashboard 容器跑在哪台node节点上，这里跑在docker-slave2上123456789101112131415root@docker-master1 pki]# kubectl get pod -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEcoredns-576cbf47c7-l5wlh 1/1 Running 1 9d 10.244.0.5 docker-master1 &lt;none&gt;coredns-576cbf47c7-zrl66 1/1 Running 1 9d 10.244.0.4 docker-master1 &lt;none&gt;etcd-docker-master1 1/1 Running 1 9d 192.168.20.210 docker-master1 &lt;none&gt;kube-apiserver-docker-master1 1/1 Running 2 9d 192.168.20.210 docker-master1 &lt;none&gt;kube-controller-manager-docker-master1 1/1 Running 2 9d 192.168.20.210 docker-master1 &lt;none&gt;kube-flannel-ds-amd64-c7wz6 1/1 Running 0 9d 192.168.20.213 docker-slave1 &lt;none&gt;kube-flannel-ds-amd64-hqvz9 1/1 Running 0 9d 192.168.20.214 docker-slave2 &lt;none&gt;kube-flannel-ds-amd64-w7n4s 1/1 Running 2 9d 192.168.20.210 docker-master1 &lt;none&gt;kube-proxy-8gj2w 1/1 Running 1 9d 192.168.20.210 docker-master1 &lt;none&gt;kube-proxy-mt6dk 1/1 Running 0 9d 192.168.20.213 docker-slave1 &lt;none&gt;kube-proxy-qtxz7 1/1 Running 0 9d 192.168.20.214 docker-slave2 &lt;none&gt;kube-scheduler-docker-master1 1/1 Running 2 9d 192.168.20.210 docker-master1 &lt;none&gt;kubernetes-dashboard-5f864b6c5f-5s2rw 1/1 Running 0 5d21h 10.244.3.9 docker-slave2 &lt;none&gt; 2. 在docker-slave2节点上查看kubernetes-dashboard容器IDroot@docker-slave2 ~]# docker ps | grep dashboard384d9dc0170b registry.cn-hangzhou.aliyuncs.com/kube_containers/kubernetes-dashboard-amd64 “/dashboard –insecu…” 5 days ago Up 44 hours k8s_kubernetes-dashboard_kubernetes-dashboard-5f864b6c5f-5s2rw_kube-system_94c8c50b-f484-11e8-80e8-000c29c3dca5_0 3.查看kubernetes-dashboard容器certs所挂载的宿主主机目录12345678910111213141516171819202122232425262728293031323334353637383940414243[root@docker-slave2 ~]# docker inspect -f &#123;&#123;.Mounts&#125;&#125; 384d9dc0170b&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/volumes/kubernetes.io~empty-dir/tmp-volume&quot;, &quot;Destination&quot;: &quot;/tmp&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/volumes/kubernetes.io~secret/kubernetes-dashboard-token-tbctd&quot;, &quot;Destination&quot;: &quot;/var/run/secrets/kubernetes.io/serviceaccount&quot;, &quot;Mode&quot;: &quot;ro&quot;, &quot;RW&quot;: false, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/etc-hosts&quot;, &quot;Destination&quot;: &quot;/etc/hosts&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/containers/kubernetes-dashboard/0e84c511&quot;, &quot;Destination&quot;: &quot;/dev/termination-log&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/volumes/kubernetes.io~secret/kubernetes-dashboard-certs&quot;, &quot;Destination&quot;: &quot;/certs&quot;, &quot;Mode&quot;: &quot;ro&quot;, &quot;RW&quot;: false, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ], 4. 这里以私有证书配置，生成dashboard证书1234openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.keyopenssl req -new -key dashboard.key -out dashboard.csropenssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey dashboard.key -out dashboard.crt 5.将生成的dashboard.crt dashboard.key放到certs对应的宿主主机souce目录1scp dashboard.crt dashboard.key 192.168.20.214:/var/lib/kubelet/pods/94c8c50b-f484-11e8-80e8-000c29c3dca5/volumes/kubernetes.io~secret/kubernetes-dashboard-certs 6.重启kubernetes-dashboard容器1docker restart 384d9dc0170b 完成以上步骤即可访问kubernetes-dashboard web了，由于使用的是私有证书，所以还是会弹出不安全的连接，需要添加例外。 7. 登录页面上有两种登录方式，这时我们使用token的方式登录。token的获取方式如下。 在master节点执行 123$kubectl -n kube-system get secret | grep kubernetes-dashboard-admin|awk &apos;&#123;print &quot;secret/&quot;$1&#125;&apos;|xargs kubectl describe -n kube-system|grep token:|awk -F : &apos;&#123;print $2&#125;&apos;|xargs echoeyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1qYm0ycCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijg5ZmFiOGFmLTNjYzEtMTFlOC1iODQ4LTAwMGMyOTQwYWRiYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.YS-ZklZ8fbkDp3tuOxFHyhiflXtCGDY0C5C3PYU1ot7YFCGA67_vDKY55OiE36sZNGNhWEmK52Yak7SrFZ75KwyMbM7TK69SGLftFiMedsUCfuUpBPB-Fc4beaxMuWWqVcHOs892VfE6I85xhhYLv_xD6t8x2DcJ1Cl6c5UVg_GBw13cSVaSA7asMpVuSj8MdOQcBNIUaRaxY04PDvZDWIN8Cqud9yDNkueFeuqP3DN_rN0FzLGg0Lqv3Q-fm4hKcIiiVi6E9J-i_T8QCsoKE36wEWg3hJdUTmzBufew2YrbPH4f0Aezq-OeKT8-x89vQwkbj1vttiVVtluTTX53T 上面获取到的就是token了，复制到登录页就可以登录了。","link":"/2018/12/14/Kubernetes-V1-9-6-安装Dashboard/"},{"title":"使用Raml构建Restful-API","text":"本文重点介绍如何使用Raml来构建RESTful API，从而来提高微服务团队开发人员的合作，涵盖了以下内容： 12341. 恰到好处的Raml。2. 十分钟快速上手。3. 搭建Mock Server。4. 使用Docker分离数据和构建过程。 本文的目的是在决策支持系统房地产调控专题中应用RAML，提高前后端开发效率及软件质量。 恰到好处的Raml在前后端分离的架构中，一个经典的优秀实践是CDCD（Consumer Driven Contract Development），即消费者驱动契约开发。该实践的两个核心点是：消费者驱动+ 契约： 121. 消费者驱动。Service所提供的API是给不同的消费者去使用的，所以消费端需要消费什么数据，API就应该返回什么数据。剩下就是合同了，通俗的将就是API契约，2. 契约。通俗的讲是一个API契约，Service和Consumer（Consumer可以是前端或另一个Service）的API结构需要一个契约来统一管理，这样一来，负责Service和Consumer的开发人员都能参照同一套API契约独立并行开发，双方完成后进行集成联调。 优秀的实践（CDCD）总是离不开优秀工具来做支撑，Raml以及其生态群恰到好处地提供了我们想要的合同功能。Raml是什么，先看一张官网： Design、Build、Test、Document、Share，我们选择 Raml，究根结底也是因为它这几大优秀的特性： 123451. 设计API。你可以快速的构造你的API，并以人类友好的格式将它呈现出来。它涵盖了一些重要设计的最佳实践，如建模、模式、模板以及代码重用。2. 构建API。一旦设计好你的API，你就可以借助一些开发工具，将设计好的静态API文档，变成一个服务器端来提供服务。3. 测试API。引入单元测试可以有效地保证API实现的正确性，你可以通过运行一些脚本来测试你服务端是否涵盖了你设计好的API。4. 文档化API。Raml可以帮助你脱离同步维护一份额外文档的痛苦。RAML是一门API描述语言，所以你的API一旦被描述，它就是一份现成的API文档。你可以借助一些工具将它生成可视化的文档。5. 分享以及维护你的API。你可以借助一个基本的JavaScript来生成一些交互式工具（API Consoles或API Nodebooks），这样其他开发者可以使用标准格式与你的维护团队进行交流。 接下来，我们主要针对 Raml 的前四个特性进行一个实践尝试。 窥探Raml 共享俨然成为了一个非常时髦的词汇。共享单车、共享汽车这些产品雨后春笋般的兴起。它们有一个共性：商家集中提供大规模的服务，用户可以付出极低的价格就可以享用这种服务。不可否认，这些产品正在促进社会进步（绿色出行），而真的共享了吗？我是持有怀疑态度的… 我们要开发一款产品共享图书馆，一句话描述该产品的理念： 1任何注册用户都可以将自己拥有的图书共享到一个集中的虚拟图书馆，同时可以从这个图书馆中借阅任何图书。 产品立项进入了开发期，第一个迭代（敏捷开发的术语，关于敏捷开发推荐阅读 我在ThoughtWorks的敏捷实践），团队目标是用户管理的功能。我们将以用户故事的形式展开展开： 作为管理员，ta可以查阅用户列表: 1234567891011121314151617181920212223/users: get: description: 查看用户列表 queryParameters: gender: description: \"性别\" required: false type: string pattern: \"MALE|FEMALE\" responses: 200: body: application/json: example: | [ &#123; \"id\": \"2nldksfr4f2ifoa4g43rvfsdfdsfdaf2\", \"account\": \"sjyuan\", \"phoneNumber\": \"18192235667\", \"gender\": \"MALE\", \"name\": \"张三\" &#125; ] 该API表示：操作资源/users，get表示使用的是HTTP GET方法。queryParameters定义了查询参数，responses定义了返回值，example描述了response body，它是一个JSON数组。 作为普通用户，ta首先要注册一个账号： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/users: post: description: 注册一个新用户 body: application/json: schema: | &#123; \"type\": \"object\", \"$schema\": \"http://json-schema.org/draft-03/schema\", \"required\": true, \"properties\": &#123; \"account\": &#123; \"type\": \"string\", \"required\": true, \"minLength\": 2, \"maxLength\": 20 &#125;, \"password\": &#123; \"type\": \"string\", \"required\": true, \"minLength\": 6, \"maxLength\": 32 &#125;, \"phoneNumber\": &#123; \"type\": \"string\", \"required\": true &#125;, \"name\": &#123; \"type\": \"string\", \"required\": false &#125; &#125; &#125; responses: 201: body: application/json: example: | &#123; \"id\": \"2nldksfr4f2ifoa4g43rvfsdfdsfdaf2\", \"account\": \"sjyuan\", \"phoneNumber\": \"18192235667\", \"name\": \"张三\" &#125; 该API使用了HTTP POST请求，schema定义了request body的校验格式。 作为普通用户，ta可以查看自己的详情： 123456789101112131415161718/users: /&#123;account&#125;: get: description: 查看给定账户名的用户信息 responses: 200: body: application/json: example: | [ &#123; \"id\": \"2nldksfr4f2ifoa4g43rvfsdfdsfdaf2\", \"account\": \"sjyuan\", \"phoneNumber\": \"18192235667\", \"gender\": \"MALE\", \"name\": \"袁慎建\" &#125; ] /{account}是一个URI参数，传入的是用户的账号。 作为普通用户，ta可以编辑自己的信息： 1234567891011121314151617181920212223242526/users: /&#123;account&#125;: put: description: 更新给定账户名的用户信息 body: application/json: schema: | &#123; \"type\": \"object\", \"$schema\": \"http://json-schema.org/draft-03/schema\", \"required\": true, \"properties\": &#123; \"password\": &#123; \"type\": \"string\", \"required\": true, \"minLength\": 6, \"maxLength\": 32 &#125;, \"name\": &#123; \"type\": \"string\", \"required\": false &#125; &#125; &#125; responses: 301: 更新一个用户信息，我们使用了HTTP POST 请求。 作为一个管理员，ta可以禁用或删除某些违规操作的用户： 123456/users: /&#123;account&#125;: delete: description: 删除给定账户名的用户 responses: 204: 删除一个用户，我们使用了HTTP DELETE请求。 到此为止，我们完成了简单的User增删改查的API设计。我们只是做一个快速的窥探，真实业务场景中的API远比这要复杂得多，我们会利用include来提高代码的复用，而一些Raml的高级特性，诸如：data type、seurity、Resource Types、Traits等，也将成为强有力的工具。关于Raml更多详细指南，可参阅： Raml-spec。 完整源代码保存在我的GitHub的 restful-api-raml。 构建你的API我们已经完成了API的设计（用户的增删改查），API契约（合同）定好了。前、后端开发人员开始进入开发阶段。 接下来有新的需求： 121. API设计好了，如何将其文档化，提供用户有好的阅读格式呢？2. API设计好，如何搭建基于设计API的Web Server，给前端的集成测试提供支持呢？ 问题一是前文提到的Raml的一大特性：文档化API。 问题二就需要我们将设计好API文档转变成一个Web Server，它也是Raml一大特性的体现：构建API。我们需要借助一些工具来完成这两件事情。 文档化API在Raml官方文档中提及了三种工具，API Console、RAML to HTML、RAML2HTML for PHP，我们将使用 RAML to HTML去做Raml到html的转换，再利用 Live Server run起一个静态文件服务，提供在浏览器查阅API文档的功能。 在开始之前，我们需要安装 npm。使用npm下载依赖，并定义一个任务： 1234567891011&#123; ... \"dependencies\": &#123; \"raml2html\": \"^6.1.0\", &#125; \"scripts\": &#123; \"docs-generator\": \"raml2html data/api.raml &gt; api.html\" \"docs-server\": \"live-server --port=8081 --watch=api.html --entry-file=api.html\" &#125; ...&#125; docs-generator 任务是用来将raml文档转换成html文档，docs-server 启动了一个轻量的静态文件服务，它可以监听api.html文件的变化（只需要刷新浏览器），运行下面命令，之后访问http://127.0.0.1:8081之后，可以看到： 12$ npm run docs-generator$ npm run docs-server","link":"/2020/12/02/使用Raml构建Restful-API/"},{"title":"榨菜指数--中国城镇化的缩影","text":"外媒指出，跟踪中国2.63亿农民工的流向是一项艰巨挑战。廉价的榨菜颇受中国农民工青睐，涪陵榨菜是销量最高的品牌。据报道，一个城市对这种低价值易耗品的需求通常会保持稳定，除非农民工人口发生变化，因此发改委把榨菜当作衡量农民工流向的替代指标。 导读外媒指出，跟踪中国2.63亿农民工的流向是一项艰巨挑战。廉价的榨菜颇受中国农民工青睐，涪陵榨菜是销量最高的品牌。据报道，一个城市对这种低价值易耗品的需求通常会保持稳定，除非农民工人口发生变化，因此发改委把榨菜当作衡量农民工流向的替代指标。 榨菜、啤酒指数榨菜指数 榨菜，属于低值易耗品，收入增长对于榨菜的消费几乎没有影响。一般情况下，城市常住人口对于方便面和榨菜等方便食品的消费量，基本上是恒定的。销量的变化，主要由流动人口造成。 国家发改委规划司官员发现，涪陵榨菜在华南地区销售份额由2007年的49%、2008年的48%、2009年的47.58%、2010年的38.50%下滑到2011年的29.99%，从占半壁江山滑落到30%以下。这个数据表明，华南地区流动人口流出速度非常快。 涪陵榨菜集团广州办事处的主任郑先生也证实了这一现象，回流人口对于华南大区的销售影响很大。他说，八年来，广东地区市场份额明显在下降。从增速上来讲，2011年当年华中、中原、西北地区营业收入比上年增长45%-57%不等，而华南地区仅有1.28%的增长。 反映出啥问题？ 专家称，一般来说，城市常住人口对于方便面和榨菜等方便食品的消费量基本上是恒定的，销量的变化主要由流动人口特别是农民工造成。国家统计局的一份报告也印证了该观点，报告称，2012年全国在中西部地区务工的农民工数量增长较快，回流趋势明显。 中西部省份经济的快速崛起，东部各类加工企业、劳动密集型企业大举西迁，造就了大量的就业机会。无论是城市基础设施，还是工资待遇和生活水平，已和东部城市相差无几，农民工外出务工意愿已“显著降低”。 农民工回流态势对回流省份农民工保障体系的建设提出了新的更高要求，依照城乡一体化的原则和精神，在农民工保障体系的建设上早做打算，实乃当务之急。当下，在农民工公共服务均等化问题上，传统的劳务输入大省，如北京、广东等正在积极制定相关政策，这些做法是契合农民工回流这个现实大背景的。但一些劳务输出大省，还存在意识不足、应对不够的问题。 啤酒指数 “我不知道大家有没有听说过啤酒指数，最新的啤酒指数是指农村啤酒量决定农民工回乡成功，也就是说失业。最近我们发现农村的啤酒指数在不断攀升，意味着大量的农民工回到农村去喝啤酒，而不是在城市消费啤酒了。”华远地产董事长任志强在2013博鳌房地产论坛上称。 任志强讲到，上半年的就业情况好转，新增就业人口增加都意味着是企业向上不断扩张的过程。但啤酒指数告诉我们的是，下半年也许会出现的是原有的一部分就业被替代，农民工回乡的开始增加，这是一个非常危险的信号。 此外，任志强提到，在土地价格上涨的背后，是因为在今年总的用地供应量的增加过程中，工业用地占的比重非常大。 他指出，在城市发展过程中，工业所占的用地数量应该是非常少的，但在国内，相当一部分工业重镇的工业用地，已经达到了用地总量的40%到50%，平均下来是占26%以上。换句话说，所有的土地，住宅用地的升值过程中，相当一部分是弥补和补贴了工业用地的超量。所以工业化率和城市化率几乎接近，而实际上应该城市化率远远大于工业化率的值，这样才能保证为工业提供更多的第三产业的服务。 克强指数克强指数，是英国著名政经杂志《经济学人》在2010年推出的用于评估中国GDP增长量的指标，源于李克强总理2007年任职辽宁省委书记时，喜欢通过耗电量、铁路货运量和贷款发放量三个指标分析当时辽宁省经济状况。该指数是三种经济指标：工业用电量新增、铁路货运量新增和银行中长期贷款新增的结合。自推出后，受到花旗银行在内的众多国际机构认可。 7月份全社会用电量同比增速创今年新高 8月14日，国家能源局发布的7月份全社会用电量数据再次透露出经济向好的征兆。数据显示，7月份，全社会用电量达到4950亿千瓦时，同比增长8.8%。1-7月，全社会用电量累计达到29901亿千瓦时，同比增长5.7%。 经济形势确实正在好转，从分产业的用电量数据看，1-7月，第二产业用电量为22077亿千瓦时，同比增长5.4%。第三产业用电量达到3509亿千瓦时，同比增长9.9%;城乡居民生活用电量为3741亿千瓦时，同比增长5.1%。 国家电网能源研究院经济与能源供需研究所所长单葆国预计，用电量在八九月份将保持9%左右的增速。他认为，用电量增长的主要原因是基础设施建设呈回暖趋势。 而数据也确实反映了居民用电的快速大幅上升。7月份，城乡居民生活用电量为3741亿千瓦时，同比增长5.1%。而6月份，城乡居民生活用电量为3156亿千瓦时，同比增长3.9%。 7月份中国国家铁路客货运量同比环比双增长 从中国铁路总公司获悉，7月份，铁路部门继续深化客货运输改革，努力提高客货运输质量，国家铁路客货运量同比环比实现双增长。 客运方面，7月份国家铁路旅客发送量完成19634万人，比上年同期增加1801万人，增长10.1%;与6月份环比增加1846万人，增长10.4%。 货运方面，7月份国家铁路货物发送量完成25972万吨，比上年同期增长639万吨，增长2.5%;与6月份环比增加706万吨，增长2.8%。 央行:7月人民币贷款增6999亿 央行今日发布2013年7月金融统计数据报告。报告显示，央行7月对市场净投放现金349亿元。存贷款方面,7月人民币存款减少2573亿元,人民币贷款增加6999亿元。银行间市场同业拆借月加权平均利率3.54%,质押式债券回购月加权平均利率3.60%。 7月末,广义货币(M2)余额105.24万亿元,同比增长14.5%,分别比上月末和上年同期高0.5个和0.6个百分点;狭义货币(M1)余额31.06万亿元,同比增长9.7%,分别比上月末和上年同期高0.7个和5.1个百分点;流通中货币(M0)余额5.44万亿元,同比增长9.5%。当月净投放现金349亿元。 外国的趣味经济指标内裤指数 其实，名人私下有趣的测市指标，又岂只得两、三个，而其中“男人内裤指数”(Men’s Underware Index)的销售数字，就是前美国联储局主席格老—你没看错，我也没写错-格林斯潘(Alan Greenspan)最信赖的经济指标之一。指标指出当经济不好时，男人就会迟些再买新内裤，从而令内裤销售额下跌，到销售额再升时，就代表经济开始回暖。 格老强调，这个指标用以评估经济是否开始复苏最为有效。想必格老也是这种顾家好男人，只是难为了为他洗“底”的靚老婆(1997年4月，时年71岁的格老迎娶比他年轻20年的电视台记者Andrea Mitchell)。但格老也不是随便说的，据英敏特公司的研究显示，2009年美国经济低迷时，男性内裤销量下降了2.3%。 除此之外，格老也曾提过另外几个类似的指数，如民众干洗衬衫的次数，当在大家到外面洗衫数量增加，就代表经济好转。还有石膏板销售量增加，则表示房屋装修的活动增加，也是代表经济好转的现象等。其实类似的测市指标还有许多，如“罩杯经济学”、“高跟鞋理论”、“土豆效应”、“iPod指数”、“床垫指数”等等，无法一一细数。可见再简单的生活，也可带出重要的经济提示。 裙摆指数 由已故美国经济学家George Taylor在1926年所创。指数原理是当经济越好时，女生的裙摆就会越短，反之当经济越差时，女生的裙子就会变得越长。他认为原因经济不佳时，女生改穿长裙是要遮掩廉价丝袜。例如在经济繁荣的60年代，迷你裙就是热卖时装;但是在1990年代初期，即经济衰退期，就流行长裙。 口红理论 经济不好的情况下，女性不再像以前那样随性买一些时尚、赶潮流的服装、化妆品，而是趋向于购买一些经典式样的、耐久使用的用品，尤其是奢侈品。口红是最便宜的一种奢侈品，单价不高，又可以使女性继续保持好容貌，所以，口红此时会大卖。同样，经典式样服装、鞋子、包等也会好卖。 关键词三驾马车 从支出角度看，GDP是最终需求─投资、消费、净出口这三种需求之和，因此经济学上常把投资、消费、出口比喻为拉动GDP增长的“三驾马车”，这是对经济增长原理最生动形象的表述。 1、内需是指内部需求，即就是本国居民的消费需求，它是经济的主要动力; 2、投资是指财政支出，即政府通过一系列的财政预算包括发行国债，对教育，科技，国防，卫生等事业的支出，是辅助性的扩大内需; 3、出口是指外部需求，即是通过本国企业的产品打入国际市场，参与国际竞争，扩大自己的产品销路。","link":"/2020/11/23/榨菜指数-中国城镇化的缩影/"},{"title":"精通Pandas探索性分析（二）：数据选择","text":"在本章中，我们将学习使用 Pandas 进行数据选择的高级技术，如何选择数据子集，如何从数据集中选择多个行和列，如何对 Pandas 数据帧或一序列数据进行排序，如何过滤 Pandas 数据帧的角色，还学习如何将多个过滤器应用于 Pandas 数据帧。 我们还将研究如何在 Pandas 中使用axis参数以及在 Pandas 中使用字符串方法。 最后，我们将学习如何更改 Pandas 序列的数据类型。 首先，我们将学习如何从 Pandas 数据帧中选择数据子集并创建序列对象。 我们将从导入真实数据集开始。 我们将介绍一些 Pandas 数据选择方法，并将这些方法应用于实际数据集，以演示数据子集的选择。 在本章中，我们将讨论以下主题： 从数据集中选择数据 排序数据集 使用 Pandas 数据帧过滤行 使用多个条件（例如 AND，OR 和 ISIN）过滤数据 在 Pandas 中使用axis参数 更改 Pandas 序列的数据类型 数据集简介我们将使用 zillow.com 的真实数据集，这是一个在线房地产市场，其发布房价数据集是他们研究工作的一部分。 这些数据集可在公共领域获得，并在归属于 zillow.com 后可免费使用。 我们将使用有关美国地区平均房价的最新数据。 它是 CSV 数据集，或带有 CSV 的文本文件。 让我们首先将 pandas 模块导入到 Jupyter 笔记本中，如下所示： 12import pandas as pddata = pd.read_csv('data-zillow.csv', sep=',') 从数据集中选择数据12regions = data['RegionName'] #根据列名选择type(regions) #series对象 多列选择123region_n_state = data[['RegionName', 'State']] #传递列名数组，选择多个列region_n_state.head() type(region_n_state)#DataFrame对象 点表示法12data.State#选择单列data['Address'] = data.County + ', ' + data.Metro + ', ' + data.State#生成新列 从 Pandas 数据帧中选择多个行和列12zillow.loc[7, 'Metro'] #行索引7，列名为Metrozillow.iloc[7,4]#行索引7，列索引4 选择单行和多列123zillow.loc[7, ['Metro', 'County']] zillow.iloc[7,[4,5]]zillow.loc[11,:] 选择多行和单列1zillow.loc[101:105, 'Metro'] 选择多行和多列123zillow.loc[201:204, \"State\":\"County\"] zillow.iloc[201:205, 3:6] zillow.loc[201:205, ['RegionName', 'State']] 从行和所有列的范围中选择值在这里，我们将使用loc方法查看行和列序列中的值。 为此，loc方法的第一个参数是要选择的行的范围索引。 由于我们需要所有列中的值，因此我们将冒号（:）作为第二个参数，如下所示： 1234zillow.loc[201:205, :] zillow.loc[[0,5,10], :] #选择不连续的行zillow.loc[zillow.County==\"Queens\"] #选择County列的值为Queens的行zillow.loc[zillow.Metro==\"New York\", \"County\"] #从County列中选择Metro列为New York的行 对pandas数据帧排序123456zillow.sort_values('Metro') #对Metro列排序（默认升序）sorted = zillow.sort_values('Metro', ascending=False) #降序排序sorted = zillow.sort_values(by=['Metro','County']) #按Metro首先对数据进行排序，然后按County列进行排sorted = zillow.sort_values(by=['Metro','County', 'Zhvi'], ascending=[True, True, False]) regions = zillow.RegionIDregions.sort_values() #对Series对象排序 过滤 Pandas 数据帧的行12345filtered_data = data.filter(items=['State', 'Metro']) #使用filter方法过滤列filtered_data = data.filter(regex='Region', axis=1) #使用正则表达式过滤列名称price_filter_series = data['Zhvi'] &gt; 500000 data[price_filter_series]data[data.Zhvi &gt;= 1000000] #过滤Zhvi列大于1000000的行 将多个过滤条件应用于 Pandas 数据帧12data[(data['Zhvi'] &gt; 1000000) &amp; (data['State'] == 'NY')] #选择价格值大于 1,000,000 且State为New York的行data[((data['State'] == 'CA') | (data['State'] == 'NY'))] 使用isin方法进行过滤筛选数据的另一种方法是使用isin方法。 我们可以使用isin方法通过一个或多个特定列的值列表来过滤数据集。 在这里，我们仅从Metro列中选择值New York或San Francisco的那些记录。 我们在Metro列上调用isin方法，并将其传递给包含我们要选择的城市的列表。 这将创建一个布尔序列。 然后，我们将布尔序列传递给数据集数据帧进行必要的过滤和选择，如下所示： 12filter = data['Metro'].isin(['New York', 'San Francisco']) data[filter].head() 在多个条件下使用isin方法我们还可以使用isin方法根据来自多列的值过滤行。 为了执行此操作，我们传递了一个字典对象，其中的键是列名，而值是我们要从中选择记录的那些列的值的列表。 举个例子，让我们选择State参数为California和Metro参数为San Francisco的值。 我们使用包含要选择的值的这两列创建一个字典对象，然后将该字典项传递给isin方法，并在数据集上调用isin方法。 然后，将过滤器传递给数据帧并选择我们的记录，如下所示： 12filter = data.isin(&#123;'State': ['CA'], 'Metro': ['San Francisco']&#125;) data[filter].head() 在 Pandas 中使用axis参数在本节中，我们将学习在 Pandas 中进行数据分析时何时何地使用axis参数或关键字。 我们将介绍axis参数，并逐步介绍可以将axis关键字设置为的各种值。 我们将演示如何将axis设置为行或列来改变方法的行为。 我们还将展示一些使用axis关键字的代码示例。 axis参数的用法1data.axes 在axis用法示例中，我们计算数据集中值的平均值。 我们已将axis传递为0。 这意味着将沿着行axis计算平均值，如下所示： 1data.mean(axis=0) 输出如下： 接下来，我们将axis设置为1。 我们在同一数据集上使用完全相同的方法； 但是，我们正在将axis从0更改为1。 由于我们将axis设置为1，因此mean的计算如下： 1data.mean(axis=1).head() 输出如下： 有时很难记住0或1是用于行还是用于列。 因此，您可以将axis设置为rows而不是使用axis0： 1data.mean(axis='rows') 输出为以下内容： 对于列，您可以将axis设置为columns。 与使用0或1具有相同的效果： 1data.mean(axis='columns').head() 输出如下： axis关键字的更多示例在这里，我们使用drop方法删除行或记录。 我们通过将关键字axis传递为0来告诉drop方法将记录删除到0的索引处： 12data.drop(0, axis=0) #删除第0行data.drop('Date', axis=1).head #删除带有Date标签的列 axis关键字1data.filter(regex='Region', axis=1) #过滤列名为正则表达式'Region'的列 在 Pandas 中使用字符串方法在本节中，我们将学习在 Pandas 序列中使用各种字符串方法。 我们将把真实的数据集读入 Pandas。 我们将探索一些字符串方法，并将使用这些字符串方法从数据集中选择和更改值。 检查子串为了学习如何使用字符串方法检查 Pandas 序列的子字符串，我们使用str包中的contains方法。 在这里，我们从数据集中调用RegionName序列上的str.contains方法。 我们正在寻找包含New子字符串的记录。 它打印出一个布尔序列，打印True找到一个子字符串，而False找到一个子字符串： 1data[data.RegionName.str.contains('New')] #过滤RegionName列中包含New字符串的行 将序列或列值更改为大写1data.RegionName.str.upper() 将值更改为小写1234data.County.str.lower()data.County.str.len()#查找列中每个值的长度data.RegionName.str.lstrip()#删除空格data.RegionName.str.replace(' ','')#空格替换为无空格 更改 Pandas 序列的数据类型在本节中，我们将学习如何更改 Pandas 序列的数据类型。 我们将看到读取其中的数据后如何更改数据类型。 我们还将学习在读取 Pandas 数据时如何更改数据类型。 我们将通过一个示例将int列更改为float。 我们还将看到如何将字符串值列转换为datetime数据类型。 1data['Zhvi'] = data.Zhvi.astype(float) #将int数据类型列更改为float 读取数据时更改数据类型在将数据读入 pandas 之后，我们只是更改了列的数据类型。 另外，我们可以在读取数据时更改数据类型。 为此，我们将列名和数据类型传递到要更改为read数据方法的列中。 我们想要的float列已导入为float64： 12data2 = pd.read_csv('data-zillow.csv', sep=',', dtype=&#123;'Zhvi':float&#125;) data2.dtypes 将字符串转换为日期时间1pd.to_datetime(data2.Date,infer_datetime_format=True) 总结在本章中，我们学习了从 Pandas 数据帧中选择数据子集的方法。 我们还了解了如何将这些方法应用于真实数据集。 我们还了解了从已读入 Pandas 的数据集中选择多个行和列的方法，并将这些方法应用于实际数据集以演示选择数据子集的方法。 我们了解了 Pandas sort_values方法。 我们看到了使用sort_values方法对 Pandas 数据帧中的数据进行排序的各种方法。 我们还学习了如何对 Pandas 序列对象进行排序。 我们了解了用于从 Pandas 数据帧过滤行和列的方法。 我们介绍了几种方法来实现此目的。 我们了解了 Pandas 的filter方法以及如何在实际数据集中使用它。 我们还学习了根据从数据创建的布尔序列过滤数据的方法，并且学习了如何将过滤数据的条件直接传递给数据帧。 我们学习了 Pandas 数据选择的各种技术，以及如何选择数据子集。 我们还学习了如何从数据集中选择多个角色和列。 我们学习了如何对 Pandas 数据帧或序列进行排序。 我们逐步介绍了如何过滤 Pandas 数据帧的行，如何对此类数据帧应用多个过滤器以及如何在 Pandas 中使用axis参数。 我们还研究了字符串方法在 Pandas 中的使用，最后，我们学习了如何更改 Pandas 序列的数据类型。 在下一章中，我们将学习处理，转换和重塑数据的技术。","link":"/2020/12/29/精通Pandas探索性分析（二）：数据选择/"},{"title":"Spring Security架构","text":"身份验证和访问控制应用程序安全性可归结为两个或多或少独立的问题：身份验证（您是谁？）和授权（您可以做什么？）。有时人们会说“访问控制”而不是“授权”，这可能会让人感到困惑，但以这种方式来思考是有帮助的，因为“授权”会在其他地方超载。Spring Security的架构旨在将身份验证与授权分开，并为两者提供策略和扩展点。 认证身份验证的主要策略接口AuthenticationManager只有一个方法： 123456public interface AuthenticationManager &#123; Authentication authenticate(Authentication authentication) throws AuthenticationException;&#125; AuthenticationManager方法可以做如下三个事情： 如果它可以验证输入是否代表有效的主体，则返回Authentication（通常带有authenticated=true）。 AuthenticationException如果它认为输入代表无效的主体，则抛出一个。 null如果它不能决定返回。 AuthenticationException是运行时异常。它通常由应用程序以通用方式处理，具体取决于应用程序的样式或用途。换句话说，通常不希望用户代码捕获并处理它。例如，Web UI将呈现一个页面，表明身份验证失败，后端HTTP服务将发送401响应，有或没有WWW-Authenticate标头，具体取决于上下文。 最常用的实现AuthenticationManager是ProviderManager，它委托给一组AuthenticationProvider实例。AuthenticationProvider有点像AuthenticationManager，但它有一个额外的方法允许调用者查询它是否支持给定的Authentication类型： 12345678public interface AuthenticationProvider &#123; Authentication authenticate(Authentication authentication) throws AuthenticationException; boolean supports(Class&lt;?&gt; authentication);&#125; supports()方法中的参数Class&lt;?&gt;类型是Class&lt;? extends Authentication&gt;（只会询问它是否支持将传递给authenticate()方法的内容）。A ProviderManager可以通过委托给一个链来支持同一个应用程序中的多个不同的身份验证机制AuthenticationProviders。如果一个 ProviderManager无法识别特定的Authentication实例类型，则会跳过它。 A ProviderManager有一个可选的父级，如果所有提供者都返回，它可以查询null。如果父母不可用，那么在AuthenticationException中返回null Authentication的结果。 有时，应用程序具有受保护资源的逻辑组（例如，与路径模式匹配的所有Web资源/api/**），并且每个组都可以拥有自己的专用AuthenticationManager。通常，每个组都有一个 ProviderManager，他们共享父母。然后，父母就是一种“全局”资源，充当所有提供者的后备资源。 图1. AuthenticationManager使用的 层次结构 ProviderManager 自定义AuthenticationManagerSpring Security提供了一些配置帮助程序，可以快速获取应用程序中设置的常见身份验证管理器功能。最常用的帮助程序AuthenticationManagerBuilder非常适合设置内存、JDBC或LDAP用户详细信息，或用于添加自定义UserDetailsService。以下是配置全局（父）的应用程序示例AuthenticationManager： 123456789101112@Configurationpublic class ApplicationSecurity extends WebSecurityConfigurerAdapter &#123; ... // web stuff here @Autowired public initialize(AuthenticationManagerBuilder builder, DataSource dataSource) &#123; builder.jdbcAuthentication().dataSource(dataSource).withUser(\"dave\") .password(\"secret\").roles(\"USER\"); &#125;&#125; 此示例涉及Web应用程序，但其使用范围AuthenticationManagerBuilder更广泛（有关如何实现Web应用程序安全性的更多详细信息，请参阅下文）。请注意，AuthenticationManagerBuilder是@Autowired进一个方法@Bean- 这是使它构建全局（父）AuthenticationManager的方法。相反，如果我们这样做： 123456789101112131415@Configurationpublic class ApplicationSecurity extends WebSecurityConfigurerAdapter &#123; @Autowired DataSource dataSource; ... // web stuff here @Override public configure(AuthenticationManagerBuilder builder) &#123; builder.jdbcAuthentication().dataSource(dataSource).withUser(\"dave\") .password(\"secret\").roles(\"USER\"); &#125;&#125; （在配置器中使用@Override的方法）然后AuthenticationManagerBuilder仅用于构建“本地” AuthenticationManager，它是全局的子节点。在Spring Boot应用程序中，您可以@Autowired将全局应用程序注入另一个bean，但除非您自己明确地公开它，否则不能使用本地bean。 Spring Boot提供了一个默认的全局AuthenticationManager（只有一个用户），除非你通过提供自己的AuthenticationManager类型的bean来抢占它。除非您主动需要自定义全局AuthenticationManager，否则默认设置足够安全，您不必担心它。如果您执行任何构建AuthenticationManager的配置，您通常可以在本地执行您正在保护的资源，而不必担心全局默认值。 授权或访问控制 一旦身份验证成功，我们就可以继续授权，这里的核心策略是AccessDecisionManager。框架提供了三个实现，并且所有三个委托给一个AccessDecisionVoter链，有点像ProviderManager委托AuthenticationProviders。 一个AccessDecisionVoter考虑一个Authentication（表示主体）和一个被ConfigAttributes装饰的安全的Object： 123456boolean supports(ConfigAttribute attribute);boolean supports(Class&lt;?&gt; clazz);int vote(Authentication authentication, S object, Collection&lt;ConfigAttribute&gt; attributes); 该Object在AccessDecisionManager和AccessDecisionVoter的签名是完全通用的-它代表用户可能要访问的任何东西（网络资源或Java类中的方法是两种最常见的情况）。该ConfigAttributes也相当通用的，它代表安全装饰的Object，它拥有决定访问它所需的权限级别的元数据。ConfigAttribute是一个接口，但它只有一个非常通用的方法并返回一个String，所以这些字符串以某种方式编码资源所有者的意图，表达允许谁访问它的规则。典型的ConfigAttribute是用户角色的名称（如ROLE_ADMIN或ROLE_AUDIT），它们通常具有特殊格式（如ROLE_ 前缀）或表示需要评估的表达式。 大多数人只使用默认AccessDecisionManager——AffirmativeBased（如果没有voters拒绝，则获得准入）。定制都倾向于在voters中发生，要么添加新的定制，要么修改现有定制的方式。 在ConfigAttributes使用Spring表达式语言（SpEL）是很常见的，例如isFullyAuthenticated() &amp;&amp; hasRole(&#39;FOO&#39;)。这是由一个AccessDecisionVoter可以处理表达式并为它们创建上下文的支持。要扩展可以处理的表达式范围，需要自定义实现SecurityExpressionRoot或者SecurityExpressionHandler。 Web安全Web层中的Spring Security（用于UI和HTTP后端）基于Servlet Filters，因此通常首先查看Filters的角色是有帮助的。下图显示了单个HTTP请求的处理程序的典型分层。 客户端向应用程序发送请求，容器根据请求URI的路径决定哪些过滤器和哪个servlet应用于它。最多只有一个servlet可以处理单个请求，但是过滤器形成一个链，因此它们是有序的，实际上，如果过滤器想要处理请求本身，它可以否决链的其余部分。过滤器还可以修改下游过滤器和servlet中使用的请求和/或响应。过滤器链的顺序非常重要，Spring Boot通过两种机制管理它：一种是Filter类型里的@Beans可以有一个@Order或一个Ordered实现，另一种是它们可以是一个FilterRegistrationBean，它本身有一个Order作为其API的一部分。一些现成的过滤器定义自己的常量来帮助指示他们彼此相对的顺序（例如Spring Session中的SessionRepositoryFilter有一个值为Integer.MIN_VALUE + 50的DEFAULT_ORDER，它告诉我们它在链条前段，但它不排除它前面的其他过滤器。 Spring Security作为单个Filter被安装到链中，其具体类型是FilterChainProxy，由于很快就会显现的原因。在Spring Boot应用程序中，安全过滤器是一个位于ApplicationContext中的@Bean，并且默认安装，以便它应用于每个请求。它被安装的位置被定义为SecurityProperties.DEFAULT_FILTER_ORDER，该位置接着被FilterRegistrationBean.REQUEST_WRAPPER_FILTER_MAX_ORDER确定（Spring Boot应用程序期望过滤器在包装请求时修改其行为的最大顺序）。除此之外还有更多：从容器的角度来看，Spring Security是一个单独的过滤器，但在其中有一些额外的过滤器，每个过滤器都扮演着特殊的角色。这是一张图片： 图2. Spring Security是一个物理实体， Filter但将处理委托给一系列内部过滤器 事实上，安全过滤器中甚至还有一层间接：它通常作为一个安装在容器中DelegatingFilterProxy，而不必是Spring @Bean。代理委托给一个FilterChainProxy，它是一个@Bean，通常具有固定名称springSecurityFilterChain。FilterChainProxy包含内部排列为过滤器链（或链）的所有安全逻辑。所有过滤器都具有相同的API（它们都实现了FilterServlet规范中的接口），并且它们都有机会否决链的其余部分。 可以有多个过滤器链，所有过滤器链都由Spring Security在同一顶层FilterChainProxy管理，并且容器都是未知的。Spring Security过滤器包含过滤器链列表，并将请求分派给与其匹配的第一个链。下图显示了基于匹配请求路径（/foo/**之前匹配/**）发生的调度。这是非常常见的，但不是匹配请求的唯一方法。此调度过程的最重要特征是只有一个链处理请求。 图3. Spring Security FilterChainProxy将请求分派给匹配的第一个链。 没有自定义安全配置的Spring Boot应用程序有若干个（称为N）过滤器链，通常n = 6。前（N-1）链那里只是忽略静态资源模式，如/css/**和/images/**以及误差视图/error（路径可以由用户用来控制security.ignored从SecurityProperties配置Bean）。最后一个链匹配所有路径即 /**并且更活跃，包含用于身份验证，授权，异常处理，会话处理，标题写入等的逻辑。默认情况下，此链中总共有11个过滤器，但通常没有必要让用户关注使用哪些过滤器以及何时使用过滤器。 Spring Security内部的所有过滤器都不为容器所知，这一点很重要，尤其是在Spring Boot应用程序中，默认情况下，所有@Beans类型Filter的容器都会自动注册到容器中。因此，如果要将自定义过滤器添加到安全链，则需要将其设置为@Bean或将其包装在FilterRegistrationBean，它明确禁用容器注册的过程。 创建和自定义筛选链Spring Boot应用程序（带有/**请求匹配器的应用程序）中的默认回调过滤器链具有SecurityProperties.BASIC_AUTH_ORDER中预定义的顺序。您可以通过security.basic.enabled=false设置完全关闭它，或者您可以将其用作后备，只需使用较低的顺序定义其他规则。要做到这一点，只需添加一个WebSecurityConfigurerAdapter（或WebSecurityConfigurer）类型的@Bean并用@Order来装饰类。例： 123456789@Configuration@Order(SecurityProperties.BASIC_AUTH_ORDER - 10)public class ApplicationConfigurerAdapter extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.antMatcher(\"/foo/**\") ...; &#125;&#125; 此bean将导致Spring Security添加新的过滤器链并排序在回调之前。 与另一组资源相比，许多应用程序对一组资源具有完全不同的访问规则。例如，承载UI和支持API的应用程序可能支持基于cookie的身份验证，其中重定向到UI部件的登录页面，而基于令牌的身份验证具有401响应未经身份验证的API部件请求。每组资源都有自己WebSecurityConfigurerAdapter的独特顺序和自己的请求匹配器。如果匹配规则重叠，则最早的有序过滤器链将获胜。 请求匹配调度和授权安全过滤器链（或等效的 WebSecurityConfigurerAdapter）具有请求匹配器，用于决定是否将其应用于HTTP请求。一旦决定应用特定过滤器链，则不应用其他过滤器链。但是在过滤器链中，您可以通过在HttpSecurity配置器中设置其他匹配器来对授权进行更精细的控制。例： 123456789101112@Configuration@Order(SecurityProperties.BASIC_AUTH_ORDER - 10)public class ApplicationConfigurerAdapter extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.antMatcher(\"/foo/**\") .authorizeRequests() .antMatchers(\"/foo/bar\").hasRole(\"BAR\") .antMatchers(\"/foo/spam\").hasRole(\"SPAM\") .anyRequest().isAuthenticated(); &#125;&#125; 配置Spring Security最容易犯的错误之一就是忘记这些匹配器适用于不同的进程，一个是整个过滤器链的请求匹配器，另一个是选择要应用的访问规则。 将应用程序安全规则应用于Actuator Rules如果您将Spring Boot Actuator用于管理端点，您可能希望它们是安全的，并且默认情况下它们将是安全的。实际上，只要将Actuator添加到安全应用程序中，您就会获得仅适用于执行器端点的附加过滤器链。它由一个仅匹配执行器端点的请求匹配器定义，并且其顺序ManagementServerProperties.BASIC_AUTH_ORDER比默认的SecurityProperties回调过滤器少5，因此在回退之前会查询它。 如果您希望将应用程序安全规则应用于actuator 端点，则可以添加比actuator 端点更早排序的过滤器链以及包含所有actuator 端点的请求匹配器。如果您更喜欢执行器端点的默认安全设置，那么最简单的方法是在actuator 端点之后添加自己的过滤器，但比回退更早（例如ManagementServerProperties.BASIC_AUTH_ORDER + 1）。例： 123456789@Configuration@Order(ManagementServerProperties.BASIC_AUTH_ORDER + 1)public class ApplicationConfigurerAdapter extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.antMatcher(\"/foo/**\") ...; &#125;&#125; Web层中的Spring Security目前与Servlet API相关联，因此它仅在嵌入式或其他方式在servlet容器中运行应用程序时才真正适用。但是，它不依赖于Spring MVC或Spring Web堆栈的其余部分，因此可以在任何servlet应用程序中使用，例如使用JAX-RS的应用程序。 方法安全除了支持保护Web应用程序外，Spring Security还支持将访问规则应用于Java方法执行。对于Spring Security，这只是一种不同类型的“受保护资源”。对于用户来说，这意味着使用相同格式的ConfigAttribute字符串（例如角色或表达式）声明访问规则，但是在代码中的不同位置。第一步是启用方法安全性，例如在我们的应用程序的顶级配置中： 1234@SpringBootApplication@EnableGlobalMethodSecurity(securedEnabled = true)public class SampleSecureApplication &#123;&#125; 然后我们可以直接装饰方法资源，例如 123456789@Servicepublic class MyService &#123; @Secured(\"ROLE_USER\") public String secure() &#123; return \"Hello Security\"; &#125;&#125; 此示例是具有安全方法的服务。如果Spring创建了@Bean这种类型，那么它将被代理，并且调用者必须在实际执行该方法之前通过安全拦截器。如果访问被拒绝，则调用者将获得AccessDeniedException而不是实际的方法结果。 其他注解可以在方法中使用以执行安全性约束，特别是@PreAuthorize和@PostAuthorize，它分别允许你写含有参数引用的表达式和返回值的方法。 将Web安全性和方法安全性结合起来并不罕见。过滤器链提供用户体验功能，如身份验证和重定向到登录页面等，方法安全性可在更细粒度的级别提供保护。 使用线程Spring Security基本上是线程绑定的，因为它需要使当前经过身份验证的主体可供各种下游消费者使用。基本构建块SecurityContext可能包含一个Authentication（当用户登录时，它将Authentication是明确的authenticated）。您始终可以访问和操作SecurityContext通过静态的方法，在SecurityContextHolder这些方法中依次操作 TheadLocal，例如 123SecurityContext context = SecurityContextHolder.getContext();Authentication authentication = context.getAuthentication();assert(authentication.isAuthenticated); 用户应用程序代码并不经常做这个操作，但它可以是有用的，比如，如果需要编写一个自定义的验证过滤器（虽然即使这样，也有Spring Security的基类，可用于需要避免使用SecurityContextHolder的）。 如果需要访问Web端点中当前经过身份验证的用户，则可以在@RequestMapping标记的方法中使用参数。例如 1234@RequestMapping(\"/foo\")public String foo(@AuthenticationPrincipal User user) &#123; ... // do stuff with user&#125; 此注解将Authentication拉出SecurityContext并调用其getPrincipal()上的方法以生成方法参数。Principalin中的类型Authentication取决于AuthenticationManager用于验证身份验证的内容，因此这对于获取对用户数据的类型安全引用来说是一个有用的小技巧。 如果spring security是使用Principal从HttpServletRequest将型的Authentication，所以你也可以直接使用： 123456@RequestMapping(\"/foo\")public String foo(Principal principal) &#123; Authentication authentication = (Authentication) principal; User = (User) authentication.getPrincipal(); ... // do stuff with user&#125; 如果您需要编写在不使用Spring Security时有效的代码（在加载Authentication类时需要更加防御），这有时会很有用。 异步处理安全方法由于SecurityContext是线程绑定的，如果要进行任何调用安全方法的后台处理，例如使用@Async，则需要确保传播上下文。这归结为包裹SecurityContext与任务（最多Runnable，Callable即在后台执行等）。Spring Security提供了一些帮助，使这更容易，例如包装Runnable和Callable。要传播需要提供的SecurityContextto @Async方法，AsyncConfigurer并确保其Executor类型正确： 123456789@Configurationpublic class ApplicationConfiguration extends AsyncConfigurerSupport &#123; @Override public Executor getAsyncExecutor() &#123; return new DelegatingSecurityContextExecutorService(Executors.newFixedThreadPool(5)); &#125;&#125;","link":"/2019/04/10/Spring-Security架构/"},{"title":"基于MongoDB+Spark的大数据分析解决方案","text":"本文转自：https://www.cnblogs.com/hanson1/p/7105288.html Spark介绍按照官方的定义，Spark 是一个通用，快速，适用于大规模数据的处理引擎。 通用性：我们可以使用Spark SQL来执行常规分析， Spark Streaming 来流数据处理， 以及用Mlib来执行机器学习等。Java，python，scala及R语言的支持也是其通用性的表现之一。 快速： 这个可能是Spark成功的最初原因之一，主要归功于其基于内存的运算方式。当需要处理的数据需要反复迭代时，Spark可以直接在内存中暂存数据，而无需像Map Reduce一样需要把数据写回磁盘。官方的数据表明：它可以比传统的Map Reduce快上100倍。 大规模：原生支持HDFS，并且其计算节点支持弹性扩展，利用大量廉价计算资源并发的特点来支持大规模数据处理。 我们能用它做什么那我们能用Spark来做什么呢？ 场景数不胜数。 最简单的可以只是统计一下某一个页面多少点击量，复杂的可以通过机器学习来预测。 个性化 是一个常见的案例，比如说，Yahoo的网站首页使用Spark来实现快速的用户兴趣分析。应该在首页显示什么新闻？原始的做法是让用户选择分类；聪明的做法就是在用户交互的过程中揣摩用户可能喜欢的文章。另一方面就是要在新闻进来时候进行分析并确定什么样的用户是可能的受众。新闻的时效性非常高，按照常规的MapReduce做法，对于Yahoo几亿用户及海量的文章，可能需要计算一天才能得出结果。Spark的高效运算可以在这里得到充分的运用，来保证新闻推荐在数十分钟或更短时间内完成。另外，如美国最大的有线电视商Comcast用它来做节目推荐，最近刚和滴滴联姻的uber用它实时订单分析，优酷则在Spark上实现了商业智能的升级版。 Spark生态系统在我们开始谈MongoDB 和Spark 之前，我们首先来了解一下Spark的生态系统。 Spark 作为一个大型分布式计算框架，需要和其他组件一起协同工作。 在Hdaoop里面，HDFS是其核心，作为一个数据层。 Spark是Hadoop生态系统的一颗新星，原生就支持HDFS。大家知道HDFS是用来管理大规模非结构化数据的存储系统，具有高可用和巨大的横向扩展能力。 而作为一个横向扩展的分布式集群，资源管理是其核心必备的能力，Spark 可以通过YARN或者MESOS来负责资源（CPU）分配和任务调度。如果你不需要管理节点的高可用需求，你也可以直接使用Spark standalone。 在有了数据层和资源管理层后， 接下来就是我们真正的计算引擎。 hadoop技术的两大基石之一的MapReduce就是用来实现集群大规模并行计算。而现在就多了一个选项：Spark。 Map Reduce的特点是，用4个字来概括，简单粗暴。采用divide &amp; conquer战术，我们可以用Map Reduce来处理PB级的数据。 而Spark 作为打了鸡血的Map Reduce增强版，利用了内存价格大量下降的时代因素，充分把计算所用变量和中间结果放到内存里，并且提供了一整套机器学习的分析算法，在加上很多语言的支持，使之成为一个较之于Map Reduce更加优秀的选择。 由于Map Reduce 是一个相对并不直观的程序接口，所以为了方便使用，一系列的高层接口如Hive或者Pig应运而生。 hive可以让我们使用非常熟悉的SQL语句的方式来做一些常见的统计分析工作。同理，在Spark 引擎层也有类似的封装，如Spark SQL、RDD以及2.0版本新推出的Dataframe等。 所以一个完整的大数据解决方案，包含了存储，资源管理，计算引擎及接口层。 那么问题来了：我们画了这么大这么圆的大饼，mongodb可以吃哪一块呢？ MongoDB是个什么？是个database。 所以自然而然，MongoDB可以担任的角色，就是数据存储的这一部分。在和 Spark一起使用的时候，MongoDB就可以扮演HDFS的角色来为Spark提供计算的原始数据，以及用来持久化分析计算的结果。 HDFS vs. MongoDB既然我们说MongoDB可以用在HDFS的地方，那我们来详细看看两者之间的差异性。 在说区别之前，其实我们可以先来注意一下两者的共同点。HDFS和MongoDB都是基于廉价x86服务器的横向扩展架构，都能支持到TB到PB级的数据量。数据会在多节点自动备份，来保证数据的高可用和冗余。两者都支持非结构化数据的存储，等等。 但是，HDFS和MongoDB更多的是差异点： 如在存储方式上 HDFS的存储是以文件为单位，每个文件64MB到128MB不等。而MongoDB则是细颗粒化的、以文档为单位的存储。 HDFS不支持索引的概念，对数据的操作局限于扫描性质的读，MongoDB则支持基于二级索引的快速检索。 MongoDB可以支持常见的增删改查场景，而HDFS一般只是一次写入后就很难进行修改。 从响应时间上来说，HDFS一般是分钟级别而MongoDB对手请求的响应时间通常以毫秒作为单位。 一个日志的例子如果说刚才的比较有些抽象，我们可以结合一个实际一点的例子来理解。 比如说，一个比较经典的案例可能是日志记录管理。在HDFS里面你可能会用日期范围来命名文件，如7月1日，7月2日等等，每个文件是个日志文本文件，可能会有几万到几十万行日志。 而在MongoDB里面，我们可以采用一个JSON的格式，每一条日志就是一个JSON document。我们可以对某几个关心的字段建索引，如时间戳，错误类型等。 我们来考虑一些场景，加入我们相对7月份所有日志做一些全量的统计，比如每个页面的所有点击量，那么这个HDFS和MongoDB都可以正常处理。 如果有一天你的经理告诉你：他想知道网站上每天有多少404错误在发生，这个时候如果你用HDFS，就还是需要通过全量扫描所有行，而MongoDB则可以通过索引，很快地找到所有的404日志，可能花数秒钟就可以解答你经理的问题。 又比如说，如果你希望对每个日志项加一个自定义的属性，在进行一些预处理后，MongoDB就会比较容易地支持到。而一般来说，HDFS是不支持更新类型操作的。 好的，我们了解了MongoDB为什么可以替换HDFS并且为什么有这个必要来做这个事情，下面我们就来看看Spark和MongoDB怎么玩！ Spark + MongoDBSpark的工作流程可以概括为三部曲：创建并发任务，对数据进行transformation操作，如map， filter，union，intersect等，然后执行运算，如reduce，count，或者简单地收集结果。 这里是Spark和MongoDB部署的一个典型架构。 Spark任务一般由Spark的driver节点发起，经过Spark Master进行资源调度分发。比如这里我们有4个Spark worker节点，这些节点上的几个executor 计算进程就会同时开始工作。一般一个core就对应一个executor。 每个executor会独立的去MongoDB取来原始数据，直接套用Spark提供的分析算法或者使用自定义流程来处理数据，计算完后把相应结果写回到MongoDB。 我们需要提到的是：在这里，所有和MongoDB的交互都是通过一个叫做Mongo-Spark的连接器来完成的。 另一种常见的架构是结合MongoDB和HDFS的。Hadoop在非结构化数据处理的场景下要比MongoDB的普及率高。所以我们可以看到不少用户会已经将数据存放在HDFS上。这个时候你可以直接在HDFS上面架Spark来跑，Spark从HDFS取来原始数据进行计算，而MongoDB在这个场景下是用来保存处理结果。为什么要这么麻烦？几个原因： Spark处理结果数量可能会很大，比如说，个性化推荐可能会产生数百万至数千万条记录，需要一个能够支持每秒万级写入能力的数据库 处理结果可以直接用来驱动前台APP，如用户打开页面时获取后台已经为他准备好的推荐列表。 Mongo Spark Connector 连接器在这里我们在介绍下MongoDB官方提供的 Mongo Spark连接器 。目前有3个连接器可用，包括社区第三方开发的和之前Mongo Hadoop连接器等，这个Mongo-Spark是最新的，也是我们推荐的连接方案。 这个连接器是专门为Spark打造的，支持双向数据，读出和写入。但是最关键的是 条件下推 ，也就是说：如果你在Spark端指定了查询或者限制条件的情况下，这个条件会被下推到MongoDB去执行，这样可以保证从MongoDB取出来、经过网络传输到Spark计算节点的数据确实都是用得着的。没有下推支持的话，每次操作很可能需要从MongoDB读取全量的数据，性能体验将会很糟糕。拿刚才的日志例子来说，如果我们只想对404错误日志进行分析，看那些错误都是哪些页面，以及每天错误页面数量的变化，如果有条件下推，那么我们可以给MongoDB一个限定条件：错误代码=404， 这个条件会在MongoDB服务器端执行，这样我们只需要通过网络传输可能只是全部日志的0.1%的数据，而不是没有条件下推情况下的全部数据。 另外，这个最新的连接器还支持和Spark计算节点Co-Lo 部署。就是说在同一个节点上同时部署Spark实例和MongoDB实例。这样做可以减少数据在网络上的传输带来的资源消耗及时延。当然，这种部署方式需要注意内存资源和CPU资源的隔离。隔离的方式可以通过Linux的cgroups。 Spark + MongoDB 成功案例目前已经有很多案例在不同的应用场景中使用Spark+MongoDB。 法国航空是法国最大的航空公司，为了提高客户体验，在最近施行的360度客户视图中，使用Spark对已经收集在MongoDB里面的客户数据进行分类及行为分析，并把结果（如客户的类别、标签等信息）写回到MongoDB内每一个客户的文档结构里。 Stratio是美国硅谷一家著名的金融大数据公司。他们最近在一家在31个国家有分支机构的跨国银行实施了一个实时监控平台。该银行希望通过对日志的监控和分析来保证客户服务的响应时间以及实时监测一些可能的违规或者金融欺诈行为。在这个应用内， 他们使用了： Stratio是美国硅谷一家著名的金融大数据公司。他们最近在一家在31个国家有分支机构的跨国银行实施了一个实时监控平台。该银行希望通过对日志的监控和分析来保证客户服务的响应时间以及实时监测一些可能的违规或者金融欺诈行为。在这个应用内， 他们使用了： Apache Flume 来收集log Spark来处理实时的log MongoDB来存储收集的log以及Spark分析的结果，如Key Performance Indicators等 东方航空最近刚完成一个Spark运价的POC测试。 东方航空的挑战东方航空作为国内的3大行之一，每天有1000多个航班，服务26万多乘客。过去，顾客在网站上订购机票，平均资料库查询200次就会下单订购机票，但是现在平均要查询1.2万次才会发生一次订购行为，同样的订单量，查询量却成长百倍。按照50%直销率这个目标计算，东航的运价系统要支持每天16亿的运价请求。 思路：空间换时间当前的运价是通过实时计算的，按照现在的计算能力，需要对已有系统进行100多倍的扩容。另一个常用的思路，就是采用空间换时间的方式。与其对每一次的运价请求进行耗时300ms的运算，不如事先把所有可能的票价查询组合穷举出来并进行批量计算，然后把结果存入MongoDB里面。当需要查询运价时，直接按照 出发+目的地+日期的方式做一个快速的DB查询，响应时间应该可以做到几十毫秒。 那为什么要用MongoDB？因为我们要处理的数据量庞大无比。按照1000多个航班，365天，26个仓位，100多渠道以及数个不同的航程类型，我们要实时存取的运价记录有数十亿条之多。这个已经远远超出常规RDBMS可以承受的范围。 MongoDB基于内存缓存的数据管理方式决定了对并发读写的响应可以做到很低延迟，水平扩展的方式可以通过多台节点同时并发处理海量请求。 事实上，全球最大的航空分销商，管理者全世界95%航空库存的Amadeus也正是使用MongoDB作为其1000多亿运价缓存的存储方案。 Spark + MongoDB 方案我们知道MongoDB可以用来做我们海量运价数据的存储方案，在大规模并行计算方案上，就可以用到崭新的Spark技术。 这里是一个运价系统的架构图。 左边是发起航班查询请求的客户端，首先会有API服务器进行预处理。一般航班请求会分为库存查询和运价查询。库存查询会直接到东航已有的库存系统（Seat Inventory），同样是实现在MongoDB上面的。在确定库存后根据库存结果再从Fare Cache系统内查询相应的运价。 Spark集群则是另外一套计算集群，通过Spark MongoDB连接套件和MongoDB Fare Cache集群连接。Spark 计算任务会定期触发（如每天一次或者每4小时一次），这个任务会对所有的可能的运价组合进行全量计算，然后存入MongoDB，以供查询使用。右半边则把原来实时运算的集群换成了Spark+MongoDB。Spark负责批量计算一年内所有航班所有仓位的所有价格，并以高并发的形式存储到MongoDB里面。每秒钟处理的运价可以达到数万条。 当来自客户端的运价查询达到服务端以后，服务端直接就向MongoDB发出按照日期，出发到达机场为条件的mongo查询。 批处理计算流程 这里是Spark计算任务的流程图。需要计算的任务，也就是所有日期航班仓位的组合，事先已经存放到MongoDB里面。 任务递交到master，然后预先加载所需参考数据，broadcast就是把这些在内存里的数据复制到每一个Spark计算节点的JVM，然后所有计算节点多线程并发执行，从Mongodb里取出需要计算的仓位，调用东航自己的运价逻辑，得出结果以后，并保存回MongoDB。 Spark 任务入口程序Spark和MongoDB的连接使用非常简单，下面就是一个代码示例： 1234567891011121314// initialization dependencies including base prices, pricing rules and some reference dataMap dependencies = MyDependencyManager.loadDependencies();// broadcasting dependenciesjavaSparkContext.broadcast(dependencies);// create job rddcabinsRDD = MongoSpark.load(javaSparkContext).withPipeline(pipeline)// for each cabin, date, airport pair, calculate the pricecabinsRDD.map(function calc_price);// collect the result, which will cause the data to be stored into MongoDBcabinsRDD.collect()cabinsRDD.saveToMongo() 处理能力和响应时间比较这里是一个在东航POC的简单测试结果。从吞吐量的角度，新的API服务器单节点就可以处理3400个并发的运价请求。在显著提高了并发的同时，响应延迟则降低了10几倍，平均10ms就可以返回运价结果。按照这个性能，6台 API服务器就可以应付将来每天16亿的运价查询。 Spark ＋ MongoDB 演示接下来是一个简单的Spark+MongoDB演示。 安装 Spark123# curl -OL http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz# mkdir -p ~/spark# tar -xvf spark-1.6.0-bin-hadoop2.6.tgz -C ~/spark --strip-components=1 测试连接器12345678910# cd ～／spark# ./bin/spark-shell \\--conf &quot;spark.mongodb.input.uri=mongodb://127.0.0.1/flights.av&quot; \\--conf &quot;spark.mongodb.output.uri=mongodb://127.0.0.1/flights.output&quot; \\--packages org.mongodb.spark:mongo-spark-connector_2.10:1.0.0import com.mongodb.spark._import org.bson.DocumentMongoSpark.load(sc).take(10).foreach(println) 简单分组统计数据： 365天，所有航班库存信息，500万文档 任务： 按航班统计一年内所有余票量 12345MongoSpark.load(sc) .map(doc=&gt;(doc.getString(&quot;flight&quot;) ,doc.getLong(&quot;seats&quot;))) .reduceByKey((x,y)=&gt;(x+y)) .take(10) .foreach(println) 简单分组统计加条件过滤数据： 365天，所有航班库存信息，500万文档 任务： 按航班统计一年内所有库存，但是只处理昆明出发的航班 12345678import org.bson.DocumentMongoSpark.load(sc) .withPipeline(Seq(Document.parse(&quot;&#123; $match: &#123; orig : &apos;KMG&apos; &#125; &#125;&quot;))) .map(doc=&gt;(doc.getString(&quot;flight&quot;) ,doc.getLong(&quot;seats&quot;))) .reduceByKey((x,y)=&gt;(x+y)) .take(10) .foreach(println) 性能优化事项 使用合适的chunksize (MB)Total data size / chunksize = chunks = RDD partitions = spark tasks 不要将所有CPU核分配给Spark预留1-2个core给操作系统及其他管理进程 同机部署适当情况可以同机部署Spark+MongoDB，利用本地IO提高性能 总结上面只是一些简单的演示，实际上Spark + MongoDB的使用可以通过Spark的很多种形式来使用。我们来总结一下Spark ＋ Mongo的应用场景。在座的同学可能很多人已经使用了MongoDB，也有些人已经使用了Hadoop。我们可以从两个角度来考虑这个事情： 对那些已经使用MongoDB的用户，如果你希望在你的MongoDB驱动的应用上提供个性化功能，比如说像Yahoo一样为你找感兴趣的新闻，能够在你的MongoDB数据上利用到Spark强大的机器学习或者流处理，你就可以考虑在MongoDB集群上部署Spark来实现这些功能。 如果你已经使用Hadoop而且数据已经在HDFS里面，你可以考虑使用Spark来实现更加实时更加快速的分析型需求，并且如果你的分析结果有数据量大、格式多变以及这些结果数据要及时提供给前台APP使用的需求，那么MongoDB可以用来作为你分析结果的一个存储方案。","link":"/2019/08/14/基于MongoDB-Spark的大数据分析解决方案/"},{"title":"精通Pandas探索性分析（四）：像专业人士一样可视化数据","text":"在本章中，我们将学习使用 seaborn 数据可视化库的数据可视化的高级技术。 特别是，我们将涵盖以下主题： 如何启用 Seaborn Seaborn 的特性 绘制不同类型的绘图 用 seaborn 绘制分类图 使用数据感知网格进行绘图 控制绘图美学在本节中，我们将学习如何使用 seaborn 绘图库来控制绘图美学。 我们将学习如何安装 seaborn 并开始使用 seaborn，以及我们需要导入的模型。 我们将探索一些海洋绘图方法来绘制几种不同类型的绘图。 我们还将看到如何使用各种 seaborn 方法和属性来控制和更改绘图美观性。 在开始用 seaborn 创建绘图之前，我们需要先安装它。 在本书中，我们一直在使用 Anaconda 来安装各种 Python 库，因此我们将继续进行下去。 要安装 seaborn，请执行以下命令： 1conda install seaborn复制ErrorOK! 执行命令之前，请确保在管理员模式下运行命令行程序。 现在，我们需要导入本节所需的 Python 模块，如下所示： 1234import pandas as pdfrom matplotlib import pyplot as plt%matplotlib inlineimport seaborn as sns复制ErrorOK! 我们需要导入 Pandas 的 Matplotlib 和 seaborn 模块。 我们正在使用 Matplotlib 的 inline magic 命令来确保我们的绘图连同代码一起正确显示在 Jupyter 笔记本中。 接下来，我们使用 pandas 和以下命令读取数据集： 12df = pd.read_csv('data-alcohol.csv')df.head()复制ErrorOK! 我们的数据集是 CSV 文件。 它由各个国家的酒精消费数据组成。 该数据可通过这里获得。 我们的第一个 seaborn 绘图在本节中，我们将仅使用一个变量来创建分布图，如下所示： 1sns.distplot(df.beer_servings)复制ErrorOK! 在这里，sns指的是 seaborn，我们之前将其导入为sns。 现在我们需要将 seaborn 方法称为distplot，并从我们之前阅读的数据中传入列名。 如下面的屏幕快照所示，这应该可以使我们通过一行代码就能得到一个不错的分布图： 此单行显示了 seaborn 库的强大功能和简单性。 使用set_style更改绘图样式现在是时候让 Seaborn 改变绘图美学了。 在此过程中，我们还将探索许多不同的绘图类型，可以使用 seaborn 进行绘制。 将绘图背景设置为白色网格默认的打印样式是蓝色网格。 我们可以使用以下命令将其更改为whitegrid： 123sns.set()sns.set_style(\"whitegrid\")sns.lmplot(x='beer_servings', y='wine_servings', data=df);复制ErrorOK! Seaborn 提供了一种称为set_style的方法，我们将其称为whitegrid作为参数。 然后，我们调用绘图方法来绘制散点图。 我们正在使用 seaborn 的lmplot方法。 然后，我们从数据集中传递两个列名称为x和y，并将 data 参数设置为我们的 Pandas 数据帧。 现在，我们应该有一个带有白色网格背景的散点图，如以下屏幕截图所示： 将绘图背景设置为黑色现在我们将研究如何将绘图背景设置为dark并且没有网格。 为此，我们使用以下命令将样式设置为dark： 123sns.set()sns.set_style(\"dark\")sns.lmplot(x='beer_servings', y='wine_servings', data=df, fit_reg=False);复制ErrorOK! 您可能已经注意到，我们在开始时还有另一行代码sns.set()。 通过调用此命令，我们在进行任何更改之前将绘图美感重置为默认值。 我们这样做是为了确保我们之前所做的更改不会影响我们的总体规划，如下所示： 将背景设置为白色我们还可以使用以下代码将图的背景设置为实心white且没有网格。 123sns.set()sns.set_style(\"white\")sns.swarmplot(x='country', y='wine_servings', data=df);复制ErrorOK! 输出如下： 添加刻度我们可以通过将style设置为ticks来添加刻度线，如以下代码所示： 123sns.set()sns.set_style(\"ticks\")sns.boxplot(data=df);复制ErrorOK! 前面的代码应为我们提供以下输出： 自定义样式在 seaborn 中，我们可以自定义预设样式，甚至比以前讨论的更多。 让我们向您展示我们可以做什么！ 样式参数首先让我们看一下这些样式组成的所有参数。 我们可以通过在 seaborn 上调用axes_style方法来获取参数，如下所示： 1sns.axes_style()复制ErrorOK! 前面代码的输出如下： 可以进一步自定义上述每个参数。 让我们尝试自定义其中之一，如以下代码片段所示： 123sns.set()sns.set_style(\"ticks\", &#123;\"axes.facecolor\": \".1\"&#125;)sns.boxplot(data=df);复制ErrorOK! 在前面的代码中，我们将style设置为ticks，背景为纯白色，但是我们可以通过分别设置facecolor来进一步自定义。 对于前面的代码，我们将获得以下输出： 请注意，我们可以向此字典添加更多参数，然后继续自定义绘图。 绘图的预定的上下文Seaborn 还提供了一些预设样式上下文。 例如，到目前为止，我们一直在使用的默认样式上下文称为notebook。 但是，还有更多，包括一个叫做paper的。 使用称为set_context的方法设置此上下文，我们将paper作为参数传递，如下所示： 123sns.set()sns.set_context(\"paper\")sns.lmplot(x='beer_servings', y='wine_servings', data=df);复制ErrorOK! 先前代码的输出如下： 还有更多可用的上下文。 例如，称为talk的一个。 我们可以使用talk设置该上下文，如下所示： 1234sns.set()sns.set_context(\"talk\")plt.figure(figsize=(8, 6))sns.lmplot(x='beer_servings', y='wine_servings', data=df);复制ErrorOK! 前面代码的输出如下： 我们可以使用的另一个上下文称为poster，它使用以下代码设置： 1234sns.set()sns.set_context(\"poster\")plt.figure(figsize=(8, 6))sns.lmplot(x='beer_servings', y='wine_servings', data=df);复制ErrorOK! 输出如下： 选择绘图的颜色在本节中，我们将学习使用调色板自定义 seaborn 中的绘图颜色。 我们将探索 seaborn 和 Matplotlib 提供的一些调色板。 我们将学习如何通过设置不同的调色板来更改绘图的颜色，并且还将学习如何使用自定义颜色创建自己的调色板。 首先，使用以下代码导入 Jupyter 笔记本中所需的模块： 1234import pandas as pdfrom matplotlib import pyplot as plt%matplotlib inlineimport seaborn as sns复制ErrorOK! 我们需要导入 Pandas，Matplotlib 和 seaborn。 然后，我们需要读取 CSV 数据集； 我们使用read_csv方法执行此操作，如下所示： 123df = pd.read_csv('data-alcohol.csv')df.head()复制ErrorOK! 前面代码的输出如下： 现在我们需要使用 seaborn 来调用color_palette方法来获取当前的调色板，该调色板是默认设置的。 然后我们使用palplot方法显示这些颜色，如下所示： 1sns.palplot(sns.color_palette())复制ErrorOK! 前面代码的输出如下： 现在让我们看一下该调色板在绘图中的外观： 12sns.set()sns.boxplot(data=df);复制ErrorOK! 前面代码的输出应类似于以下屏幕截图： 在这里，我们在数据集上绘制了箱形图。 您可能会注意到，配色方案看上去与我们在打印默认调色板时看到的相似。 更改调色板让我们继续并更改调色板，以了解它如何影响绘图的颜色。 以下代码将调色板设置为bright（seaborn 的预定义调色板之一）： 1sns.set_palette(\"bright\")复制ErrorOK! 让我们看看如何使用以下命令来改变绘图的颜色： 1sns.boxplot(data=df);复制ErrorOK! 现在，输出应类似于以下屏幕截图： 如您所见，由于我们设置了新的调色板，我们图的配色方案已经完全改变。bright不是 seaborn 中唯一的预定义调色板； 还有其他一些，包括deep，muted，pastel，bright，dark和colorblind，如下所示： 123456sns.palplot(sns.color_palette(\"deep\", 7))sns.palplot(sns.color_palette(\"muted\", 7))sns.palplot(sns.color_palette(\"pastel\", 7))sns.palplot(sns.color_palette(\"bright\", 7))sns.palplot(sns.color_palette(\"dark\", 7))sns.palplot(sns.color_palette(\"colorblind\", 7))复制ErrorOK! 每个调色板的输出如下： Seaborn 还可以将 Matplotlib 的颜色图设置为调色板。 例如： 12sns.palplot(sns.color_palette(\"RdBu\", 7))sns.palplot(sns.color_palette(\"Blues_d\", 7))复制ErrorOK! 上一条命令的输出如下： 现在让我们使用 Matplotlib 颜色图之一作为调色板。 我们使用以下命令执行此操作： 1sns.set_palette(\"Blues_d\")复制ErrorOK! 在这里，我们将调色板设置为Blues_d，这是 Matplotlib 颜色图。 现在，让我们使用以下代码重绘图以查看其影响： 1sns.boxplot(data=df);复制ErrorOK! 前面命令的输出应类似于以下屏幕截图： 如您所见，我们的绘图现在具有来自蓝色色图的调色板。 建立自定义调色板要构建自定义调色板，我们首先需要创建一个列表并为其分配所需的颜色，如下所示： 1234my_palette = ['#4B0082', '#0000FF', '#00FF00', '#FFFF00', '#FF7F00', '#FF0000']sns.set_palette(my_palette)sns.palplot(sns.color_palette())复制ErrorOK! 输出如下： 在前面的屏幕截图中，我们创建了一个名为my_palette的新调色板，具有七种颜色。 然后，我们将调色板设置为新创建的调色板，该调色板向我们展示颜色的外观。 让我们使用以下命令，通过新的自定义调色板查看绘图的外观： 1sns.boxplot(data=df);复制ErrorOK! 前面命令的输出应类似于以下屏幕截图： 绘制分类数据在本节中，我们将了解 seaborn 支持的各种分类图以及如何绘制它们。 我们将演示如何绘制包括散点图，实线图，箱形图，条形图等的图。 我们还将学习如何绘制宽形的分类图。 让我们开始使用以下代码在 Jupyter 笔记本中导入我们的 pandas 模块： 1234import pandas as pdfrom matplotlib import pyplot as plt%matplotlib inlineimport seaborn as sns复制ErrorOK! 除了 Pandas，我们还需要导入 Matplotlib 和 Seaborn Python 库。 然后，我们读取 CSV 数据集，如下所示： 12df = pd.read_csv('data_simpsons_episodes.csv')df.head()复制ErrorOK! 本节中的数据集用于著名的动画电视连续剧《辛普森一家》： 前面的数据集包含每个《辛普森一家》绘图的发行日期，收视率数字，评分以及其他观察结果的序列。 散点图让我们从绘制散点图开始； 我们使用以下命令执行此操作： 1sns.stripplot(x=\"season\", y=\"us_viewers_in_millions\", data=df);复制ErrorOK! 输出应如下所示： 在这里，我们使用了 Seaborn 的stripplot方法。 我们在 x 轴上绘制了季节编号，并在 y 轴上绘制了以百万计的美国观众。 我们还指定了使用的数据帧的名称。 群图现在让我们绘制swarmplot。 为此，我们使用 seaborn 的swarmplot方法： 1sns.swarmplot(x=\"season\", y=\"us_viewers_in_millions\", data=df);复制ErrorOK! 输出如下： 在这里，我们还通过了 x 轴上的季节编号，并在 y 轴上使用了数百万的观众。 箱形图现在，我们使用相同的数据并使用boxplot方法创建箱形图，如下所示： 1sns.boxplot(x=\"season\", y=\"us_viewers_in_millions\", data=df);复制ErrorOK! 上一条命令的输出如下： 提琴图使用violinplot()方法创建小提琴图，如下所示： 1sns.violinplot(x=\"season\", y=\"us_viewers_in_millions\", data=df);复制ErrorOK! 前面代码的输出如下： 条形图要绘制条形图，我们使用以下barplot方法： 1sns.barplot(x=\"season\", y=\"us_viewers_in_millions\", data=df);复制ErrorOK! 输出如下： 请注意，还有另一种条形图可用，这些条形图是使用countplot方法绘制的，如下所示： 1sns.countplot(x=\"season\", data=df);复制ErrorOK! 前面代码的输出如下： 当您要显示每个类别中的观察次数而不是计算第二个变量的状态时，可以使用这种样式的图。 宽形图Seaborn 还支持宽格式的数据图。 让我们阅读以下数据集来演示一个： 12df = pd.read_csv('data-alcohol.csv')df.head()复制ErrorOK! 输出如下： 我们可以使用以下命令创建宽形箱形图： 1sns.boxplot(data=df, orient=\"h\");复制ErrorOK! 前面代码的输出如下： 在这里，我们通过传入数据集并将方向作为h来创建宽幅箱形图。 使用数据感知网格进行绘图在本节中，我们将学习在数据集的不同子集上绘制同一图的多个实例。 我们将学习使用 seaborn 的FacetGrid方法进行网格绘图。 我们还将探索 seaborn 的PairGrid和PairPlot方法进行网格绘图。 让我们从下面的代码在 Jupyter 笔记本中导入 Python 模块开始： 1234import pandas as pdfrom matplotlib import pyplot as plt%matplotlib inlineimport seaborn as sns复制ErrorOK! 现在，我们需要使用以下代码读取第一个 CSV 数据集： 12df = pd.read_csv('data-titanic.csv')df.head()复制ErrorOK! The output from the preceding command is as follows: 使用FacetGrid()方法进行绘图让我们开始研究如何使用FacetGrid方法绘制多维图，如以下代码所示： 123g = sns.FacetGrid(df, col=\"Sex\", hue='Survived')g.map(plt.hist, \"Age\");g.add_legend();复制ErrorOK! 前面代码的输出应类似于以下屏幕截图： 在这里，我们已经使用FacetGrid方法绘制了男性和女性乘客的两个并排直方图。 这种并排显示有助于我们比较按年龄划分的男女乘客的存活率。 为了进行绘制，我们首先使用FacetGrid方法创建了一个网格。 然后，我们将数据集的数据帧列传递为Sex，将hue传递为Survived。 色相代表绘图的深度。 然后，这创建了带有两个分别用于男性和女性乘客的绘图的网格。 然后我们在网格上调用map方法并传递了plt.hist和Age参数，它们绘制了我们的两个直方图。 最后，我们使用add_legend方法添加了图例。 使用PairGrid()方法进行绘图现在让我们看看如何使用PairGrid方法绘制可识别网格的图。 我们正在为此使用 MLB 球员数据集，如以下代码所示： 12mlb = pd.read_csv('data-mlb-players.csv')mlb.head()复制ErrorOK! 输出如下： (Source: http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) 让我们用以下代码创建一个图： 123g = sns.PairGrid(mlb, vars=[\"Height\", \"Weight\"], hue=\"Position\")g.map(plt.scatter);g.add_legend();复制ErrorOK! 在这里，我们已经传递了 MLB 球员的数据集，并将vars设置为包含球员的Height和Weight的列表。 然后我们将hue设置为Position。 然后我们在此网格上使用scatterplot方法调用map。 最后，我们添加了图例，该图例提供了一个2 x 2网格，其中包括身高和体重曲线的所有组合，如以下屏幕截图所示： 这些位置的深度由玩家位置列提供。 使用PairPlot()方法进行绘图通过传递数据集可以直接调用PairPlot，如下所示。 深度由hue和size参数组成： 1sns.pairplot(mlb, hue=\"Position\", size=2.5);复制ErrorOK! 前面的命令为我们提供了3 x 3网格中的多图。 这是因为我们对每个位置都有三个观察值或列，如以下屏幕截图所示： 当前观察到的是Height，Weight和Age。 总结在本章中，我们了解了使用 Seaborn 的数据可视化库进行数据可视化的高级技术。 我们学习了如何开始 seaborn，然后探索了其中的一些功能，包括如何控制绘图的美感，如何选择绘图的颜色等等。 我们学习了如何绘制几种不同类型的图，以及如何使用 seaborn 绘制分类数据。 最后，我们学习了如何使用数据感知网格来创建图。","link":"/2020/12/31/精通Pandas探索性分析（四）：像专业人士一样可视化数据/"},{"title":"精通Pandas探索性分析（三）：处理，转换和重塑数据","text":"在本章中，我们将学习以下主题： 使用inplace参数修改 Pandas 数据帧 使用groupby方法的场景 如何处理 Pandas 中的缺失值 探索 Pandas 数据帧中的索引 重命名和删除 Pandas 数据帧中的列 处理和转换日期和时间数据 处理SettingWithCopyWarning 将函数应用于 Pandas 序列或数据帧 将多个数据帧合并并连接成一个 使用 inplace 参数修改 Pandas 数据帧在本节中，我们将学习如何使用inplace参数修改数据帧。 我们首先将一个真实的数据集读入 Pandas。 然后我们将介绍 pandas 的inplace参数，并查看它如何影响方法的执行最终结果。 我们还将执行带有和不带有inplace参数的方法，以演示inplace的效果。 12import pandas as pdtop_movies = pd.read_table('data-movies-top-grossing.csv', sep=',') 由于它是 CSV 文件，因此我们正在使用 Pandas 的read_csv函数。 现在我们已经将数据集读入了数据帧中，让我们看一些记录： 1top_movies 我们正在使用的数据来自维基百科； 这是迄今为止全球顶级电影的交叉附件数据。 大多数 Pandas 数据帧方法都返回一个新的数据帧。 但是，您可能想使用一种方法来修改原始数据帧本身。 这是inplace参数有用的地方。 让我们在不带inplace参数的数据帧上调用方法以查看其在代码中的工作方式： 1top_movies.set_index('Rank').head() 在这里，我们将其中一列设置为数据帧的索引。 我们可以看到索引已在内存中设置。 现在，让我们检查一下它是否已修改原始数据帧： 1top_movies.head() 我们可以看到在原始数据帧中没有任何变化。set_index方法仅在内存中全新的数据帧中创建了更改，我们可以将其保存在新的数据帧中。 现在让我们看看如果传递inplace参数，它将如何工作： 1top_movies.set_index('Rank', inplace=True) 我们将inplace=True传递给该方法。 现在让我们检查原始的数据帧： 1top_movies.head() 我们可以看到传递inplace=True确实修改了原始数据帧。 并非所有方法都需要使用inplace参数来修改原始数据帧。 例如，rename(columns)方法修改原始的数据帧，而不需要inplace参数： 1top_movies.rename(columns = &#123;'Year': 'Release Year'&#125;).head() 熟悉哪些方法需要inplace，哪些不需要inplace，这是一个好主意。 在本节中，我们学习了如何使用inplace参数修改数据帧。 我们介绍了 Pandas inplace参数，以及它如何影响方法的执行最终结果。 我们探讨了带有inplace参数和不带有inplace参数的方法的执行情况，以证明结果的差异。 在下一节中，我们将学习如何使用groupby方法。 使用groupby方法在本节中，我们将学习如何使用groupby方法将数据拆分和聚合为组。 我们将通过分成几部分来探讨groupby方法的工作方式。 我们将用统计方法和其他方法演示groupby。 我们还将学习groupby方法迭代组数据的能力如何做有趣的事情。 我们将像上一节中一样将pandas模块导入 Jupyter 笔记本中： 1import pandas as pd 然后，我们将读取 CSV 数据集： 12data = pd.read_table('data-zillow.csv', sep=',')data.head() 让我们先问一个问题，看看 Pandas 的groupby方法是否可以帮助我们获得答案。 我们想要获取每个State的平均值Price值： 12grouped_data = data[['State', 'Price']].groupby('State').mean()grouped_data.head() 在这里，我们使用groupby方法按状态汇总数据，并获得每个State的平均值Price。 在后台，groupby方法将数据分成几组，然后我们然后将函数应用于拆分后的数据，然后将结果放在一起并显示出来。 让我们将这段代码分成几部分，看看它是如何发生的。 首先，按以下步骤进行分组： 1grouped_data = data[['State', 'Price']].groupby('State') 我们选择了仅具有State和Price列的数据子集。 然后，我们对该数据调用groupby方法，并将其传递到State列中，因为这是我们希望对数据进行分组的列。 然后，我们将数据存储在一个对象中。 让我们使用list方法打印出这些数据： 1list(grouped_data) 我们还可以遍历拆分组，并对其进行有趣的操作，如下所示： 12for state, grouped_data in data.groupby('State'): print(state, '\\n', grouped_data) 在这里，我们通过State迭代组数据，并以State作为标题发布结果，然后是该State的所有记录的表。 在本节中，我们学习了如何使用groupby方法将数据拆分和聚合为组。 我们将groupby方法分解为多个部分，以探讨其工作方式。 我们用统计方法和其他方法演示了groupby，并且还通过遍历组数据学习了如何通过groupby做有趣的事情。 在下一节中，我们将学习如何使用 Pandas 处理数据中的缺失值。 处理 Pandas 中的缺失值在本节中，我们将探索如何使用各种 Pandas 技术来处理数据集中的缺失数据。 我们将学习如何找出缺少的数据以及从哪些列中找出数据。 我们将看到如何删除所有或大量记录丢失数据的行或列。 我们还将学习如何（而不是删除数据）如何用零或剩余值的平均值填充丢失的记录。 12345import pandas as pddata = pd.read_csv('data-titanic.csv')data.head()data.shapedata.count()#每一列中的记录数 总记录与每列计数之间的差表示该列中缺少的记录数。 在12列中，我们有 3 列缺少值。 例如，Age的891行总数中只有714值；Cabin仅具有204记录的值；Embarked具有889记录的值。 我们可以使用不同的方法来处理这些缺失的值。 一种方法是删除缺少值的任何行，即使是单列也是如此，如下所示： 12data_missing_dropped = data.dropna()#删除只要有空值的行data_missing_dropped.shape 当运行此放置行方法时，我们将结果分配回新的数据帧中。 在891.总数中，仅剩下183记录，但是，这可能会导致丢失大量数据，并且可能无法接受。 另一种方法是只删除那些缺少所有值的行。 这是一个例子： 12data_all_missing_dropped = data.dropna(how=\"all\")#删除所有行为空的行data_all_missing_dropped.shape 为此，我们将dropna方法的how参数设置为all。 代替删除行，另一种方法是用一些数据填充缺少的值。 例如，我们可以使用0填写缺失值，如以下屏幕截图所示： 12data_filled_zeros = data.fillna(0)data_filled_zeros.count() 在这里，我们使用 pandas 的fillna方法，并将0的数值传递到应填充数据的列。您可以看到，现在我们已经用0填充了所有缺少的值，并且因此，所有列的计数已增加到数据集中记录总数。 另外，除了用0填充缺失值外，我们还可以用剩余的现有值的平均值填充它们。 为此，我们在要填充值的列上调用fillna方法，然后将列的平均值作为参数传递： 12data_filled_in_mean = data.copy()data_filled_in_mean.Age.fillna(data.Age.mean(), inplace=True) data_filled_in_mean.count() 例如，在这里，我们用现有值的平均值填充Age的缺失值。 在本节中，我们探讨了如何使用各种 Pandas 技术来处理数据集中的缺失数据。 我们学习了如何找出丢失的数据量以及从哪几列中查找。 我们看到了如何删除所有或很多记录丢失数据的行或列。 我们还看到了如何代替删除，也可以用0或剩余值的平均值来填写缺失的记录。 在下一节中，我们将学习如何在 Pandas 数据帧中进行数据集索引。 在 Pandas 数据帧中建立索引在本节中，我们将探讨如何设置索引并将其用于 Pandas 中的数据分析。 我们将学习如何在读取数据后以及读取数据时在DataFrame上设置索引。 我们还将看到如何使用该索引进行数据选择。 与往常一样，我们首先将pandas模块导入 Jupyter 笔记本： 1import pandas as pd 然后，我们读取数据集： 1data = pd.read_csv('data-titanic.csv') 以下是我们的默认索引现在的样子，它是一个从0开始的数字索引： 1data.head() 让我们将其设置为我们选择的列。 在这里，我们使用set_index方法根据我们的数据将索引设置为乘客的姓名： 1data.set_index('Name') 如您所见，索引已从0的简单数值更改为数据集中乘客的姓名。 接下来，我们将看到在读取数据时如何设置索引。 为此，我们将一个额外的参数index_col传递给read方法： 1data = pd.read_table('data-titanic.csv', sep=',', index_col=3) index_col参数采用单个数字值或值的序列。 在这里，我们传递Name列的索引。 接下来，让我们看看如何使用索引进行数据选择。 在以下屏幕截图中，我们在数据帧上调用loc方法，并传入我们要选择的记录的索引级别： 1dta.loc['Braund, Mr.Owen Harris',:] 在这种情况下，它是数据集中一位乘客的名字。 之所以可以这样做，是因为我们先前将名称设置为数据集的索引。 最后，我们可以将索引重置为更改之前的值。 我们为此使用reset_index方法： 1data.reset_index(inplace=True) 我们正在传递inplace=True，因为我们想在原始数据帧本身中将其重置。 在本节中，我们探讨了如何设置索引并将其用于 Pandas 中的数据分析。 我们还学习了在读取数据后如何在数据帧上设置索引。 我们还看到了如何在从 CSV 文件读取数据时设置索引。 最后，我们看到了一些使我们可以使用索引进行数据选择的方法。 在下一节中，我们将学习如何重命名 Pandas 数据帧中的列。 重命名 Pandas 数据帧中的列在本节中，我们将学习在 Pandas 中重命名列标签的各种方法。 我们将学习如何在读取数据后和读取数据时重命名列，并且还将看到如何重命名所有列或特定列。 首先，将pandas模块导入 Jupyter 笔记本： 1import pandas as pd 我们可以通过几种方法来重命名 Pandas 数据帧中的列。 一种方法是在从数据集中读取数据时重命名列。 为此，我们需要将列名作为列表传递给read_csv方法的names参数： 123list_columns= ['Date', 'Region ID', Region Name', 'State', 'City', 'County', 'Size Rank', 'Price']data = pd.read_csv('data-zillow.csv', names = list_columns)data.head() 在前面的示例中，我们首先创建了所需列名称的列表； 此数字应与实际数据集中的列数相同。 然后，将列表传递给read_csv方法中的names参数。 然后，我们看到我们拥有所需的列名，因此read_csv方法已将列名从默认情况下的文本文件更改为我们提供的名称。 读取数据后，我们还可以重命名列名称。 让我们再次从 CSV 文件中读取数据集，但是这次不提供任何列名。 我们可以使用rename方法重命名列。 让我们首先看一下数据集中的列： 1data.columns 现在，我们在数据帧上调用rename方法，并将列名（旧值和新值）传递给columns参数： 1data.rename(columns=&#123;'RegionName':'Region', 'Metro':'City'&#125;, inplace=True) 在前面的代码块中，我们仅更改了一些列名，而不是全部。 让我们再次调用columns属性，以查看是否确实更改了列名 ， ： 1data.columns 现在，我们在数据集中有了新的列名。 读取数据后，我们还可以重命名所有列，如下所示： 1data.columns = ['Date', 'Region ID', 'Region Name', 'State', 'City', 'County', 'Size Rank','Price'] 我们已经将columns属性设置为一个名称列表，我们希望将所有列都重命名为该名称。 在本节中，我们了解了重命名 Pandas 中列级别的各种方法。 我们学习了在读取数据后如何重命名列，并学习了在从 CSV 文件读取数据时如何重命名列。 我们还看到了如何重命名所有列或特定列。 从 Pandas 数据帧中删除列在本节中，我们将研究如何从 Pandas 的数据集中删除列或行。 我们将详细了解drop()方法及其参数的功能。 首先，我们首先将pandas模块导入 Jupyter 笔记本： 1import pandas as pd 之后，我们使用以下代码读取 CSV 数据集： 12data = pd.read_csv('data-titanic.csv', index_col=3)data.head() 数据集应类似于以下内容： 要从我们的数据集中删除单个列，请使用 pandas drop()方法。drop()方法由两个参数组成。 第一个参数是需要删除的列的名称； 第二个参数是axis。 此参数告诉drop方法是否应该删除行或列，并将inplace设置为True，这告诉该方法将其从原始数据帧本身删除。 在此示例中，我们考虑删除Ticket或列。 的代码如下： 1data.drop('Ticket', axis=1, inplace=True) 执行此操作后，我们的数据集应类似于以下内容： 1data.head() 如果我们仔细观察，很明显Ticket列已从我们的数据集中删除或删除。 要删除多个列，我们将需要删除的列作为列表传递给drop()方法。drop()方法的所有其他参数将保持不变。 让我们看一个如何使用drop()方法消除行的示例。 在此示例中，我们将删除多行。 因此，与其传递列名，不如传递一个列表形式的行索引标签。 以下代码将用于执行此操作： 12data.drop(['Parch', 'Fare'], axis=1, inplace=True)data.head() 结果，传递给drop()方法的对应于乘客姓名的两行将从数据集中删除。 现在，我们将继续仔细研究如何处理日期和时间数据。 处理日期和时间序列数据在本节中，我们将仔细研究如何处理 Pandas 中的日期和时间序列数据。 我们还将看到如何： 将字符串转换为datetime类型，以进行高级datetime序列操作 选择并过滤datetime序列数据 探索序列数据的属性 我们首先将pandas模块导入到我们的 Jupyter 笔记本中： 1import pandas as pd 对于此示例，让我们创建自己的数据帧数据集。 我们可以使用以下代码来做到这一点： 1234dataset = pd.DataFrame(&#123;'DOB': ['1976-06-01', '1980-09-23', '1984-03-30', '1991-12-31', '1994-10-2', '1973-11-11'], 'Sex': ['F', 'M', 'F', 'M', 'M', 'F'], 'State': ['CA', 'NY', 'OH', 'OR', 'TX', 'CA'], 'Name': ['Jane', 'John', 'Cathy', 'Jo', 'Sam', 'Tai']&#125;)) 该数据集包含对应于五个虚构人物的四列和五行。 我们的数据集中存在的行之一是DOB，其中包含五个人的出生日期。 必须检查，，，，DOB，， 列中的数据是否正确。 为此，我们使用以下代码： 1dataset.dtypes 从输出中可以看出，在创建过程中DOB列可能设置为object或string数据类型。 要将其更改为datetime数据类型，我们使用to_datetime()方法并将DOB列传递给它，如下所示： 1dataset.DOB = pd.to_datetime(dataset.DOB) 再次，我们可以使用以下代码来验证是否已将DOB设置为datetime数据类型： 1dataset.dtypes 在继续选择和过滤datetime序列之前，我们需要确保为DOB列设置了索引。 为此，我们使用以下代码： 1dataset.set_index('DOB', inplace=True) 之后，我们的DOB列已准备好进行探索。 如果要查看数据集，可以使用代码字dataset如下所示： 1dataset 在开始过滤之前，我们需要了解有四种可能的方法可以过滤DOB列中的数据。 它们如下： 一年的记录：要显示一年的记录，我们使用以下代码： 1dataset['1980'] 此代码表示将显示当年1980存在的所有记录。 Pandas 不需要我们提及整个日期，因为即使是日期的一部分也会帮助我们产生结果。 特定年份和之后的记录：要显示特定年份和之后的所有记录，我们使用以下代码： 1dataset['1980':] 直到特定年份的记录：要显示直到特定年份（包括该年份）的所有记录，我们需要使用以下代码： 1dataset[':1980'] 几年范围内的记录：要显示给定年份范围内的记录，我们可以使用以下代码： 1dataset['1980':'1984'] 我们还可以使用时间序列属性来最有效地利用datetime序列数据。 使用此功能的缺点是datetime字段必须是列，而不是行。 这可以通过将DOB重置为索引来完成。 这样做如下： 1dataset.reset_index(inplace=True) 我们还需要为datetime列中的每个值获取一年中的相应日期。 可以通过调用dayofyear属性来完成此操作，如下所示： 1dataset.DOB.dt.dayofyear 我们还可以通过调用weekday_name属性来显示星期几，如下所示： 1dataset.DOB.dt.weekday_name 这些是datetime序列数据的方法和属性的一些示例。 在 Pandas 的参考文档中可以找到更多内容，网址为。 处理SettingWithCopyWarning在本节中，我们将学习SettingWithCopyWarning警告以及解决方法。 我们还将看一下可能遇到SettingWithCopyWarning的一些情况，以便我们了解如何摆脱它。 Pandas 的狂热用户肯定会遇到SettingWithCopyWarning。 各种网站，例如 Stack Overflow 和其他论坛，都充斥着有关处理此警告的查询。 它看起来像以下内容： 要了解如何摆脱它，我们需要了解SettingWithCopyWarning实际代表什么。 我们都知道，Pandas 中的不同数据操作会返回数据视图或副本。 修改数据时，这可能会引起问题。SettingWithCopyWarning的目的是警告我们，当我们想修改副本时，我们可能正在尝试修改原始数据，反之亦然。 这种情况通常在链接分配期间发生。 解决方案是使用block方法将患者链合并为一个手术。 这可以帮助 Pandas 知道必须修改哪个数据帧。 为了更好地理解这一点，让我们看下面的示例。 与往常一样，我们首先将pandas模块导入到 Jupyter 笔记本中，如下所示： 1import pandas as pd 然后，我们读取 CSV 数据集： 12data = pd.read_csv('data-titanic.csv')data.head() 此后，我们继续创建一个可能遇到SettingWithCopyWarning的场景。 对于此示例，我们选择Age列为空的记录，并将它们设置为等于Age列中值的平均值。 以下是生成的错误： 1data[data.Age.isnull()].Age = data.Age.mean() 为了确认我们的代码不起作用，我们需要检查是否还有Age为空的记录。 这是通过使用以下代码完成的： 1data[data.Age.isnull()].Age.head() 很明显，存在这样的记录。 为了处理这种情况，我们使用loc方法，如下所示： 1data.loc[data.Age.isnull(), 'Age'] = data.Age.mean 此时，我们需要返回以确认该方法是否已解决SettingWithCopyWarning，我们通过使用以下代码行来完成此操作： 1data[data.Age.isnull()] 我们可以看到，问题尚未解决，因此处理警告的另一种方法是将其关闭。 我们需要记住，我们能够并且应该将其关闭的唯一原因是因为它是警告，而不是错误。 为此，我们将mode.chained_assignment选项设置为None： 1pd.set_option('mode.chained_assignment', None) 不建议使用此解决方案，因为这可能会影响我们的运营结果。 解决此警告的另一种方法是使用is_copy方法。 在这里，我们创建数据帧的新副本并将is_copy设置为None，如下所示： 12data1 = data.loc[data.Age.isnull()]data1.is_copy = None 现在让我们看一下如何将函数应用于 Pandas 序列或数据帧。 将函数应用于 Pandas 序列或数据帧在本节中，我们将学习如何将 Python 的预构建函数和自构建函数应用于 pandas 数据对象。 我们还将学习有关将函数应用于 Pandas 序列和 Pandas 数据帧的知识。 首先，将pandas模块导入 Jupyter 笔记本： 12import pandas as pd import numpy as np 我们将读取我们的 CSV 数据集： 12data = pd.read_csv('data-titanic.csv')data.head() 让我们继续使用 Pandas 的apply方法来应用函数。 在此示例中，我们将使用lambda创建一个函数，如下所示： 1func_lower = lambda x: x.lower() 在这里，我们传递一个值x并将其转换为小写。 然后，我们使用apply()方法将此函数应用于数据集中的Name字段，如下所示： 1data.Name.apply(func_lower) 如果仔细观察，Name字段中的值已转换为小写。 接下来，我们了解如何将函数应用于多个列或整个数据帧中的值。 我们可以使用applymap()方法。 它以类似于apply()方法的方式工作，但是在多列或整个数据帧上。 以下代码描述了如何将applymap()方法应用于Age和Pclass列： 1data[['Age', 'Pclass']].applymap(np.square) 我们还将 Numpy 的secure方法应用于这两个列。 前面的步骤用于预定义函数。 现在，让我们继续创建自己的函数，然后将其应用于值，如下所示： 12def my_func(i): return i + 20 创建的函数是一个简单的函数，它带有一个值，将20添加到其中，然后返回结果。 我们使用applymap()将此函数应用于Age和Pclass列中的每个值，如下所示： 1data[['Age', 'Pclass']].applymap(my_func) 让我们继续学习有关将多个数据帧合并和连接在一起的知识。 将多个数据帧合并并连接成一个本节重点介绍如何使用 Pandas merge()和concat()方法组合两个或多个数据帧。 我们还将探讨merge()方法以各种方式加入数据帧的用法。 我们将从导入pandas模块开始。 让我们创建两个数据帧，其中两个都包含具有相同数据但具有不同记录的相同参数： 12345678910dataset1 = pd.DataFrame(&#123;'Age': ['32', '26', '29'], 'Sex': ['F', 'M', 'F'], 'State': ['CA', 'NY', 'OH']&#125;, index=['Jane', 'John', 'Cathy'])dataset2 = pd.DataFrame(&#123;'Age': ['34', '23', '24', '21'], 'Sex': ['M', 'F', 'F', 'F'], State': ['AZ', 'OR', 'CA', 'WA']&#125;, index=['Dave', 'Kris', 'Xi', 'Jo'])dataset1dataset2 在此示例中，让我们将这两个数据帧垂直放置在一起。 使用 pandas concat()方法通过传递两个数据帧作为其参数来执行此操作： 1pd.concat([dataset1, dataset2]) 我们可以看到dataset2已垂直附加到dataset1。 连接数据集的另一种方法是使用append()方法。 使用此方法获得的结果将与以前的方法相同： 1dataset1.append(dataset2) 到目前为止，我们已经连接了数据集中的行，但是也可以连接列。 对于此示例，让我们创建两个新的数据集，它们具有相同的行级别但具有不同的列，如下所示： 1234567dataset1 = pd.DataFrame(&#123;'Age': ['32', '26', '29'], 'Sex': ['F', 'M', 'F'], 'State': ['CA', 'NY', 'OH']&#125;, index=['Jane', 'John', 'Cathy'])dataset2 = pd.DataFrame(&#123;'City': ['SF', 'NY', 'Columbus'], 'Work Status': ['No', 'Yes', 'Yes']&#125;, index=['Jane', 'John', 'Cathy']) 在这种情况下，我们将水平连接。 要按列连接，我们需要将axis参数传递为1： 1pd.concat([dataset1, dataset2], axis=1) 数据集连接的第三个变体是连接具有不同行和列的数据集。 我们首先创建两个具有不同参数的数据集，如下所示： 1234567dataset1 = pd.DataFrame(&#123;'Age': ['32', '26', '29'], 'Sex': ['F', 'M', 'F'], 'State': ['CA', 'NY', 'OH']&#125;, index=['Jane', 'John', 'Cathy'])dataset2 = pd.DataFrame(&#123;'City': ['SF', 'NY', 'Columbus'], 'Work Status': ['No', 'Yes', 'Yes']&#125;, index=['Jane', 'John', 'Cathy']) 为了对这些数据集执行内部合并，我们将数据帧传递给merge()方法。 我们还指定必须在其上进行合并的列，同时确保我们指定它是内部合并。 您的数据集应类似于下表： 1pd.merge(dataset1, dataset2, on='Name', how='inner') 现在，这意味着我们将两个数据集中的数据放在一起。 它仅包含在两个数据帧中具有通用标签的那些行。 接下来，我们进行外部合并。 这是通过将how参数作为left传递给merge()方法来完成的： 1pd.merge(dataset1, dataset2, on='Name', how='left') 此操作的结果是将保留两个数据集中的行以及仅在第一个数据集中存在的行。 第二个数据集中仅存在的行将被丢弃。 为了进行右合并，我们将how参数设置为right： 1pd.merge(dataset1, dataset2, on='name', how='right') 为了保留所有内容，我们进行了完整的外部合并。 通过将how参数传递为outer来完成完整的外部合并： 现在，即使对于没有值并标记为NaN的列，它也包含所有行，而不管它们是否存在于一个或另一个数据集中，或存在于两个数据集中。 总结在本章中，我们学习了各种 Pandas 技术来操纵和重塑数据。 我们学习了如何使用inplace参数修改 Pandas 数据帧。 我们还学习了可以使用groupby方法的方案。 我们看到了如何处理 Pandas 中缺失的值。 我们探索了 Pandas 数据帧中的索引，以及重命名和删除 Pandas 数据帧中的列。 我们学习了如何处理和转换日期和时间数据。 我们学习了如何处理SettingWithCopyWarning，还了解了如何将函数应用于 Pandas 序列或数据帧。 最后，我们学习了如何合并和连接多个数据帧。 在下一章中，我们将学习使用seaborn Python 库将数据可视化的技术，像一个专家一样。","link":"/2020/12/29/精通Pandas探索性分析（三）：处理，转换和重塑数据/"},{"title":"精通Pandas探索性分析（一）：处理不同种类的数据集","text":"从 CSV 文件读取数据时使用高级选项1234567891011import pandas as pddf = pd.read_csv('IMDB.csv', encoding=\"UTF-8\", header=2,#指定行为列名 index_col=\"Title\",#指定列为索引 usecols=['Title','Genre1'],#选择要读取的列的子集 na_values[''],#将视为NaN的值列表传递给读取函数 skip_blank_lines=False,#默认情况下，read_csv会忽略空白行，但是我们可以通过将skip_blank_lines设置为False来关闭此行 skiprows=[1,3,7],#选择跳过哪些行 skipfooter=2,#从页脚或文件末尾跳过行 nrows=100,#仅读取数据集中的前一百行 ) 查看数据123df.head()df.tail()df.shape 从 Excel 文件读取数据1234567891011121314151617df = pd.read_excel('IMDB.xlsx', sheetname=0)xls_file = pd.ExcelFile('IMDB.xlsx')xls_file.sheet_names#读取多个工作表df1 = xls_file.parse('movies')df2 = xls_file.parse('by genre')df = pd.read_excel('IMDB.xlsx', sheetname=1, #读取第2个表格 header=3,#第4行为标题 header=None,#没有标题 skiprows=7,#跳过开头的行 ski_footer=10,#跳过末尾的行 parse_cols=2,#读取前三列 names=['X','Title','Rating'],#设置列名 index_col='Title',#设置索引列 na_values=[' '],#将列表中的数据视为丢失数据 ) 将 JSON 数据读入 Pandas为了读取 JSON 数据，pandas 提供了一种名为read_json的方法，其中我们传递了要读取的 JSON 数据文件的文件名和位置。 文件位置可以是本地文件，甚至可以是具有有效 URL 方案的互联网。 我们将结果数据帧分配给变量DF。 read_json方法读取 JSON 数据并将其转换为 Pandas 数据帧对象，即表格数据格式，如以下代码所示。 JSON 数据现在可以以数据帧格式轻松访问，可以更轻松地进行操作和浏览： 12movies_json = pd.read_json('IMDB.json')movies_json.head() 读取 HTML 数据pandas 内部使用lxml Python 模块读取 HTML 数据。 您可以通过执行conda install lxml，从命令行程序安装它，如以下屏幕截图所示： 1pd.read_html('IMDB.html') 读取 PICKLE 文件PICKLE 是将任何类型的 Python 对象（包括列表，字典等）转换为字符串的一种方式。 这个想法是，该字符串包含在另一个 Python 脚本中重构对象所需的所有信息。 我们使用read_pickle方法读取我们的 PICKLE 文件，如以下代码所示。 与其他数据格式一样，Pandas 根据读取的数据创建数据帧： 12df = pd.read_pickle('IMDB.p')df.head() ApacheCN 数据科学和机器学习译文集 TutorialsPoint NumPy 教程 NumPy 秘籍中文第二版 零、前言 一、使用 IPython 二、高级索引和数组概念 三、掌握常用函数 四、将 NumPy 与世界的其他地方连接 五、音频和图像处理 六、特殊数组和通用函数 七、性能分析和调试 八、质量保证 九、使用 Cython 加速代码 十、Scikits 的乐趣 十一、最新最强的 NumPy 十二、使用 NumPy 进行探索性和预测性数据分析 NumPy 初学者指南中文第三版 零、前言 一、NumPy 快速入门 二、从 NumPy 基本原理开始 三、熟悉常用函数 四、为您带来便利的便利函数 五、使用矩阵和 ufunc 六、深入探索 NumPy 模块 七、了解特殊例程 八、通过测试确保质量 九、matplotlib 绘图 十、当 NumPy 不够用时 - SciPy 及更多 十一、玩转 Pygame 附录 A：小测验答案 附录 B：其他在线资源 附录 C：NumPy 函数的参考 NumPy 基础知识 零、前言 一、NumPy 简介 二、NumPy ndarray对象 三、使用 NumPy 数组 四、NumPy 核心和子模块 五、NumPy 中的线性代数 六、NumPy 中的傅立叶分析 七、构建和分发 NumPy 代码 八、使用 Cython 加速 NumPy 九、NumPy C-API 简介 十、扩展阅读 精通 NumPy 数值分析 零、前言 一、使用 NumPy 数组 二、NumPy 线性代数 三、使用 NumPy 统计函数对波士顿住房数据进行探索性数据分析 四、使用线性回归预测房价 五、使用 NumPy 对批发分销商的客户进行聚类 六、NumPy，SciPy，Pandas 和 Scikit-Learn 七、高级 NumPy 八、高性能数值计算库概述 九、性能基准 NumPy 数组学习手册 零、前言 一、NumPy 入门 二、NumPy 基础 三、使用 NumPy 的基本数据分析 四、使用 NumPy 的简单预测性分析 五、信号处理技术 六、性能分析，调试和测试 七、Python 科学生态系统 精通 SciPy 零、前言 一、数值线性代数 二、插值和近似 三、微分与积分 四、非线性方程式和最优化 五、常微分方程的初值问题 六、计算几何 七、描述性统计 八、推断和数据分析 九、数字图像处理 Pandas 秘籍 零、前言 一、Pandas 基础 二、数据帧基本操作 三、开始数据分析 四、选择数据子集 五、布尔索引 六、索引对齐 七、分组以进行汇总，过滤和转换 八、将数据重组为整齐的表格 九、组合 Pandas 对象 十、时间序列分析 十一、Pandas，Matplotlib 和 Seaborn 的可视化 Pandas 学习手册中文第二版 零、前言 一、Pandas 与数据分析 二、启动和运行 Pandas 三、用序列表示单变量数据 四、用数据帧表示表格和多元数据 五、数据帧的结构操作 六、索引数据 七、类别数据 八、数值统计方法 九、存取数据 十、整理数据 十一、合并，连接和重塑数据 十二、数据聚合 十三、时间序列建模 十四、可视化 十五、历史股价分析 精通 Pandas 零、前言 一、Pandas 和数据分析简介 二、Pandas 安装和支持软件 三、Pandas 数据结构 四、Pandas 的操作，第一部分 – 索引和选择 五、Pandas 的操作，第二部分 – 数据的分组，合并和重塑 六、处理缺失数据，时间序列和 Matplotlib 绘图 七、统计之旅 – 经典方法 八、贝叶斯统计简介 九、Pandas 库体系结构 十、R 与 Pandas 的比较 十一、机器学习简介 NumPy 和 Pandas 数据分析实用指南 零、前言 一、配置 Python 数据分析环境 二、探索 NumPy 三、NumPy 数组上的运算 四、Pandas 很有趣！ 什么是 Pandas？ 五、Pandas 的算术，函数应用以及映射 六、排序，索引和绘图 精通 Pandas 探索性分析 零、前言 一、处理不同种类的数据集 二、数据选择 三、处理，转换和重塑数据 四、像专业人士一样可视化数据 Matplotlib 3.0 秘籍 零、前言 一、Matplotlib 的剖析 二、基本绘图入门 三、绘制多个图表和子图 四、开发可视化来提高发布质量 五、使用高级功能的绘图 六、嵌入文本和表达式 七、以不同格式保存图形 八、开发交互式绘图 九、在图形用户界面中嵌入绘图 十、使用mplot3d工具包绘制 3D 图形 十一、使用axisartist工具包 十二、使用axes_grid1工具包 十三、使用 Cartopy 工具包绘制地理地图 十四、使用 Seaborn 工具包的探索性数据分析 Matplotlib 绘图秘籍 零、前言 一、第一步 二、自定义颜色和样式 三、处理标注 四、处理图形 五、文件输出 六、处理地图 七、处理 3D 图形 八、用户界面 Sklearn 秘籍 第一章 模型预处理 第二章 处理线性模型 第三章 使用距离向量构建模型 第四章 使用 scikit-learn 对数据分类 第五章 模型后处理 Sklearn 学习手册 一、机器学习 - 温和的介绍 二、监督学习 三、无监督学习 四、高级功能 UCSD COGS108 数据科学实战中文笔记 零、数据科学实战 一、Jupyter 笔记本 二、数据分析 三、Python 四、Python 中的数据科学 五、数据收集 六、数据整理 七、数据清理 八、数据隐私和匿名化 九、使用 Python 进行数据可视化 十、分布 十一、检验分布 十三、普通最小二乘 十四、线性模型 十五、聚类 十六、降维 十七、分类 十八、自然语言处理 附录一、有用的 Python 数据科学包 附录二、git/Github 版本控制工具 USF MSDS501 计算数据科学中文讲义 一、起步 1.1 一些动机（音频处理） 1.2 Python 工具的初次尝试 1.3 播放声音 二、设计和构建程序 2.1 编程导论 2.2 在内存中表示数据 2.3 计算模型 2.4 Python 中的编程模式 2.5 数据别名 2.6 使用函数组织你的代码 2.7 如何阅读代码 2.8 面向对象编程 三、关键编程模式 3.1 加载文件 3.2 数据帧 3.3 操纵和可视化数据 四、用于计算和优化的迭代式方法 4.1 生成均匀的随机数 4.2 近似平方根 4.3 单变量梯度下降 五、常见编程工具 5.1 使用 bash 走向胜利 5.2 使用 git 版本控制工具 5.3 在 Amazon Web Services 上启动虚拟机 六、可选 6.1 链表 fast.ai 数值线性代数讲义中文版 v2 一、我们为什么在这里 二、SVD 背景消除 三、使用 NMF 和 SVD 的主题建模 四、随机化 SVD 五、LU 分解 六、使用鲁棒回归的 CT 扫描的压缩感知 七、线性回归和健康结果 八、如何实现线性回归 九、PageRank 和特征值分解 十、实现 QR 分解 SciPyCon 2018 sklearn 教程 一、Python 机器学习简介 二、Python 中的科学计算工具 三、数据表示和可视化 四、训练和测试数据 五、监督学习第一部分：分类 六、监督学习第二部分：回归分析 七、无监督学习第一部分：变换 八、无监督学习第二部分：聚类 九、sklearn 估计器接口回顾 十、案例学习：泰坦尼克幸存者 十一、文本特征提取 十二、案例学习：用于 SMS 垃圾检测的文本分类 十三、交叉验证和得分方法 十四、参数选择、验证和测试 十五、估计器流水线 十六、模型评估、得分指标和处理不平衡类别 十七、深入：线性模型 十八、深入：决策树与森林 十九、自动特征选择 二十、无监督学习：层次和基于密度的聚类算法 二十一、无监督学习：非线性降维 二十二、无监督学习：异常检测 二十三、核外学习 - 用于语义分析的大规模文本分类 社交媒体挖掘 第一部分 数据挖掘 1 应了解的编程语言 2 从哪里获取数据 3 用代码获取数据 4 收集自己的 FACEBOOK 数据 5 抓取实时站点 第二部分 数据分析 6 数据分析导论 7 数据可视化 8 数据分析的高级工具 9 在 REDDIT 数据中寻找趋势 10 测量公众人物的 Twitter 活动 11 何去何从 附录 1 编写程序通过 API 获取网站的信息 2 通过解析网页直接获取哔哩某播主的详细信息 3 在离线表格软件中打开和处理 csv 文件 Python 机器学习在线指南 作者 引言 核心概念 交叉验证 线性回归 过拟合和欠拟合 正则化 监督学习 逻辑回归 朴素贝叶斯分类 决策树 k 最近邻 线性支持向量机 无监督学习 聚类 主成分分析 深度学习 多层感知机 卷积神经网络 自编码器 原文的协议 数据科学和人工智能技术笔记 一、向量、矩阵和数组 二、数据准备 三、数据预处理 四、图像预处理 五、文本预处理 六、日期时间预处理 七、特征工程 八、特征选择 九、模型验证 十、模型选择 十一、线性回归 十二、逻辑回归 十三、树和森林 十四、K 最近邻 十五、支持向量机 十六、朴素贝叶斯 十七、聚类 十八、Keras 十九、数据整理（上） 十九、数据整理（下） 二十、数据可视化 二十一、统计学 写给人类的机器学习 一、为什么机器学习重要 2.1 监督学习 2.2 监督学习 II 2.3 监督学习 III 三、无监督学习 四、神经网络和深度学习 五、强化学习 六、最好的机器学习资源 机器学习超级复习笔记 一、处理不同种类的数据集在本章中，我们将学习如何在 Pandas 中使用不同种类的数据集格式。 我们将学习如何使用 Pandas 导入的 CSV 文件提供的高级选项。 我们还将研究如何在 Pandas 中使用 Excel 文件，以及如何使用read_excel方法的高级选项。 我们将探讨其他一些使用流行数据格式的 Pandas 方法，例如 HTML，JSON，PKL 文件，SQL 等。 从 CSV 文件读取数据时使用高级选项在本部分中，我们将 CSV 和 Pandas 结合使用，并学习如何使用read_csv方法读取 CSV 数据集以及高级选项。 导入模块首先，我们将使用以下命令导入pandas模块： 1import pandas as pd复制ErrorOK! 要读取 CSV 文件，我们使用read_csv方法，如下所示： 12df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\")df.head()复制ErrorOK! 为了执行基本导入，请将数据集的文件名传递给read_csv，并将结果数据帧分配给变量。 在以下屏幕截图中，我们可以看到 Pandas 已将数据集转换为表格格式： 高级读取选项在 Python 中，pandas 具有read_csv方法的许多高级选项，您可以在其中控制如何从 CSV 文件读取数据。 处理列，索引位置和名称默认情况下，read_csv将 CSV 文件第一行中的条目视为列名。 我们可以通过将header设置为None来关闭此功能，如以下代码块所示： 12df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", header=None)df.head()复制ErrorOK! 输出如下： 指定另一行作为标题您还可以通过将行号传递给header选项，从而从其他行（而不是默认的第一行）设置列名，如下所示： 123df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", header=2)df.head()复制ErrorOK! 输出如下： 将列指定为索引默认情况下，read_csv在读取数据时分配一个默认的数字索引，该索引从零开始。 但是，您可以通过将列名传递给索引列选项来更改此行为。 然后，Pandas 会将索引设置为此列，如以下代码所示： 12df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", index_col='Title')df.head()复制ErrorOK! 在这里，我们传递了电影标题作为索引名称。 现在，索引名称为Title，而不是默认的数字索引，如以下屏幕截图所示： 选择要读取的列的子集我们还可以选择读取 CSV 文件中特定列的子集。 为此，我们将列名作为列表传递，以使用columns选项，如下所示： 12df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", usecols=['Title', 'Genre1'])df.head()复制ErrorOK! 前面的代码段的输出如下： 处理缺失和不适用的数据接下来，我们将看到如何通过读取 CSV 文件来处理丢失的数据。 默认情况下，read_csv认为缺少以下值并将其标记为NaN： 但是，您可以添加到此列表。 为此，只需将要视为NaN的值列表传递给，如以下代码所示： 1df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", na_values=[''])复制ErrorOK! 选择是否跳过空白行有时整行没有值； 因此，我们可以在读取数据时选择处理这些行。 默认情况下，read_csv会忽略空白行，但是我们可以通过将skip_blank_lines设置为False来关闭此行，如下所示： 1df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", skip_blank_lines=False)复制ErrorOK! 数据解析选项我们可以通过读取 CSV 文件来选择跳过哪些行。 我们可以将行号作为列表传递给skiprows选项。 第一行的索引为零，如下所示： 123df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", skiprows = [1,3,7])df.head()复制ErrorOK! 输出如下： 从文件的页脚或结尾跳过行要从页脚或文件末尾跳过行，请使用skipfooter选项并传递一个数字，该数字指定要跳过的行数。 在以下代码中，我们通过了2。 如我们所见，在跳过最后两行之后，我们创建的上一个数据帧与我们创建的数据帧之间存在差异： 123df.tail(2)df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", skipfooter=2, engine='python')df.tail(2)复制ErrorOK! 以下屏幕截图显示了输出： 读取文件的子集或一定数量的行有时数据文件太大，我们只想看一下前几行。 我们可以通过将要导入的行数传递到nrows选项来做到这一点，如以下代码所示。 在这里，我们将100传递给nrows，然后nrows仅读取数据集中的前一百行： 12df = pd.read_csv('IMDB.csv', encoding = \"ISO-8859-1\", nrows=100)df.shape复制ErrorOK! 从 Excel 文件读取数据在本节中，我们将学习如何使用 Pandas 使用 Excel 数据来处理表格，以及如何使用 Pandas 的read_excel方法从 Excel 文件中读取数据。 我们将阅读并探索一个真实的 Excel 数据集，并使用 xplore 解析一些可用于解析 Excel 数据的高级选项。 熊猫内部使用 Python Excel 库rd从 Excel 文件中提取数据。 我们可以通过执行conda install xlrd来安装它。 首先，请确保命令行程序在安装前以管理员模式运行，如以下屏幕截图所示： 以下屏幕截图显示了我们将使用 Pandas 阅读和探索的 Excel 数据集： 上一个屏幕截图是电影分级的集合，可以在这里找到它。 基本的 Excel 读取我们正在使用 Pandas 的read_excel方法读取此数据。 以最简单的格式，我们只是将想要的 Excel 数据集的文件名传递给read_excel方法。 pandas 将 Excel 文件中的数据转换为 Pandas 数据帧。 Pandas 内部为此使用 Excel rd库。 在这里，Pandas 已读取数据并在内存中创建了表格数据对象，我们可以在我们的代码中访问，浏览和操作，如以下代码所示： 123df = pd.read_excel('IMDB.xlsx')df.head()复制ErrorOK! 前一个代码块的输出如下： pandas 有很多高级选项，我们可以使用它们来控制应如何读取数据。如以下屏幕截图所示： 指定应读取的工作表要指定应读取的纸张，请将值传递给sheetname选项。 如下面的屏幕快照所示，我们只是传递0，它是 Excel 工作表中第一张工作表的索引值。 这非常方便，尤其是当我们不知道确切的工作表名称时： 12df = pd.read_excel('IMDB.xlsx', sheetname=0)df.head()复制ErrorOK! 输出如下： 从多张表读取数据Excel 数据集文件附带数据和多个工作表。 实际上，这是许多用户更喜欢 Excel 而不是 CSV 的主要原因之一。 幸运的是，Pandas 支持从多张纸中读取数据。 查找工作表名称要找出工作表的名称，请将 Excel 文件传递到ExcelFile类，然后在结果对象上调用sheet_names属性。 该类将 Excel 文件中的图纸名称打印为列表。 如果我们想从名为data-movies的工作表中读取数据，它将类似于以下代码片段： 12xls_file = pd.ExcelFile('IMDB.xlsx')xls_file.sheet_names复制ErrorOK! 接下来，我们在之前创建的 Excel 文件对象上调用parse方法，并传入我们想要读取的工作表名称。 然后我们将结果分配给两个单独的数据帧对象，如下所示： 123df1 = xls_file.parse('movies')df2 = xls_file.parse('by genre')df1.head()复制ErrorOK! 现在，我们从两个单独的数据帧，中的两个工作表中获取数据，如以下屏幕截图所示： 选择标题或列标签默认情况下，pandas 会将列名称或标题设置为 Excel 文件中第一个非空白行的值。 但是，我们可以更改此行为。 在以下屏幕截图中，我们将值3传递给header选项，该选项告诉read_excel方法设置索引行3中的标题名称： 12df = pd.read_excel('IMDB.xlsx', sheetname=1, header=3)df.head()复制ErrorOK! 前面代码的输出如下： 没有标题我们还可以告诉read_excel忽略标题并将所有行都视为记录。 只要 Excel 没有标题行，就很方便。 为此，我们将header设置为None和，如以下代码所示： 12df = pd.read_excel('IMDB.xlsx', sheetname=1, header=None)df.head()复制ErrorOK! 输出如下： 在开头跳过行要跳过文件开头的行，只需将skiprows设置为要跳过的行数，如以下代码所示： 1df = pd.read_excel('IMDB.xlsx', sheetname=1, skiprows=7)复制ErrorOK! 在末尾跳过行为此，我们使用skip_footer选项，如下所示： 1df = pd.read_excel('IMDB.xlsx', sheetname=1, ski_footer=10)复制ErrorOK! 选择列我们还可以选择只读取列的子集。 这是通过将parse_cols选项设置为数值来完成的，这将导致将列从0读取到我们设置解析列值的任何索引。 我们在这种情况下设置了parse_cols=2，它将读取 Excel 文件中的前三列，如以下代码片段所示： 12df = pd.read_excel('IMDB.xlsx', sheetname= 0, parse_cols=2)df.head()复制ErrorOK! 以下是输出： 列名我们可以选择给列使用不同的名称，而不是标题行中提供的默认名称。 为此，我们将列名列表传递给names参数，如下所示： 123df = pd.read_excel('IMDB.xlsx', sheetname=0, parse_cols = 2, names=['X','Title', 'Rating'], )df.head()复制ErrorOK! 在下面的屏幕截图中，我们将列名设置为读取时传递的名称： 读取数据时设置索引默认情况下，read_excel用数字索引标记零，从0开始。 我们可以将索引或行标签设置为更高的值或我们的选择。 为此，我们将数据集的列名传递给index_col选项。 在以下代码中，我们将索引设置为Title列： 12df = pd.read_excel('IMDB.xlsx', sheetname=0, index_col='Title')df.head()复制ErrorOK! 输出如下： 读取时处理丢失的数据read_excel方法有一个值列表，它将被视为丢失，然后将其设置为NaN。 我们可以在使用na_values参数传递值列表时添加此代码，如以下代码所示： 1df = pd.read_excel('IMDB.xlsx', sheetname= 0, na_values=[' '])复制ErrorOK! 读取其他流行格式的数据在本节中，我们将探索 Pandas 的功能，以读取和使用各种流行的数据格式。 我们还将学习如何从 JSON 格式，HTML 文件和 PICKLE 数据集中读取数据，并且可以从基于 SQL 的数据库中读取数据。 读取 JSON 文件JSON 是用于结构化数据的最小可读格式。 它主要用于在服务器和 Web 应用之间传输数据，以替代 XML，如以下屏幕快照所示： 将 JSON 数据读入 Pandas为了读取 JSON 数据，pandas 提供了一种名为read_json的方法，其中我们传递了要读取的 JSON 数据文件的文件名和位置。 文件位置可以是本地文件，甚至可以是具有有效 URL 方案的互联网。 我们将结果数据帧分配给变量DF。 read_json方法读取 JSON 数据并将其转换为 Pandas 数据帧对象，即表格数据格式，如以下代码所示。 JSON 数据现在可以以数据帧格式轻松访问，可以更轻松地进行操作和浏览： 12movies_json = pd.read_json('IMDB.json')movies_json.head()复制ErrorOK! 上一个代码块将产生以下输出： 读取 HTML 数据pandas 内部使用lxml Python 模块读取 HTML 数据。 您可以通过执行conda install lxml，从命令行程序安装它，如以下屏幕截图所示： 我们还可以从本地文件甚至直接从互联网导入 HTML 数据： 在这里，我们将 HTML 文件或 URL 的位置传递给read_html方法。read_html从 HTML 提取表格数据，然后将其转换为 Pandas 数据帧。 在以下代码中，我们以表格格式获取了从 HTML 文件提取的数据： 1pd.read_html('IMDB.html')复制ErrorOK! 输出如下： 读取 PICKLE 文件酸洗是将任何类型的 Python 对象（包括列表，字典等）转换为字符串的一种方式。 这个想法是，该字符串包含在另一个 Python 脚本中重构对象所需的所有信息。 我们使用read_pickle方法读取我们的 PICKLE 文件，如以下代码所示。 与其他数据格式一样，Pandas 根据读取的数据创建数据帧： 12df = pd.read_pickle('IMDB.p')df.head()复制ErrorOK! 输出如下： 读取 SQL 数据在这里，我们将从流行的数据库浏览器 SQLite 中读取 SQL 数据，可以通过执行以下命令进行安装： 1conda install sqlite 1234import sqlite3conn = sqlite3.connect(\"IMDB.sqlite\")df = pd.read_sql_query(\"SELECT * FROM IMDB;\", conn)df.head() 从剪贴板读取数据使用 pandas 的read_clipboard方法读取数据并创建一个数据帧，如下所示： 12df = pd.read_clipboard()df.head() 总结在本章中，我们学习了如何在 Pandas 中使用不同种类的数据集格式。 我们学习了在导入 CSV 文件时如何使用 Pandas 提供的高级选项。 我们还看到了如何使用 Excel 数据集，并且探讨了可用于处理各种数据格式（例如 HTML，JSON，PICKLE 文件，SQL 等）的方法。 在下一章中，我们将学习如何在高级数据选择中使用 Pandas 技术。","link":"/2020/12/28/精通Pandas探索性分析（一）：处理不同种类的数据集/"},{"title":"使用Debezium和Kafka为MySQL数据库设置变化数据捕捉CDC","text":"讲解本教程将引导您逐步运行Debezium 1.0.2.Final，以进行更改数据捕获（CDC）。您将使用Docker（1.9或更高版本）启动Debezium服务，运行带有简单示例数据库的MySQL数据库服务器，使用Debezium监视数据库，并查看结果事件流随着数据库中数据的更改而响应。 您已经完成了教程？使用Docker Compose 尝试快速入门，包括Debezium支持的所有数据库（MySQL，Postgres，MongoDB，SQL Server和Oracle）的示例设置。 什么是Debezium？Debezium是一个分布式平台，可将您现有的数据库转换为事件流，因此应用程序可以查看数据库中的每个行级更改并立即对其做出响应。Debezium建立在Apache Kafka之上，并提供Kafka Connect兼容连接器，用于监视特定的数据库管理系统。Debezium在Kafka日志中记录数据更改的历史记录，您的应用程序从该位置开始使用它们。这使您的应用程序可以轻松，正确，完全使用所有事件。即使您的应用程序停止（或崩溃），在重新启动后，它也会开始使用中断处的事件，因此不会丢失任何内容。 Debezium 1.0.2.Final通过其MySQL连接器支持监视MySQL数据库服务器，这就是本演示中将使用的内容。在将来的版本中将添加对其他DBMS的支持。 使用Docker运行Debezium运行Debezium涉及三项主要服务：ZooKeeper，Kafka和Debezium的连接器服务。本教程将引导您使用Docker和Debezium的Docker映像启动这些服务的单个实例。另一方面，生产环境要求运行每个服务的多个实例以提供性能，可靠性，复制和容错能力。这可以通过OpenShift和Kubernetes之类的平台来完成，该平台管理在多个主机和机器上运行的多个Docker容器，但是通常您需要将其安装在专用硬件上。 启动Docker确保Docker已安装并在Linux，OS X或Windows上运行。我们强烈建议在这些平台上使用最新版本的Docker，并且牢记这些说明。（通过Docker Machine在虚拟机中运行Docker不再是首选方法，Docker建议您升级。） 从Debezium开始简单为了进行简单的评估和实验，本教程将引导您在本地计算机上的单独容器中启动每个服务的单个实例。ZooKeeper和Kafka都在容器内部本地存储数据，正常使用需要将主机目录作为卷安装在主机上，以便在容器停止时保留持久数据。尽管我们的Docker映像的文档描述了如何执行此操作，但在本教程中我们将跳过该操作。这意味着，当删除容器时，所有持久数据将丢失。这实际上是我们实验的理想选择，因为完成后您的计算机上将没有任何东西，而且您可以多次运行此实验，而不必清理它们之间的任何内容。 在本地运行多个服务可能会造成混乱，因此我们将使用一个单独的终端在前台运行每个容器。这样，容器的所有输出将显示在用于运行它的终端中。 这不是运行Docker容器的唯一方法。-itDocker无需在前台运行容器（使用），而是让您以分离模式运行容器（使用-d），在该模式下，容器已启动，并且Docker命令立即返回。分离模式容器不会在终端中显示其输出，尽管您始终可以使用来查看输出docker logs --follow --name &lt;container-name&gt;。这是我们为运行的每个容器命名的原因之一。有关更多详细信息，请参阅Docker文档。 启动ZooKeeper在组成Debezium的所有不同服务/过程中，第一个启动的是ZooKeeper。运行以下命令，启动一个新的终端并使用ZooKeeper启动一个容器： 1$ docker run -it --rm --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 debezium/zookeeper:1.0 这将使用debezium/zookeeper映像的1.0版运行一个新容器，并将名称分配zookeeper给该容器。该-it标志使容器具有交互性，这意味着它将终端的标准输入和输出附加到容器，以便您可以看到容器中正在发生的事情。该--rm标志指示Docker在停止时删除容器。这三个-p选项将容器的三个端口（例如2181、2888和3888）映射到Docker主机上的相同端口，以便其他容器（和容器外部的软件）可以与ZooKeeper进行通信。 您应该在终端中看到ZooKeeper的典型输出： 123456789101112131415161718192021222324252627282930Starting up in standalone modeZooKeeper JMX enabled by defaultUsing config: /zookeeper/conf/zoo.cfg2017-09-21 07:15:55,417 - INFO [main:QuorumPeerConfig@134] - Reading configuration from: /zookeeper/conf/zoo.cfg2017-09-21 07:15:55,419 - INFO [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 32017-09-21 07:15:55,419 - INFO [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 12017-09-21 07:15:55,420 - WARN [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running in standalone mode2017-09-21 07:15:55,420 - INFO [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.2017-09-21 07:15:55,425 - INFO [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.2017-09-21 07:15:55,427 - INFO [main:QuorumPeerConfig@134] - Reading configuration from: /zookeeper/conf/zoo.cfg2017-09-21 07:15:55,427 - INFO [main:ZooKeeperServerMain@96] - Starting server2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:host.name=51b46dd211d02017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.version=1.8.0_1312017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.vendor=Oracle Corporation2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-3.b12.el7_3.x86_64/jre2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.class.path=/zookeeper/bin/../build/classes:/zookeeper/bin/../build/lib/*.jar:/zookeeper/bin/../lib/slf4j-log4j12-1.6.1.jar:/zookeeper/bin/../lib/slf4j-api-1.6.1.jar:/zookeeper/bin/../lib/netty-3.10.5.Final.jar:/zookeeper/bin/../lib/log4j-1.2.16.jar:/zookeeper/bin/../lib/jline-0.9.94.jar:/zookeeper/bin/../zookeeper-3.4.10.jar:/zookeeper/bin/../src/java/lib/*.jar:/zookeeper/conf:2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2017-09-21 07:15:55,432 - INFO [main:Environment@100] - Server environment:java.io.tmpdir=/tmp2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:java.compiler=&lt;NA&gt;2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:os.name=Linux2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:os.arch=amd642017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:os.version=4.4.0-93-generic2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:user.name=zookeeper2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:user.home=/zookeeper2017-09-21 07:15:55,433 - INFO [main:Environment@100] - Server environment:user.dir=/zookeeper2017-09-21 07:15:55,435 - INFO [main:ZooKeeperServer@829] - tickTime set to 20002017-09-21 07:15:55,435 - INFO [main:ZooKeeperServer@838] - minSessionTimeout set to -12017-09-21 07:15:55,435 - INFO [main:ZooKeeperServer@847] - maxSessionTimeout set to -12017-09-21 07:15:55,440 - INFO [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181 最后一行很重要，它报告ZooKeeper已准备就绪并且正在侦听端口2181。在ZooKeeper生成该终端时，终端将继续显示其他输出。 启动卡夫卡打开一个新终端，并通过运行以下命令使用它在新容器中启动Kafka： 1$ docker run -it --rm --name kafka -p 9092:9092 --link zookeeper:zookeeper debezium/kafka:1.0 在本教程中，我们总是从Docker容器中连接到Kafka，kafka只要我们链接到该kafka容器，他们就始终能够看到该容器并与之通信。如果我们想从Docker容器外部连接到Kafka ，则希望Kafka 通过Docker主机发布其地址，我们可以通过添加Docker主机-e ADVERTISED_HOST_NAME=的IP地址或可解析的主机名来实现， Mac上的Linux或Docker，这是主机的IP地址（不是localhost）。 这将使用debezium/kafka映像的1.0版运行一个新容器，并将名称分配kafka给该容器。该-it标志使容器具有交互性，这意味着它将终端的标准输入和输出附加到容器，以便您可以看到容器中正在发生的事情。该--rm标志指示Docker在停止时删除容器。该命令将容器中的端口9092映射到Docker主机上的同一端口，以便容器外部的软件可以与Kafka进行通信。最后，命令使用--link zookeeper:zookeeper参数告诉容器它可以在zookeeper运行在同一Docker主机上的容器中找到ZooKeeper 。 您应该在终端中看到Kafka的典型输出，结尾为： 123456789101112131415161718192021222324252627282930313233343536...2017-09-21 07:16:59,085 - INFO [main-EventThread:ZkClient@713] - zookeeper state changed (SyncConnected)2017-09-21 07:16:59,218 - INFO [main:Logging$class@70] - Cluster ID = LPtcBFxzRvOzDSXhc6AamA2017-09-21 07:16:59,221 - WARN [main:Logging$class@85] - No meta.properties file under dir /kafka/data/1/meta.properties2017-09-21 07:16:59,247 - INFO [ThrottledRequestReaper-Fetch:Logging$class@70] - [ThrottledRequestReaper-Fetch]: Starting2017-09-21 07:16:59,247 - INFO [ThrottledRequestReaper-Produce:Logging$class@70] - [ThrottledRequestReaper-Produce]: Starting2017-09-21 07:16:59,248 - INFO [ThrottledRequestReaper-Request:Logging$class@70] - [ThrottledRequestReaper-Request]: Starting2017-09-21 07:16:59,308 - INFO [main:Logging$class@70] - Loading logs.2017-09-21 07:16:59,312 - INFO [main:Logging$class@70] - Logs loading complete in 4 ms.2017-09-21 07:16:59,349 - INFO [main:Logging$class@70] - Starting log cleanup with a period of 300000 ms.2017-09-21 07:16:59,353 - INFO [main:Logging$class@70] - Starting log flusher with a default period of 9223372036854775807 ms.2017-09-21 07:16:59,385 - INFO [main:Logging$class@70] - Awaiting socket connections on 172.17.0.4:9092.2017-09-21 07:16:59,387 - INFO [main:Logging$class@70] - [Socket Server on Broker 1], Started 1 acceptor threads2017-09-21 07:16:59,394 - INFO [ExpirationReaper-1-Produce:Logging$class@70] - [ExpirationReaper-1-Produce]: Starting2017-09-21 07:16:59,395 - INFO [ExpirationReaper-1-Fetch:Logging$class@70] - [ExpirationReaper-1-Fetch]: Starting2017-09-21 07:16:59,395 - INFO [ExpirationReaper-1-DeleteRecords:Logging$class@70] - [ExpirationReaper-1-DeleteRecords]: Starting2017-09-21 07:16:59,435 - INFO [ExpirationReaper-1-topic:Logging$class@70] - [ExpirationReaper-1-topic]: Starting2017-09-21 07:16:59,441 - INFO [ExpirationReaper-1-Heartbeat:Logging$class@70] - [ExpirationReaper-1-Heartbeat]: Starting2017-09-21 07:16:59,442 - INFO [controller-event-thread:Logging$class@70] - Creating /controller (is it secure? false)2017-09-21 07:16:59,447 - INFO [ExpirationReaper-1-Rebalance:Logging$class@70] - [ExpirationReaper-1-Rebalance]: Starting2017-09-21 07:16:59,456 - INFO [controller-event-thread:Logging$class@70] - Result of znode creation is: OK2017-09-21 07:16:59,458 - INFO [main:Logging$class@70] - [GroupCoordinator 1]: Starting up.2017-09-21 07:16:59,459 - INFO [main:Logging$class@70] - [GroupCoordinator 1]: Startup complete.2017-09-21 07:16:59,460 - INFO [group-metadata-manager-0:Logging$class@70] - [Group Metadata Manager on Broker 1]: Removed 0 expired offsets in 1 milliseconds.2017-09-21 07:16:59,487 - INFO [main:Logging$class@70] - [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 12017-09-21 07:16:59,530 - INFO [main:Logging$class@70] - [Transaction Coordinator 1]: Starting up.2017-09-21 07:16:59,532 - INFO [TxnMarkerSenderThread-1:Logging$class@70] - [Transaction Marker Channel Manager 1]: Starting2017-09-21 07:16:59,532 - INFO [main:Logging$class@70] - [Transaction Coordinator 1]: Startup complete.2017-09-21 07:16:59,551 - INFO [main:Logging$class@70] - Will not load MX4J, mx4j-tools.jar is not in the classpath2017-09-21 07:16:59,590 - INFO [main:Logging$class@70] - Creating /brokers/ids/1 (is it secure? false)2017-09-21 07:16:59,604 - INFO [main:Logging$class@70] - Result of znode creation is: OK2017-09-21 07:16:59,605 - INFO [main:Logging$class@70] - Registered broker 1 at path /brokers/ids/1 with addresses: EndPoint(172.17.0.4,9092,ListenerName(PLAINTEXT),PLAINTEXT)2017-09-21 07:16:59,606 - WARN [main:Logging$class@85] - No meta.properties file under dir /kafka/data/1/meta.properties2017-09-21 07:16:59,648 - INFO [main:AppInfoParser$AppInfo@83] - Kafka version : 0.11.0.02017-09-21 07:16:59,648 - INFO [main:AppInfoParser$AppInfo@84] - Kafka commitId : cb8625948210849f2017-09-21 07:16:59,649 - INFO [main:Logging$class@70] - [Kafka Server 1], started 上面显示的最后一行报告说，Kafka代理已成功启动并且已准备好进行客户端连接。随着Kafka的生成，终端将继续显示其他输出。 Debezium 1.0.2.Final需要Kafka Connect 2.4.0，在本教程中，我们还将使用Kafka Broker的2.4.0版本。查看有关不同版本的Kafka Connect和Kafka代理之间兼容性的Kafka文档。 启动一个MySQL数据库至此，我们已经启动了ZooKeeper和Kafka，但是还没有Debezium可以从中捕获更改的数据库服务器。现在，让我们用示例数据库启动一个MySQL服务器。 打开一个新终端，并使用它来启动一个新容器，该容器运行一个预先配置有inventory数据库的MySQL数据库服务器： 1$ docker run -it --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:1.0 这将运行使用的版本1.0中的新的容器debezium/example-mysql的图像，这是基于所述的MySQL：5.7图像，限定并填充的样品的“库存”数据库，并创建一个debezium用户与密码dbz具有由Debezium的MySQL的连接器所需的最小特权。该命令将名称分配给mysql容器，以便以后可以方便地引用它。该-it标志使容器具有交互性，这意味着它将终端的标准输入和输出附加到容器，以便您可以看到容器中正在发生的事情。的--rm标志指示Docker在停止时删除容器。该命令将容器中的端口3306（默认的MySQL端口）映射到Docker主机上的同一端口，以便容器外部的软件可以连接到数据库服务器。最后，该机还采用了-e选项三次的设置MYSQL_ROOT_PASSWORD，MYSQL_USER以及MYSQL_PASSWORD环境变量设置为特定值。 您应该在终端中看到以下内容： 123...017-09-21T07:18:50.824629Z 0 [Note] mysqld: ready for connections.Version: &apos;5.7.19-log&apos; socket: &apos;/var/run/mysqld/mysqld.sock&apos; port: 3306 MySQL Community Server (GPL) 请注意，随着修改配置，MySQL服务器会启动和停止几次。上面列出的最后一行报告MySQL服务器正在运行并且可以使用。 启动MySQL命令行客户端打开一个新终端，并使用它为MySQL命令行客户端启动一个新容器，并将其连接到在该mysql容器中运行的MySQL服务器： 1$ docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c 'exec mysql -h\"$MYSQL_PORT_3306_TCP_ADDR\" -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p\"$MYSQL_ENV_MYSQL_ROOT_PASSWORD\"' 在这里，我们使用mysql：5.7映像启动容器，命名该容器mysqlterm并将其链接到mysql运行数据库服务器的容器。该--rm选项告诉Docker在停止时删除容器，该命令的其余部分定义了容器应运行的shell命令。此shell命令运行MySQL命令行客户端，并指定正确的选项，以便可以正确连接。 容器应输出类似于以下内容的行： 1234567891011121314mysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.17-log MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 与其他容器不同，此容器运行一个产生提示的过程。我们将使用提示与数据库进行交互。首先，切换到“库存”数据库： 1mysql&gt; use inventory; 然后列出数据库中的表： 1mysql&gt; show tables; 然后应显示： 123456789+---------------------+| Tables_in_inventory |+---------------------+| customers || orders || products || products_on_hand |+---------------------+4 rows in set (0.00 sec) 使用MySQL命令行客户端浏览数据库并查看数据库中预加载的数据。例如： 1mysql&gt; SELECT * FROM customers; 启动Kafka Connect打开一个新终端，并通过运行以下命令使用它在新容器中启动Kafka Connect服务： 1$ docker run -it --rm --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses --link zookeeper:zookeeper --link kafka:kafka --link mysql:mysql debezium/connect:1.0 这将运行一个connect使用该debezium/connect映像的1.0版命名的新Docker容器。该-it标志使容器具有交互性，这意味着它将终端的标准输入和输出附加到容器，以便您可以看到容器中正在发生的事情。该--rm标志指示Docker在停止时删除容器。该命令将容器中的端口8083映射到Docker主机上的相同端口，以便容器外部的软件可以使用Kafka Connect的REST API来设置和管理新的连接器实例。该命令使用--link zookeeper:zookeeper，--link kafka:kafka以及--link mysql:mysql，参数告诉容器，它可以找到的ZooKeeper中指定的容器中运行zookeeper，卡夫卡经纪人在名为容器中运行kafka，以及在名为的容器中运行的MySQL服务器mysql，它们均在同一Docker主机上运行。最后，该机还采用了-e选择三次设置GROUP_ID，CONFIG_STORAGE_TOPIC，OFFSET_STORAGE_TOPIC，和STATUS_STORAGE_TOPIC环境变量，它们都通过这个Debezium图像所需（尽管您可以根据需要使用不同的值）。 您应该在终端中看到Kafka Connect终端的典型输出，结尾为： 1234567891011121314...2017-09-21 07:21:14,912 INFO || Kafka version : 0.11.0.0 [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:21:14,912 INFO || Kafka commitId : cb8625948210849f [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:21:14,929 INFO || Discovered coordinator 172.17.0.4:9092 (id: 2147483646 rack: null) for group 1. [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:21:14,931 INFO || Finished reading KafkaBasedLog for topic my_connect_configs [org.apache.kafka.connect.util.KafkaBasedLog]2017-09-21 07:21:14,932 INFO || Started KafkaBasedLog for topic my_connect_configs [org.apache.kafka.connect.util.KafkaBasedLog]2017-09-21 07:21:14,932 INFO || Started KafkaConfigBackingStore [org.apache.kafka.connect.storage.KafkaConfigBackingStore]2017-09-21 07:21:14,932 INFO || Herder started [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:21:14,938 INFO || Discovered coordinator 172.17.0.4:9092 (id: 2147483646 rack: null) for group 1. [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:21:14,940 INFO || (Re-)joining group 1 [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:21:15,022 INFO || Successfully joined group 1 with generation 1 [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:21:15,022 INFO || Joined group and got assignment: Assignment&#123;error=0, leader=&apos;connect-1-4d60cb71-cb93-4388-8908-6f0d299a9d94&apos;, leaderUrl=&apos;http://172.17.0.7:9092/&apos;, offset=-1, connectorIds=[], taskIds=[]&#125; [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:21:15,023 INFO || Starting connectors and tasks using config offset -1 [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:21:15,023 INFO || Finished starting connectors and tasks [org.apache.kafka.connect.runtime.distributed.DistributedHerder] 上面显示的最后几行报告该服务已启动并且已准备好进行连接。随着Kafka Connect服务生成该终端，该终端将继续显示其他输出。 使用Kafka Connect REST APIKafka Connect服务公开了RESTful API来管理连接器集，因此让我们通过curl命令行工具使用该API 。因为我们已将connect容器（运行Kafka Connect服务的端口）中的端口8083 映射到Docker主机上的端口8083，所以我们可以通过将请求发送到Docker主机上的端口8083来与该服务进行通信，然后将请求转发到Docker主机上。 Kafka Connect服务。我们localhost在示例中使用的是非本地Docker平台的用户（例如Windows和OS X上的Docker Toolbox用户），应替换localhost为其Docker主机的IP地址。 打开一个新终端，并使用它来检查Kafka Connect服务的状态： 1$ curl -H \"Accept:application/json\" localhost:8083/ Kafka Connect服务应返回类似于以下内容的JSON响应消息： 1&#123;\"version\":\"2.4.0\",\"commit\":\"cb8625948210849f\"&#125; 这表明我们正在运行Kafka Connect 2.4.0版本。接下来，再次检查连接器列表，再次使用您的IP地址代替localhost： 1$ curl -H \"Accept:application/json\" localhost:8083/connectors/ 应该返回以下内容： 1[] 这确认Kafka Connect服务正在运行，我们可以与之交谈，并且当前没有连接器。让我们补救一下，方法是启动一个连接器，该连接器将从MySQL数据库中捕获更改。 监控MySQL数据库此时，我们正在运行Debezium服务，带有示例inventory数据库的MySQL数据库服务器以及连接到数据库的MySQL命令行客户端。下一步是注册一个连接器，该连接器将开始监视MySQL数据库服务器的binlog，并为已更改（或将要更改）的每一行生成更改事件。由于这是一个新的连接器，因此在启动时它将从MySQL二进制日志的开头开始读取，该日志记录了所有事务，包括单个行的更改和对模式的更改。 通常，我们可能希望使用Kafka工具来手动创建必要的主题，包括指定副本数。但是，对于本教程，Kafka配置为仅使用1个副本自动创建主题。 在同一终端上，我们将使用curl我们要启动的连接器信息向我们的Kafka Connect服务提交JSON请求消息。由于此命令将不在Docker容器中，因此我们需要使用Docker主机的IP地址（因此Windows和OS X上的Docker Toolbox用户应替换localhost为其IP地址）： 1$ curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d '&#123; \"name\": \"inventory-connector\", \"config\": &#123; \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"184054\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"inventory\", \"database.history.kafka.bootstrap.servers\": \"kafka:9092\", \"database.history.kafka.topic\": \"dbhistory.inventory\" &#125; &#125;' | | Windows用户可能需要转义双引号，如下所示：$ curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors/ -d '&#123; \\\"name\\\": \\\"inventory-connector\\\", \\\"config\\\": &#123; \\\"connector.class\\\": \\\"io.debezium.connector.mysql.MySqlConnector\\\", \\\"tasks.max\\\": \\\"1\\\", \\\"database.hostname\\\": \\\"mysql\\\", \\\"database.port\\\": \\\"3306\\\", \\\"database.user\\\": \\\"debezium\\\", \\\"database.password\\\": \\\"dbz\\\", \\\"database.server.id\\\": \\\"184054\\\", \\\"database.server.name\\\": \\\"dbserver1\\\", \\\"database.whitelist\\\": \\\"inventory\\\", \\\"database.history.kafka.bootstrap.servers\\\": \\\"kafka:9092\\\", \\\"database.history.kafka.topic\\\": \\\"dbhistory.inventory\\\" &#125; &#125;'`为避免此错误：`&#123;\"error_code\":500,\"message\":\"Unexpected character ('n' (code 110)): was expecting double-quote to start field name\\n at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 4]\"&#125;``` |12345678910111213141516171819202122| ---- | ------------------------------------------------------------ || | ```json &#123;\"error_code\":500,\"message\":\"Unexpected character ('n' (code 110)): was expecting double-quote to start field name\\n at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 4]\"&#125;``` |此命令使用Kafka Connect服务的RESTful API提交带有描述我们新连接器的JSON文档的资源`POST`请求`/connectors`。这是一条更具可读性的JSON消息：```json&#123; \"name\": \"inventory-connector\", \"config\": &#123; \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"184054\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"inventory\", \"database.history.kafka.bootstrap.servers\": \"kafka:9092\", \"database.history.kafka.topic\": \"schema-changes.inventory\" &#125;&#125; JSON消息将连接器名称指定为inventory-connector，并提供我们的MySQL连接器的详细配置属性： 一项任务应该恰好在任何时候运行。由于MySQL连接读取MySQL服务器的binlog，因此使用单个连接器任务是确保正确顺序和所有事件得到正确处理的唯一方法。 数据库主机指定为mysql，这是运行MySQL服务器的Docker容器的名称。回想一下，Docker在我们的容器中操纵了网络堆栈，以便可以通过/etc/hosts使用容器名称作为主机名来解析每个链接的容器。如果MySQL在正常网络上运行，我们只需为此值指定IP地址或可解析的主机名即可。 指定了MySQL服务器的端口。 我们正在运行的MySQL数据库已debezium为我们的目的明确设置了一个用户，因此我们在此处指定该用户名和密码。 给出了唯一的服务器ID和名称。服务器名称是MySQL服务器或服务器集群的逻辑标识符，将用作所有Kafka主题的前缀。 我们只想检测inventory数据库中的更改，因此我们使用白名单。 连接器应使用命名的代理（与我们将事件发送到的代理相同）和主题名称在Kafka中存储数据库架构的历史记录。重新启动后，连接器将恢复连接器应开始读取时二进制日志中存在的时间点的数据库模式。 此命令应产生类似于以下内容的响应（也许更紧凑）： 12345678910111213141516171819202122232425HTTP/1.1 201 CreatedDate: Tue, 07 Feb 2017 20:49:34 GMTLocation: http://localhost:8083/connectors/inventory-connectorContent-Type: application/jsonContent-Length: 471Server: Jetty(9.2.15.v20160210)&#123; \"name\": \"inventory-connector\", \"config\": &#123; \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"184054\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"inventory\", \"database.history.kafka.bootstrap.servers\": \"kafka:9092\", \"database.history.kafka.topic\": \"dbhistory.inventory\", \"name\": \"inventory-connector\" &#125;, \"tasks\": []&#125; 此响应描述了/connectors/inventory-connector服务刚刚创建的连接器资源，并包括连接器的配置和有关任务的信息。由于连接器是刚刚创建的，因此该服务尚未完成启动任务。 我们甚至可以使用RESTful API来验证我们的连接器是否包含在连接器列表中： 1$ curl -H \"Accept:application/json\" localhost:8083/connectors/ 应该返回以下内容： 1[\"inventory-connector\"] 回想一下，Kafka Connect服务使用连接器启动一个或多个完成该任务的任务，并且它将自动在Kafka Connect服务群集中分配正在运行的任务。如果任何服务停止或崩溃，这些任务将重新分配给正在运行的服务。当我们获得连接器的状态时，我们可以看到任务： 1$ curl -i -X GET -H \"Accept:application/json\" localhost:8083/connectors/inventory-connector 返回： 1234567891011121314151617181920212223242526272829HTTP/1.1 200 OKDate: Mon, 27 Mar 2017 17:09:28 GMTContent-Type: application/jsonContent-Length: 515Server: Jetty(9.2.15.v20160210)&#123; \"name\": \"inventory-connector\", \"config\": &#123; \"name\": \"inventory-connector\", \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"184054\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"inventory\", \"database.history.kafka.bootstrap.servers\": \"kafka:9092\", \"database.history.kafka.topic\": \"dbhistory.inventory\" &#125;, \"tasks\": [ &#123; \"connector\": \"inventory-connector\", \"task\": 0 &#125; ]&#125; 在这里，我们可以看到连接器正在运行一个任务（例如任务0）以完成其工作。MySQL连接器仅支持一个任务，因为MySQL将其所有活动记录在一个连续的二进制日志中，因此MySQL连接器仅需要一个读取器即可获得所有这些事件的一致且完全有序的视图。 如果我们查看connect容器的输出，我们会看到连接器产生了很多输出。与我们的连接器相关的前几行是由Kafka Connect输出的，并以以下内容开头： 12345678910...2017-09-21 07:23:59,051 INFO || Connector inventory-connector config updated [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:23:59,550 INFO || Rebalance started [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:23:59,550 INFO || Finished stopping tasks in preparation for rebalance [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:23:59,550 INFO || (Re-)joining group 1 [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:23:59,556 INFO || Successfully joined group 1 with generation 2 [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:23:59,556 INFO || Joined group and got assignment: Assignment&#123;error=0, leader=&apos;connect-1-4d60cb71-cb93-4388-8908-6f0d299a9d94&apos;, leaderUrl=&apos;http://172.17.0.7:9092/&apos;, offset=1, connectorIds=[inventory-connector], taskIds=[]&#125; [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:23:59,557 INFO || Starting connectors and tasks using config offset 1 [org.apache.kafka.connect.runtime.distributed.DistributedHerder]2017-09-21 07:23:59,557 INFO || Starting connector inventory-connector [org.apache.kafka.connect.runtime.distributed.DistributedHerder]... 接下来是Kafka Connect的大量输出，内容涉及启动此连接器以及各种生产者和消费者配置。最终，我们从MySQL连接器看到以下输出： 123456789...2017-09-21 07:24:01,151 INFO MySQL|dbserver1|task Kafka version : 0.11.0.0 [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:24:01,151 INFO MySQL|dbserver1|task Kafka commitId : cb8625948210849f [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:24:01,584 INFO MySQL|dbserver1|task Found no existing offset, so preparing to perform a snapshot [io.debezium.connector.mysql.MySqlConnectorTask]2017-09-21 07:24:01,614 INFO || Source task WorkerSourceTask&#123;id=inventory-connector-0&#125; finished initialization and start [org.apache.kafka.connect.runtime.WorkerSourceTask]2017-09-21 07:24:01,615 INFO MySQL|dbserver1|snapshot Starting snapshot for jdbc:mysql://mysql:3306/?useInformationSchema=true&amp;nullCatalogMeansCurrent=false&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;characterSetResults=UTF-8&amp;zeroDateTimeBehavior=convertToNull with user &apos;debezium&apos; [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,617 INFO MySQL|dbserver1|snapshot Snapshot is using user &apos;debezium&apos; with these MySQL grants: [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,618 INFO MySQL|dbserver1|snapshot GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &apos;debezium&apos;@&apos;%&apos; [io.debezium.connector.mysql.SnapshotReader]... 首先，Debezium日志输出使用映射的诊断上下文（即MDC），它允许日志消息包括特定于线程的信息，例如连接器类型（例如，MySQL在上面的“ INFO”或“ WARN”字段之后的日志消息中），连接器（例如，逻辑名dbserver1同上），并且连接器的活动（例如，task，snapshot和binlog）。希望这些将使您更容易理解多线程Kafka Connect服务中的情况。 前几行涉及task连接器的活动，并且基本上报告一些簿记信息，以使连接器在没有任何偏移的情况下启动。新的三行内容涉及snapshot连接器的活动，特别是正在使用debeziumMySQL用户以及与该用户关联的MySQL授权启动快照。 如果连接器无法连接或看不到任何表或Binlog，请检查这些授予以确保包括上面列出的所有授予。 连接器输出的下一条消息如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253...2017-09-21 07:24:01,618 INFO MySQL|dbserver1|snapshot MySQL server variables related to change data capture: [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,625 INFO MySQL|dbserver1|snapshot binlog_cache_size = 32768 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,625 INFO MySQL|dbserver1|snapshot binlog_checksum = CRC32 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,625 INFO MySQL|dbserver1|snapshot binlog_direct_non_transactional_updates = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,625 INFO MySQL|dbserver1|snapshot binlog_error_action = ABORT_SERVER [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_format = ROW [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_group_commit_sync_delay = 0 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_group_commit_sync_no_delay_count = 0 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_gtid_simple_recovery = ON [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_max_flush_queue_time = 0 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_order_commits = ON [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_row_image = FULL [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_rows_query_log_events = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot binlog_stmt_cache_size = 32768 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_client = utf8 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_connection = utf8 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_database = latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_filesystem = binary [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_results = utf8 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_server = latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_set_system = utf8 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot character_sets_dir = /usr/share/mysql/charsets/ [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot collation_connection = utf8_general_ci [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot collation_database = latin1_swedish_ci [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot collation_server = latin1_swedish_ci [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot enforce_gtid_consistency = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,626 INFO MySQL|dbserver1|snapshot gtid_executed_compression_period = 1000 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot gtid_mode = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot gtid_next = AUTOMATIC [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot gtid_owned = [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot gtid_purged = [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot innodb_api_enable_binlog = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot innodb_locks_unsafe_for_binlog = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot innodb_version = 5.7.19 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot log_statements_unsafe_for_binlog = ON [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot max_binlog_cache_size = 18446744073709547520 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot max_binlog_size = 1073741824 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot max_binlog_stmt_cache_size = 18446744073709547520 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot protocol_version = 10 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot session_track_gtids = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot slave_type_conversions = [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot sync_binlog = 1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot system_time_zone = UTC [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot time_zone = SYSTEM [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot tls_version = TLSv1,TLSv1.1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot tx_isolation = REPEATABLE-READ [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot tx_read_only = OFF [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot version = 5.7.19-log [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot version_comment = MySQL Community Server (GPL) [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,627 INFO MySQL|dbserver1|snapshot version_compile_machine = x86_64 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,628 INFO MySQL|dbserver1|snapshot version_compile_os = Linux [io.debezium.connector.mysql.SnapshotReader]... 这将报告由我们的MySQL连接器找到的相关MySQL服务器设置。最重要的一项是binlog_format，它被设置为ROW。这些行之后是构成快照操作的9个步骤的输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374...2017-09-21 07:24:01,628 INFO MySQL|dbserver1|snapshot Step 0: disabling autocommit and enabling repeatable read transactions [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,631 INFO MySQL|dbserver1|snapshot Step 1: start transaction with consistent snapshot [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,634 INFO MySQL|dbserver1|snapshot Step 2: flush and obtain global read lock to prevent writes to database [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,636 INFO MySQL|dbserver1|snapshot Step 3: read binlog position of MySQL master [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,638 INFO MySQL|dbserver1|snapshot using binlog 'mysql-bin.000003' at position '154' and gtid '' [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,638 INFO MySQL|dbserver1|snapshot Step 4: read list of available databases [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,638 INFO MySQL|dbserver1|snapshot list of available databases is: [information_schema, inventory, mysql, performance_schema, sys] [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,639 INFO MySQL|dbserver1|snapshot Step 5: read list of available tables in each database [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,641 INFO MySQL|dbserver1|snapshot including 'inventory.customers' [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,641 INFO MySQL|dbserver1|snapshot including 'inventory.orders' [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,641 INFO MySQL|dbserver1|snapshot including 'inventory.products' [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,641 INFO MySQL|dbserver1|snapshot including 'inventory.products_on_hand' [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,642 INFO MySQL|dbserver1|snapshot 'mysql.columns_priv' is filtered out, discarding [io.debezium.connector.mysql.SnapshotReader]...2017-09-21 07:24:01,670 INFO MySQL|dbserver1|snapshot snapshot continuing with database(s): [inventory] [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,670 INFO MySQL|dbserver1|snapshot Step 6: generating DROP and CREATE statements to reflect current database schemas: [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,679 INFO MySQL|dbserver1|snapshot SET character_set_server=latin1, collation_server=latin1_swedish_ci; [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,724 WARN MySQL|dbserver1|task Error while fetching metadata with correlation id 1 : &#123;dbhistory.inventory=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]2017-09-21 07:24:01,853 INFO MySQL|dbserver1|snapshot DROP TABLE IF EXISTS `inventory`.`products_on_hand` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,861 INFO MySQL|dbserver1|snapshot DROP TABLE IF EXISTS `inventory`.`customers` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,864 INFO MySQL|dbserver1|snapshot DROP TABLE IF EXISTS `inventory`.`orders` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,866 INFO MySQL|dbserver1|snapshot DROP TABLE IF EXISTS `inventory`.`products` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,881 INFO MySQL|dbserver1|snapshot DROP DATABASE IF EXISTS `inventory` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,889 INFO MySQL|dbserver1|snapshot CREATE DATABASE `inventory` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,893 INFO MySQL|dbserver1|snapshot USE `inventory` [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,914 INFO MySQL|dbserver1|snapshot CREATE TABLE `customers` ( `id` int(11) NOT NULL AUTO_INCREMENT, `first_name` varchar(255) NOT NULL, `last_name` varchar(255) NOT NULL, `email` varchar(255) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `email` (`email`)) ENGINE=InnoDB AUTO_INCREMENT=1005 DEFAULT CHARSET=latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,932 INFO MySQL|dbserver1|snapshot CREATE TABLE `orders` ( `order_number` int(11) NOT NULL AUTO_INCREMENT, `order_date` date NOT NULL, `purchaser` int(11) NOT NULL, `quantity` int(11) NOT NULL, `product_id` int(11) NOT NULL, PRIMARY KEY (`order_number`), KEY `order_customer` (`purchaser`), KEY `ordered_product` (`product_id`), CONSTRAINT `orders_ibfk_1` FOREIGN KEY (`purchaser`) REFERENCES `customers` (`id`), CONSTRAINT `orders_ibfk_2` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=10005 DEFAULT CHARSET=latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,937 INFO MySQL|dbserver1|snapshot CREATE TABLE `products` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `description` varchar(512) DEFAULT NULL, `weight` float DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,941 INFO MySQL|dbserver1|snapshot CREATE TABLE `products_on_hand` ( `product_id` int(11) NOT NULL, `quantity` int(11) NOT NULL, PRIMARY KEY (`product_id`), CONSTRAINT `products_on_hand_ibfk_1` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,947 INFO MySQL|dbserver1|snapshot Step 7: releasing global read lock to enable MySQL writes [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,949 INFO MySQL|dbserver1|snapshot Step 7: blocked writes to MySQL for a total of 00:00:00.312 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,950 INFO MySQL|dbserver1|snapshot Step 8: scanning contents of 4 tables while still in transaction [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,953 INFO MySQL|dbserver1|snapshot Step 8: - scanning table 'inventory.customers' (1 of 4 tables) [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,958 INFO MySQL|dbserver1|snapshot Step 8: - Completed scanning a total of 4 rows from table 'inventory.customers' after 00:00:00.005 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:01,959 INFO MySQL|dbserver1|snapshot Step 8: - scanning table 'inventory.orders' (2 of 4 tables) [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,014 INFO MySQL|dbserver1|snapshot Step 8: - Completed scanning a total of 4 rows from table 'inventory.orders' after 00:00:00.055 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,016 INFO MySQL|dbserver1|snapshot Step 8: - scanning table 'inventory.products' (3 of 4 tables) [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,017 INFO MySQL|dbserver1|snapshot Step 8: - Completed scanning a total of 9 rows from table 'inventory.products' after 00:00:00.001 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,018 INFO MySQL|dbserver1|snapshot Step 8: - scanning table 'inventory.products_on_hand' (4 of 4 tables) [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,019 INFO MySQL|dbserver1|snapshot Step 8: - Completed scanning a total of 9 rows from table 'inventory.products_on_hand' after 00:00:00.001 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,020 INFO MySQL|dbserver1|snapshot Step 8: scanned 26 rows in 4 tables in 00:00:00.069 [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,020 INFO MySQL|dbserver1|snapshot Step 9: committing transaction [io.debezium.connector.mysql.SnapshotReader]2017-09-21 07:24:02,021 INFO MySQL|dbserver1|snapshot Completed snapshot in 00:00:00.405 [io.debezium.connector.mysql.SnapshotReader]... 这些步骤中的每一个都报告连接器正在执行的操作，以执行一致的快照。例如，步骤6涉及对捕获的表的DDL create语句进行反向工程；步骤7在获取全局写锁后仅0.3秒就释放了它，步骤8读取每个表中的所有行，并报告花费的时间和找到的行数。请注意，在我们的示例数据库中，MySQL连接器仅用0.38秒即可完成其一致的快照。 对于您的数据库，此过程将花费更长的时间，但是连接器将输出足够的日志消息，以便即使表具有大量行，您也可以跟踪其正在执行的操作。而且，尽管在快照过程开始时使用了排他写锁定，但即使对于大型数据库，此锁定也应该很短。在复制任何数据之前，将释放此锁定。有关更多详细信息，请参见MySQL连接器文档。 Kafka Connect的新五行听起来不祥，但基本上告诉我们，已经创建了新的 Kafka主题，而Kafka必须为每一个主题指定新的领导者： 1234567...2017-09-21 07:24:02,632 WARN || Error while fetching metadata with correlation id 1 : &#123;dbserver1=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]2017-09-21 07:24:02,775 WARN || Error while fetching metadata with correlation id 5 : &#123;dbserver1.inventory.customers=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]2017-09-21 07:24:02,910 WARN || Error while fetching metadata with correlation id 9 : &#123;dbserver1.inventory.orders=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]2017-09-21 07:24:03,045 WARN || Error while fetching metadata with correlation id 13 : &#123;dbserver1.inventory.products=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]2017-09-21 07:24:03,179 WARN || Error while fetching metadata with correlation id 17 : &#123;dbserver1.inventory.products_on_hand=LEADER_NOT_AVAILABLE&#125; [org.apache.kafka.clients.NetworkClient]... 最后，我们看到一行报告，表明连接器已从其快照模式转换为连续读取MySQL服务器的binlog： 123456...Sep 21, 2017 7:24:03 AM com.github.shyiko.mysql.binlog.BinaryLogClient connectINFO: Connected to mysql:3306 at mysql-bin.000003/154 (sid:184054, cid:7)2017-09-21 07:24:03,373 INFO MySQL|dbserver1|binlog Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000003', pos=154, skipping 0 events plus 0 rows [io.debezium.connector.mysql.BinlogReader]2017-09-21 07:25:01,096 INFO || Finished WorkerSourceTask&#123;id=inventory-connector-0&#125; commitOffsets successfully in 18 ms [org.apache.kafka.connect.runtime.WorkerSourceTask]... 查看变更事件我们在连接器的输出中看到，事件被写入五个主题： dbserver1 dbserver1.inventory.products dbserver1.inventory.products_on_hand dbserver1.inventory.customers dbserver1.inventory.orders 如MySQL连接器文档中所述，每个主题名称均以开头dbserver1，这是我们为连接器赋予的逻辑名称。第一个是我们所有DDL语句都写入到的架构更改主题。其余四个主题用于捕获四个表中每个表的更改事件，它们的主题名称包括数据库名称（例如inventory）和表名称。 让我们看一下该dbserver1.inventory.customers主题中的所有数据更改事件。我们将使用debezium/kafkaDocker映像来启动一个新容器，该容器运行Kafka的一个实用程序来从主题开始观看该主题： 1$ docker run -it --name watcher --rm --link zookeeper:zookeeper --link kafka:kafka debezium/kafka:1.0 watch-topic -a -k dbserver1.inventory.customers 再次使用--rm标记，因为我们希望容器在停止时被删除，并且使用-a标记on watch-topic表示我们希望查看自主题开始以来的所有事件。（如果要删除该-a标志，则在开始观看之后，我们只会看到记录在主题中的事件。）该-k标志指定输出应包含事件的键，在我们的例子中，该键包含该行的主键。这是输出： 12345678Using ZOOKEEPER_CONNECT=172.17.0.3:2181Using KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://172.17.0.8:9092Contents of topic dbserver1.inventory.customers:Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1001&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1001,\"first_name\":\"Sally\",\"last_name\":\"Thomas\",\"email\":\"sally.thomas@acme.com\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":0,\"ts_sec\":0,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":154,\"row\":0,\"snapshot\":true,\"thread\":null,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490634537160&#125;&#125;&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1002&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1002,\"first_name\":\"George\",\"last_name\":\"Bailey\",\"email\":\"gbailey@foobar.com\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":0,\"ts_sec\":0,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":154,\"row\":0,\"snapshot\":true,\"thread\":null,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490634537160&#125;&#125;&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1003&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1003,\"first_name\":\"Edward\",\"last_name\":\"Walker\",\"email\":\"ed@walker.com\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":0,\"ts_sec\":0,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":154,\"row\":0,\"snapshot\":true,\"thread\":null,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490634537160&#125;&#125;&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1004&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1004,\"first_name\":\"Anne\",\"last_name\":\"Kretchmar\",\"email\":\"annek@noanswer.org\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":0,\"ts_sec\":0,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":154,\"row\":0,\"snapshot\":true,\"thread\":null,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490634537160&#125;&#125; 该实用程序一直在监视，因此只要该实用程序继续运行，任何新事件都会自动出现。而且该watch-topic实用程序非常简单，并且功能和实用性受到限制-我们在这里使用它只是为了了解我们的连接器生成的事件的种类。想要使用事件的应用程序将改为使用Kafka使用者，而这些使用者库提供了更大的灵活性和功能。实际上，正确配置的客户端使我们的应用程序永远不会错过任何事件，即使那些应用程序崩溃或正常关闭也是如此。 这些事件碰巧是用JSON编码的，因为这就是我们配置Kafka Connect服务的方式。每个事件都包含一个JSON文档作为密钥，并包含一个JSON文档。让我们通过首先重新格式化事件的键使其更易于阅读来更详细地查看最后一个事件： 1234567891011121314151617&#123; \"schema\": &#123; \"type\": \"struct\", \"name\": \"dbserver1.inventory.customers.Key\" \"optional\": false, \"fields\": [ &#123; \"field\": \"id\", \"type\": \"int32\", \"optional\": false &#125; ] &#125;, \"payload\": &#123; \"id\": 1004 &#125;&#125; 事件的键分为两部分：a schema和payload。该schema包含卡夫卡连接架构描述什么是有效载荷，并在我们的情况下这意味着payload是一个命名结构dbserver1.inventory.customers.Key是不可选的，有一个名为必填字段id类型int32。 如果我们查看键payload字段的值，就会发现它确实是一个结构（在JSON中只是一个对象），只有一个id字段，其值为1004。 因此，我们将此事件解释为适用于主键列的值为的inventory.customers表中的行（来自名为的连接器的输出dbserver1）。id`1004` 现在让我们看一下同一事件的value，我们再次将其重新格式化以使其更易于阅读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169&#123; \"schema\": &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"id\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"email\" &#125; ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"before\" &#125;, &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"id\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"email\" &#125; ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"after\" &#125;, &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"string\", \"optional\": true, \"field\": \"version\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"name\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"server_id\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"ts_sec\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"gtid\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"file\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"pos\" &#125;, &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"row\" &#125;, &#123; \"type\": \"boolean\", \"optional\": true, \"field\": \"snapshot\" &#125;, &#123; \"type\": \"int64\", \"optional\": true, \"field\": \"thread\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"db\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"table\" &#125; ], \"optional\": false, \"name\": \"io.debezium.connector.mysql.Source\", \"field\": \"source\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"op\" &#125;, &#123; \"type\": \"int64\", \"optional\": true, \"field\": \"ts_ms\" &#125; ], \"optional\": false, \"name\": \"dbserver1.inventory.customers.Envelope\", \"version\": 1 &#125;, \"payload\": &#123; \"before\": null, \"after\": &#123; \"id\": 1004, \"first_name\": \"Anne\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" &#125;, \"source\": &#123; \"version\": \"1.0.2.Final\", \"name\": \"dbserver1\", \"server_id\": 0, \"ts_sec\": 0, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 154, \"row\": 0, \"snapshot\": true, \"thread\": null, \"db\": \"inventory\", \"table\": \"customers\" &#125;, \"op\": \"c\", \"ts_ms\": 1486500577691 &#125;&#125; 事件的这一部分要大得多，但是像事件的键一样，它也有a schema和a payload。schema包含一个名为dbserver1.inventory.customers.Envelope（版本1）的Kafka Connect模式，该模式可以包含5个字段： op是必填字段，其中包含描述操作类型的字符串值。MySQL连接器的值用于c创建（或插入），u用于更新，d用于删除和r用于读取（在非初始快照的情况下）。 before是一个可选字段，如果存在，则包含事件发生之前行的状态。该结构将由dbserver1.inventory.customers.ValueKafka Connect模式描述，连接dbserver1器将其用于inventory.customers表中的所有行。 after是一个可选字段，如果存在，则包含事件发生后行的状态。该结构由中使用的相同的dbserver1.inventory.customers.ValueKafka Connect模式描述before。 source 是必填字段，包含描述事件源元数据的结构，在MySQL中，该字段包含多个字段：连接器名称，记录事件的binlog文件的名称，binlog文件在其中的位置。事件出现，事件中的行（如果有多个），受影响的数据库和表的名称，进行更改的MySQL线程ID，此事件是否是快照的一部分以及MySQL服务器（如果有） ID，以及以秒为单位的时间戳。 ts_ms 是可选的，如果存在，则包含连接器处理事件的时间（使用运行Kafka Connect任务的JVM中的系统时钟）。 如果我们看一下payload事件的的价值，我们可以看到在事件的信息，即它是在描述该行的创建，包含id，first_name，last_name和email插入行。 您可能已经注意到，事件的JSON表示比它们描述的行大得多。这是因为Kafka Connect附带了每个事件密钥，并重视描述有效负载的架构。随着时间的流逝，这种结构可能会发生变化，并且事件本身具有键和值的架构会使得使用应用程序的用户更容易理解消息，尤其是随着时间的发展。Debezium MySQL连接器根据数据库表的结构来构造这些模式。如果使用DDL语句更改MySQL数据库中的表定义，则连接器将读取这些DDL语句并更新其Kafka Connect模式。这是每个事件的结构完全类似于事件发生时的来源表的唯一方式。但是包含单个表的所有事件的Kafka主题可能具有与表定义的每种状态相对应的事件。JSON转换器确实会产生非常冗长的事件，因为它在每条消息中都包含键和值模式。该Avro的转换器，在另一方面，是远远聪明，结果小得多事件消息。Avro转换器将每个Kafka Connect架构转换为Avro架构，并将Avro架构存储在单独的架构注册表服务中。因此，当Avro转换器对事件消息进行序列化时，它将仅放置该模式的唯一标识符以及该值的Avro编码的二进制表示形式。因此，通过网络传输并存储在Kafka中的序列化消息比上面显示的要小得多。实际上，Avro Converter能够使用Avro模式演变技术来维护模式注册表中每个模式的历史记录。 我们可以将它们与数据库状态进行比较。返回到运行MySQL命令行客户端的终端，并运行以下语句： 1mysql&gt; SELECT * FROM customers; 产生以下输出： 123456789+------+------------+-----------+-----------------------+| id | first_name | last_name | email |+------+------------+-----------+-----------------------+| 1001 | Sally | Thomas | sally.thomas@acme.com || 1002 | George | Bailey | gbailey@foobar.com || 1003 | Edward | Walker | ed@walker.com || 1004 | Anne | Kretchmar | annek@noanswer.org |+------+------------+-----------+-----------------------+4 rows in set (0.00 sec) 如我们所见，所有事件记录都与数据库匹配。 现在我们正在监视更改，更改数据库中的记录之一会发生什么？在MySQL命令行客户端中运行以下语句： 1mysql&gt; UPDATE customers SET first_name='Anne Marie' WHERE id=1004; 产生以下输出： 12Query OK, 1 row affected (0.05 sec)Rows matched: 1 Changed: 1 Warnings: 0 重新运行该select …语句以查看更新的表： 12345678910mysql&gt; select * from customers;+------+------------+-----------+-----------------------+| id | first_name | last_name | email |+------+------------+-----------+-----------------------+| 1001 | Sally | Thomas | sally.thomas@acme.com || 1002 | George | Bailey | gbailey@foobar.com || 1003 | Edward | Walker | ed@walker.com || 1004 | Anne Marie | Kretchmar | annek@noanswer.org |+------+------------+-----------+-----------------------+4 rows in set (0.00 sec) 现在，回到终端运行watch-topic，我们应该看到一个新的第五事件： 1&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1004&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":&#123;\"id\":1004,\"first_name\":\"Anne\",\"last_name\":\"Kretchmar\",\"email\":\"annek@noanswer.org\"&#125;,\"after\":&#123;\"id\":1004,\"first_name\":\"Anne Marie\",\"last_name\":\"Kretchmar\",\"email\":\"annek@noanswer.org\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":223344,\"ts_sec\":1490635059,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":364,\"row\":0,\"snapshot\":null,\"thread\":3,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"u\",\"ts_ms\":1490635059389&#125;&#125; 让我们重新格式化新事件的键，以便于阅读： 1234567891011121314151617&#123; \"schema\": &#123; \"type\": \"struct\", \"name\": \"dbserver1.inventory.customers.Key\" \"optional\": false, \"fields\": [ &#123; \"field\": \"id\", \"type\": \"int32\", \"optional\": false &#125; ] &#125;, \"payload\": &#123; \"id\": 1004 &#125;&#125; 该密钥与我们在第四条记录中看到的密钥完全相同。这是新事件的值，其格式设置为更易于阅读： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174&#123; \"schema\": &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"id\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"email\" &#125; ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"before\" &#125;, &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"id\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"email\" &#125; ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"after\" &#125;, &#123; \"type\": \"struct\", \"fields\": [ &#123; \"type\": \"string\", \"optional\": false, \"field\": \"version\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"name\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"server_id\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"ts_sec\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"gtid\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"file\" &#125;, &#123; \"type\": \"int64\", \"optional\": false, \"field\": \"pos\" &#125;, &#123; \"type\": \"int32\", \"optional\": false, \"field\": \"row\" &#125;, &#123; \"type\": \"boolean\", \"optional\": true, \"field\": \"snapshot\" &#125;, &#123; \"type\": \"int64\", \"optional\": true, \"field\": \"thread\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"db\" &#125;, &#123; \"type\": \"string\", \"optional\": true, \"field\": \"table\" &#125; ], \"optional\": false, \"name\": \"io.debezium.connector.mysql.Source\", \"field\": \"source\" &#125;, &#123; \"type\": \"string\", \"optional\": false, \"field\": \"op\" &#125;, &#123; \"type\": \"int64\", \"optional\": true, \"field\": \"ts_ms\" &#125; ], \"optional\": false, \"name\": \"dbserver1.inventory.customers.Envelope\", \"version\": 1 &#125;, \"payload\": &#123; \"before\": &#123; \"id\": 1004, \"first_name\": \"Anne\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" &#125;, \"after\": &#123; \"id\": 1004, \"first_name\": \"Anne Marie\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" &#125;, \"source\": &#123; \"name\": \"1.0.2.Final\", \"name\": \"dbserver1\", \"server_id\": 223344, \"ts_sec\": 1486501486, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 364, \"row\": 0, \"snapshot\": null, \"thread\": 3, \"db\": \"inventory\", \"table\": \"customers\" &#125;, \"op\": \"u\", \"ts_ms\": 1486501486308 &#125;&#125; 当我们将其与第四个事件中的值进行比较时，在该schema部分中没有看到任何更改，而在该部分中看到了一些更改payload： 该op字段的值是现在u，表示此行被更改，因为更新 before现在，该字段具有行状态以及数据库提交前的值 after现在，该字段具有该行的更新状态，在这里可以看到该first_name值是now Anne Marie。 该source场结构具有许多相同的价值观和以前一样，除了ts_sec和pos领域发生了变化（而file可能在其他情况下发生了变化）。 该ts_ms节目的时间戳Debezium处理此事件。 仅看本payload节，我们可以学到几件事。我们可以比较before和after结构，以确定由于提交而实际更改的内容。该source结构告诉我们有关MySQL更改记录的信息（提供可追溯性），但更重要的是，它具有可以与本主题和其他主题中的其他事件进行比较的信息，以了解此事件是在同一MySQL之前，之后还是作为其一部分发生的像其他事件一样提交。 到目前为止，我们已经看到了创建和更新事件的示例。现在，让我们看一下删除事件。由于Anne Marie尚未下任何订单，因此我们可以使用MySQL命令行客户端从数据库中删除她的记录： 1mysql&gt; DELETE FROM customers WHERE id=1004; | | 如果以上命令因违反外键约束而失败，则必须使用以下语句从地址表中删除客户地址的引用：DELETE FROM addresses WHERE customer_id12345678| ---- | ------------------------------------------------------------ || | |在运行的终端中`watch-topic`，我们看到*两个*新事件：```json&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;dbserver1.inventory.customers.Key&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:1004&#125;&#125; &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;&#125;],&quot;optional&quot;:true,&quot;name&quot;:&quot;dbserver1.inventory.customers.Value&quot;,&quot;field&quot;:&quot;before&quot;&#125;,&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;&#125;],&quot;optional&quot;:true,&quot;name&quot;:&quot;dbserver1.inventory.customers.Value&quot;,&quot;field&quot;:&quot;after&quot;&#125;,&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;version&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;name&quot;&#125;,&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;server_id&quot;&#125;,&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;ts_sec&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;gtid&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;file&quot;&#125;,&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;pos&quot;&#125;,&#123;&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;row&quot;&#125;,&#123;&quot;type&quot;:&quot;boolean&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;snapshot&quot;&#125;,&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;thread&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;db&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;table&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;io.debezium.connector.mysql.Source&quot;,&quot;field&quot;:&quot;source&quot;&#125;,&#123;&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;op&quot;&#125;,&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;ts_ms&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;dbserver1.inventory.customers.Envelope&quot;,&quot;version&quot;:1&#125;,&quot;payload&quot;:&#123;&quot;before&quot;:&#123;&quot;id&quot;:1004,&quot;first_name&quot;:&quot;Anne Marie&quot;,&quot;last_name&quot;:&quot;Kretchmar&quot;,&quot;email&quot;:&quot;annek@noanswer.org&quot;&#125;,&quot;after&quot;:null,&quot;source&quot;:&#123;&quot;version&quot;:&quot;1.0.2.Final&quot;,&quot;name&quot;:&quot;dbserver1&quot;,&quot;server_id&quot;:223344,&quot;ts_sec&quot;:1490635100,&quot;gtid&quot;:null,&quot;file&quot;:&quot;mysql-bin.000003&quot;,&quot;pos&quot;:725,&quot;row&quot;:0,&quot;snapshot&quot;:null,&quot;thread&quot;:3,&quot;db&quot;:&quot;inventory&quot;,&quot;table&quot;:&quot;customers&quot;&#125;,&quot;op&quot;:&quot;d&quot;,&quot;ts_ms&quot;:1490635100301&#125;&#125;&#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;dbserver1.inventory.customers.Key&quot;&#125;,&quot;payload&quot;:&#123;&quot;id&quot;:1004&#125;&#125; &#123;&quot;schema&quot;:null,&quot;payload&quot;:null&#125; 发生了什么？我们只删除了一行，但是现在有两个事件。要了解MySQL连接器的作用，让我们看一下两个新消息中的第一个。重新格式化后的键更易于阅读： 1234567891011121314151617&#123; \"schema\": &#123; \"type\": \"struct\", \"name\": \"dbserver1.inventory.customers.Key\" \"optional\": false, \"fields\": [ &#123; \"field\": \"id\", \"type\": \"int32\", \"optional\": false &#125; ] &#125;, \"payload\": &#123; \"id\": 1004 &#125;&#125; 再次，此密钥与我们查看的前两个事件完全相同。这是第一个新事件的值，其格式设置为更易于阅读： 123456789101112131415161718192021222324252627&#123; \"schema\": &#123;...&#125;, \"payload\": &#123; \"before\": &#123; \"id\": 1004, \"first_name\": \"Anne Marie\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" &#125;, \"after\": null, \"source\": &#123; \"name\": \"1.0.2.Final\", \"name\": \"dbserver1\", \"server_id\": 223344, \"ts_sec\": 1486501558, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 725, \"row\": 0, \"snapshot\": null, \"thread\": 3, \"db\": \"inventory\", \"table\": \"customers\" &#125;, \"op\": \"d\", \"ts_ms\": 1486501558315&#125; 再次，schema与前面的消息相同，但是该payload片段有一些注意事项： 该op字段的值是现在d，这标志着该行已被删除 before现在，该字段具有通过数据库提交删除的行的状态 该after字段为空，表示该行不再存在 该source场结构具有许多相同的价值观和以前一样，除了ts_sec和pos领域发生了变化（而file可能在其他情况下发生了变化）。 该ts_ms节目的时间戳Debezium处理此事件。 此事件为使用者提供了可用于处理删除此行的各种信息。我们之所以包含旧值，是因为某些消费者可能会要求他们以正确处理删除内容，而没有删除它们，他们可能不得不诉诸更为复杂的行为。 还记得删除行时看到的两个事件吗？让我们看看第二个事件。这是活动的关键： 1234567891011121314151617&#123; \"schema\": &#123; \"type\": \"struct\", \"name\": \"dbserver1.inventory.customers.Key\" \"optional\": false, \"fields\": [ &#123; \"field\": \"id\", \"type\": \"int32\", \"optional\": false &#125; ] &#125;, \"payload\": &#123; \"id\": 1004 &#125;&#125; 再次，此密钥与我们查看的前三个事件完全相同。这是同一事件的值： 1234&#123; \"schema\": null, \"payload\": null&#125; 是什么赋予了？好吧，可以将MySQL连接器写入的所有Kafka主题设置为日志压缩，这意味着Kafka可以从该主题中删除较旧的消息，只要该主题后面至少有一条消息完全相同即可。键。这是卡夫卡收集垃圾的方法。这最后一个事件是Debezium所谓的逻辑删除事件，并且由于它具有一个键和一个空值，Kafka理解它可以使用此键删除所有先前的消息。 Kafka日志压缩很棒，因为它仍然允许消费者从一开始就阅读主题，而不会错过任何事件。 重新启动Kafka Connect服务Kafka Connect服务的一项功能是，它可以自动管理已注册连接器的任务。并且，由于它将数据存储在Kafka中，因此如果正在运行的服务停止或完全消失，则在重新启动（可能在另一台主机上）时，服务器将启动所有非运行任务。为了说明这一点，让我们停止我们的Kafka Connect服务，更改数据库中的某些数据，然后重新启动我们的服务。 在新的终端中，使用以下Docker命令停止connect运行我们的Kafka Connect服务的容器： 1$ docker stop connect 像这样停止容器将停止在其中运行的进程，但是Kafka Connect服务会通过正常关闭来处理该过程。并且由于我们使用--rm标志运行容器，因此Docker在停止容器后将其删除。 服务关闭时，让我们回到MySQL命令行客户端并添加一些记录： 12mysql&gt; INSERT INTO customers VALUES (default, \"Sarah\", \"Thompson\", \"kitt@acme.com\");mysql&gt; INSERT INTO customers VALUES (default, \"Kenneth\", \"Anderson\", \"kander@acme.com\"); 注意，在我们运行的终端中watch-topic，没有更新。另外，由于Kafka仍在运行，因此我们仍然可以观看该主题。 在生产系统中，您将有足够的代理来处理生产者和使用者，并为每个主题维护最少数量的同步副本。因此，如果有足够的经纪人失败，以致没有最小的ISR，Kafka应该变得不可用。生产商（例如Debezium连接器）和消费者将只耐心地等待Kafka群集或网络恢复。是的，这意味着您的使用者在数据库中更改数据时可能暂时看不到任何更改事件，但这是因为未生成任何更改事件。一旦Kafka群集重新启动或网络恢复，Debezium将继续产生变更事件，而您的使用者将继续使用中断的事件。 现在，在新终端中，使用之前使用的相同命令启动新容器： 1$ docker run -it --rm --name connect -p 8083:8083 -e GROUP_ID=1 -e CONFIG_STORAGE_TOPIC=my_connect_configs -e OFFSET_STORAGE_TOPIC=my_connect_offsets -e STATUS_STORAGE_TOPIC=my_connect_statuses --link zookeeper:zookeeper --link kafka:kafka --link mysql:mysql debezium/connect:1.0 这将创建一个运行Kafka Connect分布式服务的全新容器，并且由于我们已使用相同的主题信息对其进行了初始化，因此该新服务将连接到Kafka，读取先前服务的配置，并启动已注册的连接器，该连接器将继续在他们最后一次离开。 这是此重启服务的最后几行： 1234567891011121314...2017-09-21 07:38:48,385 INFO MySQL|dbserver1|task Kafka version : 0.11.0.0 [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:38:48,386 INFO MySQL|dbserver1|task Kafka commitId : cb8625948210849f [org.apache.kafka.common.utils.AppInfoParser]2017-09-21 07:38:48,390 INFO MySQL|dbserver1|task Discovered coordinator 172.17.0.4:9092 (id: 2147483646 rack: null) for group inventory-connector-dbhistory. [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:38:48,390 INFO MySQL|dbserver1|task Revoking previously assigned partitions [] for group inventory-connector-dbhistory [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]2017-09-21 07:38:48,391 INFO MySQL|dbserver1|task (Re-)joining group inventory-connector-dbhistory [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:38:48,402 INFO MySQL|dbserver1|task Successfully joined group inventory-connector-dbhistory with generation 1 [org.apache.kafka.clients.consumer.internals.AbstractCoordinator]2017-09-21 07:38:48,403 INFO MySQL|dbserver1|task Setting newly assigned partitions [dbhistory.inventory-0] for group inventory-connector-dbhistory [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]2017-09-21 07:38:48,888 INFO MySQL|dbserver1|task Step 0: Get all known binlogs from MySQL [io.debezium.connector.mysql.MySqlConnectorTask]2017-09-21 07:38:48,903 INFO MySQL|dbserver1|task MySQL has the binlog file 'mysql-bin.000003' required by the connector [io.debezium.connector.mysql.MySqlConnectorTask]Sep 21, 2017 7:38:49 AM com.github.shyiko.mysql.binlog.BinaryLogClient connectINFO: Connected to mysql:3306 at mysql-bin.000003/154 (sid:184054, cid:10)2017-09-21 07:38:49,045 INFO MySQL|dbserver1|binlog Connected to MySQL binlog at mysql:3306, starting at binlog file 'mysql-bin.000003', pos=154, skipping 0 events plus 0 rows [io.debezium.connector.mysql.BinlogReader]2017-09-21 07:38:49,046 INFO || Source task WorkerSourceTask&#123;id=inventory-connector-0&#125; finished initialization and start [org.apache.kafka.connect.runtime.WorkerSourceTask] 如您所见，这些行显示该服务在关闭之前找到了上一个任务先前记录的偏移量，然后它连接到MySQL数据库，开始从该位置读取binlog，并根据任何更改生成事件从那时起在MySQL数据库中运行。 跳回到运行中的终端watch-topic，您现在应该看到两个新记录的事件： 12&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1005&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1005,\"first_name\":\"Sarah\",\"last_name\":\"Thompson\",\"email\":\"kitt@acme.com\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":223344,\"ts_sec\":1490635153,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":1046,\"row\":0,\"snapshot\":null,\"thread\":3,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490635181455&#125;&#125;&#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Key\"&#125;,\"payload\":&#123;\"id\":1006&#125;&#125; &#123;\"schema\":&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"before\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"id\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"first_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"last_name\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"email\"&#125;],\"optional\":true,\"name\":\"dbserver1.inventory.customers.Value\",\"field\":\"after\"&#125;,&#123;\"type\":\"struct\",\"fields\":[&#123;\"type\":\"string\",\"optional\":true,\"field\":\"version\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"name\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"server_id\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"ts_sec\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"gtid\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"file\"&#125;,&#123;\"type\":\"int64\",\"optional\":false,\"field\":\"pos\"&#125;,&#123;\"type\":\"int32\",\"optional\":false,\"field\":\"row\"&#125;,&#123;\"type\":\"boolean\",\"optional\":true,\"field\":\"snapshot\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"thread\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"db\"&#125;,&#123;\"type\":\"string\",\"optional\":true,\"field\":\"table\"&#125;],\"optional\":false,\"name\":\"io.debezium.connector.mysql.Source\",\"field\":\"source\"&#125;,&#123;\"type\":\"string\",\"optional\":false,\"field\":\"op\"&#125;,&#123;\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"&#125;],\"optional\":false,\"name\":\"dbserver1.inventory.customers.Envelope\",\"version\":1&#125;,\"payload\":&#123;\"before\":null,\"after\":&#123;\"id\":1006,\"first_name\":\"Kenneth\",\"last_name\":\"Anderson\",\"email\":\"kander@acme.com\"&#125;,\"source\":&#123;\"version\":\"1.0.2.Final\",\"name\":\"dbserver1\",\"server_id\":223344,\"ts_sec\":1490635160,\"gtid\":null,\"file\":\"mysql-bin.000003\",\"pos\":1356,\"row\":0,\"snapshot\":null,\"thread\":3,\"db\":\"inventory\",\"table\":\"customers\"&#125;,\"op\":\"c\",\"ts_ms\":1490635181456&#125;&#125; 这些事件是与我们之前看到的事件类似的创建事件。但是，需要理解的重要一点是，只要在MySQL数据库开始清除我们从其binlog中错过的提交之前重新启动，即使Debezium仍在运行时，它仍将报告数据库中的所有更改。 探索继续并使用MySQL命令行客户端在数据库表中添加，修改和删除行，然后查看对主题的影响。您可能需要watch-topic为每个主题运行单独的命令。请记住，您不能删除外键引用的行。玩得开心！ 清理您可以使用Docker停止所有正在运行的容器： 1$ docker stop mysqlterm watcher connect mysql kafka zookeeper 同样，由于我们--rm在启动连接器时使用了该标志，因此Docker应该在停止连接器后立即将其删除。我们可以验证所有其他进程都已停止并删除： 1$ docker ps -a 当然，如果仍在运行，只需使用docker stop &lt;name&gt;或停止它们docker stop &lt;containerId&gt;。 Docker Compose设置如果您已经完成了本教程，并且想快速进行设置，那么可以使用示例存储库中的本教程的Docker Compose版本。我们提供Docker Compose文件，用于与MySQL，Postgres，MongoDB，SQL Server和Oracle一起运行本教程。请按照自述文件中描述的步骤进行操作。","link":"/2020/03/04/使用Debezium和Kafka为MySQL数据库设置变化数据捕捉CDC/"}],"tags":[{"name":"Ceph","slug":"Ceph","link":"/tags/Ceph/"},{"name":"Cloud Native GIS","slug":"Cloud-Native-GIS","link":"/tags/Cloud-Native-GIS/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"Flink","slug":"Flink","link":"/tags/Flink/"},{"name":"机器学习 索引","slug":"机器学习-索引","link":"/tags/机器学习-索引/"},{"name":"Linux LVM","slug":"Linux-LVM","link":"/tags/Linux-LVM/"},{"name":"Minio","slug":"Minio","link":"/tags/Minio/"},{"name":"Mvnw","slug":"Mvnw","link":"/tags/Mvnw/"},{"name":"Oracle 运维","slug":"Oracle-运维","link":"/tags/Oracle-运维/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"Presto","slug":"Presto","link":"/tags/Presto/"},{"name":"service mesh","slug":"service-mesh","link":"/tags/service-mesh/"},{"name":"Sharding-Sphere","slug":"Sharding-Sphere","link":"/tags/Sharding-Sphere/"},{"name":"Kubenetes","slug":"Kubenetes","link":"/tags/Kubenetes/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/tags/TensorFlow/"},{"name":"spark, excel, spark-excel","slug":"spark-excel-spark-excel","link":"/tags/spark-excel-spark-excel/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"Guava","slug":"Guava","link":"/tags/Guava/"},{"name":"神经网络","slug":"神经网络","link":"/tags/神经网络/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Docker, Maven, sed","slug":"Docker-Maven-sed","link":"/tags/Docker-Maven-sed/"},{"name":"Gitlab, kubenetes","slug":"Gitlab-kubenetes","link":"/tags/Gitlab-kubenetes/"},{"name":"决策支持系统","slug":"决策支持系统","link":"/tags/决策支持系统/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Delta-lake, 大数据, spark","slug":"Delta-lake-大数据-spark","link":"/tags/Delta-lake-大数据-spark/"},{"name":"摄影","slug":"摄影","link":"/tags/摄影/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"music","slug":"music","link":"/tags/music/"},{"name":"数据湖","slug":"数据湖","link":"/tags/数据湖/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"空间数据库 空间索引 综述","slug":"空间数据库-空间索引-综述","link":"/tags/空间数据库-空间索引-综述/"},{"name":"Kubernetes Ingress","slug":"Kubernetes-Ingress","link":"/tags/Kubernetes-Ingress/"},{"name":"Raml","slug":"Raml","link":"/tags/Raml/"},{"name":"经济, 大数据","slug":"经济-大数据","link":"/tags/经济-大数据/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"spring security","slug":"spring-security","link":"/tags/spring-security/"},{"name":"Mongo, Spark, 大数据","slug":"Mongo-Spark-大数据","link":"/tags/Mongo-Spark-大数据/"},{"name":"Debezium, Kafka, MySQL, CDC","slug":"Debezium-Kafka-MySQL-CDC","link":"/tags/Debezium-Kafka-MySQL-CDC/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"},{"name":"Lifestyle","slug":"Lifestyle","link":"/categories/Lifestyle/"},{"name":"Music","slug":"Music","link":"/categories/Music/"},{"name":"Research","slug":"Research","link":"/categories/Research/"}]}